{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES â†’ BBF Migration Data Analysis (v6)\n",
    "\n",
    "This notebook analyzes all data required for the ES to BBF Salesforce migration.\n",
    "\n",
    "## Version 6 Changes\n",
    "- **NEW**: Node__c analysis - query nodes referenced by migration orders\n",
    "- **NEW**: Node A/Z determination logic based on Address matching\n",
    "- **NEW**: Fields_Node sheet with ES â†’ BBF field mapping\n",
    "- **NEW**: Node added to Records to Migrate\n",
    "\n",
    "## Version 5 Changes (inherited)\n",
    "- Comprehensive Legend sheet with full documentation\n",
    "- Field reference sheets for each object\n",
    "\n",
    "## Version 4 Changes (inherited)\n",
    "- OrderItem analysis for migration scope orders\n",
    "- Active Date uses `Billing_Start_Date__c` (primary) â†’ OSS `bill_start_date` (fallback)\n",
    "\n",
    "## Version 3 Changes (inherited)\n",
    "- Include OA (Accepted) orders in addition to CL (Closed)\n",
    "- Work Orders enriched with data from `workorders.workorders` table\n",
    "\n",
    "## Driving Principle\n",
    "**Everything is driven from Active ES Orders that are ACTUALLY BILLING in OSS** - we migrate only the data needed to support truly active services.\n",
    "\n",
    "## Filter Pipeline\n",
    "1. Status IN ('Activated', 'Suspended (Late Payment)', 'Disconnect in Progress')\n",
    "2. Project_Group__c NOT LIKE '%PA MARKET DECOM%'\n",
    "3. Service_Order_Record_Type__c = 'Service Order Agreement' (excludes Work Orders)\n",
    "4. OSS Actively Billing:\n",
    "   - order_state_cd IN ('CL', 'OA')\n",
    "   - bill_start_date <= today\n",
    "   - bill_end_date IS NULL or > today\n",
    "5. Has BBF BAN mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\vjero\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe\n",
      "Pandas: 2.2.3\n",
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "# === SETUP & IMPORTS ===\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from simple_salesforce import Salesforce\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration loaded\n",
      "   Active Statuses: ['Activated', 'Suspended (Late Payment)', 'Disconnect in Progress']\n",
      "   Record Type: Service Order Agreement\n",
      "   Excluding: Project_Group__c LIKE '%PA MARKET DECOM%'\n",
      "   OSS Active States: ['CL', 'OA']\n",
      "   Output: es_bbf_migration_analysis_v6_20260108_121628.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# ES (Source) Salesforce Credentials\n",
    "ES_USERNAME = \"sfdcapi@everstream.net\"\n",
    "ES_PASSWORD = \"pV4CAxns8DQtJsBq!\"\n",
    "ES_TOKEN = \"r1uoYiusK19RbrflARydi86TA\"\n",
    "ES_DOMAIN = \"login\"  # 'login' for production\n",
    "\n",
    "# OSS Database Credentials\n",
    "OSS_HOST = \"pg01.comlink.net\"\n",
    "OSS_PORT = \"5432\"\n",
    "OSS_DB = \"GLC\"\n",
    "OSS_USER = \"oss_server\"\n",
    "OSS_PASSWORD = \"3wU3uB28X?!r2?@ebrUg\"\n",
    "\n",
    "# Active Order Status Filter\n",
    "ACTIVE_STATUSES = [\"Activated\", \"Suspended (Late Payment)\", \"Disconnect in Progress\"]\n",
    "\n",
    "# Record Type Filter\n",
    "VALID_RECORD_TYPE = \"Service Order Agreement\"\n",
    "\n",
    "# PA Market Decom Exclusion\n",
    "PA_DECOM_FILTER = \"PA MARKET DECOM\"\n",
    "\n",
    "# OSS Order States that qualify as \"actively billing\"\n",
    "ACTIVE_OSS_STATES = [\"CL\", \"OA\"]  # Closed and Accepted\n",
    "\n",
    "# OSS Order States Reference\n",
    "OSS_ORDER_STATES = {\n",
    "    \"CL\": \"Closed (Active/Billing)\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"OS\": \"Submitted\",\n",
    "    \"OC\": \"Created\",\n",
    "    \"PN\": \"Pending\",\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"OR\": \"Rejected\",\n",
    "    \"OV\": \"Validated (Disabled)\",\n",
    "}\n",
    "\n",
    "# OSS Work Order Types Reference\n",
    "WORKORDER_TYPES = {\n",
    "    \"IT\": \"Professional Services\",\n",
    "    \"MR\": \"Maintenance/Repair\",\n",
    "    \"OS\": \"Other Service\",\n",
    "    \"VS\": \"Voice Service\",\n",
    "}\n",
    "\n",
    "# OSS Work Order States Reference\n",
    "WORKORDER_STATES = {\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"CL\": \"Closed\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"PN\": \"Pending\",\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_FILE = f\"es_bbf_migration_analysis_v6_{TIMESTAMP}.xlsx\"\n",
    "\n",
    "print(\"ðŸ“‹ Configuration loaded\")\n",
    "print(f\"   Active Statuses: {ACTIVE_STATUSES}\")\n",
    "print(f\"   Record Type: {VALID_RECORD_TYPE}\")\n",
    "print(f\"   Excluding: Project_Group__c LIKE '%{PA_DECOM_FILTER}%'\")\n",
    "print(f\"   OSS Active States: {ACTIVE_OSS_STATES}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONNECTING TO ES SALESFORCE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to ES...\n",
      "âœ… Connected to ES: everstream.my.salesforce.com\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO ES SALESFORCE ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONNECTING TO ES SALESFORCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to ES...\")\n",
    "es_sf = Salesforce(\n",
    "    username=ES_USERNAME,\n",
    "    password=ES_PASSWORD,\n",
    "    security_token=ES_TOKEN,\n",
    "    domain=ES_DOMAIN,\n",
    ")\n",
    "print(f\"âœ… Connected to ES: {es_sf.sf_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONNECTING TO OSS DATABASE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to OSS...\n",
      "âœ… Connected to OSS: pg01.comlink.net/GLC\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO OSS DATABASE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONNECTING TO OSS DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to OSS...\")\n",
    "oss_conn = psycopg2.connect(\n",
    "    dbname=OSS_DB,\n",
    "    user=OSS_USER,\n",
    "    password=OSS_PASSWORD,\n",
    "    host=OSS_HOST,\n",
    "    port=OSS_PORT,\n",
    ")\n",
    "print(f\"âœ… Connected to OSS: {OSS_HOST}/{OSS_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: QUERYING ACTIVE ORDERS\n",
      "================================================================================\n",
      "Querying all orders with active statuses...\n",
      "\n",
      "âœ… Total orders with active status: 17,980\n",
      "\n",
      "ðŸ“Š Record Type Breakdown:\n",
      "Service_Order_Record_Type__c\n",
      "Service Order Agreement    14059\n",
      "Work Order                  3921\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: QUERY ALL ACTIVE ORDERS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: QUERYING ACTIVE ORDERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status_filter = \"','\".join(ACTIVE_STATUSES)\n",
    "\n",
    "orders_query = f\"\"\"\n",
    "SELECT \n",
    "    Id, \n",
    "    Name,\n",
    "    Service_ID__c,\n",
    "    Status,\n",
    "    AccountId,\n",
    "    Account.Name,\n",
    "    Billing_Invoice__c,\n",
    "    Address_A__c,\n",
    "    Address_Z__c,\n",
    "    Node__c,\n",
    "    OpportunityId,\n",
    "    Service_Start_Date__c,\n",
    "    Billing_Start_Date__c,\n",
    "    Service_End_Date__c,\n",
    "    Service_Provided__c,\n",
    "    SOF_MRC__c,\n",
    "    OSS_Order__c,\n",
    "    OSS_Service_ID__c,\n",
    "    Vendor_Circuit_ID__c,\n",
    "    Primary_Product_Family__c,\n",
    "    Primary_Product_Name__c,\n",
    "    Project_Group__c,\n",
    "    Service_Order_Record_Type__c,\n",
    "    CreatedDate,\n",
    "    LastModifiedDate\n",
    "FROM Order\n",
    "WHERE Status IN ('{status_filter}')\n",
    "ORDER BY Service_ID__c\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying all orders with active statuses...\")\n",
    "result = es_sf.query_all(orders_query)\n",
    "orders_raw = result[\"records\"]\n",
    "\n",
    "# Flatten results\n",
    "all_orders = []\n",
    "for order in orders_raw:\n",
    "    all_orders.append(\n",
    "        {\n",
    "            \"Id\": order[\"Id\"],\n",
    "            \"Name\": order.get(\"Name\"),\n",
    "            \"Service_ID__c\": order.get(\"Service_ID__c\"),\n",
    "            \"Status\": order[\"Status\"],\n",
    "            \"AccountId\": order.get(\"AccountId\"),\n",
    "            \"Account_Name\": order[\"Account\"][\"Name\"] if order.get(\"Account\") else None,\n",
    "            \"Billing_Invoice__c\": order.get(\"Billing_Invoice__c\"),\n",
    "            \"Address_A__c\": order.get(\"Address_A__c\"),\n",
    "            \"Address_Z__c\": order.get(\"Address_Z__c\"),\n",
    "            \"Node__c\": order.get(\"Node__c\"),\n",
    "            \"OpportunityId\": order.get(\"OpportunityId\"),\n",
    "            \"Service_Start_Date__c\": order.get(\"Service_Start_Date__c\"),\n",
    "            \"Billing_Start_Date__c\": order.get(\"Billing_Start_Date__c\"),\n",
    "            \"Service_End_Date__c\": order.get(\"Service_End_Date__c\"),\n",
    "            \"Service_Provided__c\": order.get(\"Service_Provided__c\"),\n",
    "            \"SOF_MRC__c\": order.get(\"SOF_MRC__c\"),\n",
    "            \"OSS_Order__c\": order.get(\"OSS_Order__c\"),\n",
    "            \"OSS_Service_ID__c\": order.get(\"OSS_Service_ID__c\"),\n",
    "            \"Vendor_Circuit_ID__c\": order.get(\"Vendor_Circuit_ID__c\"),\n",
    "            \"Primary_Product_Family__c\": order.get(\"Primary_Product_Family__c\"),\n",
    "            \"Primary_Product_Name__c\": order.get(\"Primary_Product_Name__c\"),\n",
    "            \"Project_Group__c\": order.get(\"Project_Group__c\"),\n",
    "            \"Service_Order_Record_Type__c\": order.get(\"Service_Order_Record_Type__c\"),\n",
    "            \"CreatedDate\": order.get(\"CreatedDate\"),\n",
    "            \"LastModifiedDate\": order.get(\"LastModifiedDate\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_orders_df = pd.DataFrame(all_orders)\n",
    "print(f\"\\nâœ… Total orders with active status: {len(all_orders_df):,}\")\n",
    "\n",
    "# Show record type breakdown\n",
    "print(f\"\\nðŸ“Š Record Type Breakdown:\")\n",
    "print(all_orders_df[\"Service_Order_Record_Type__c\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FILTER - PA MARKET DECOM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PA MARKET DECOM Filter:\n",
      "   Before: 17,980\n",
      "   âŒ Excluded (PA MARKET DECOM): 887\n",
      "   âœ… Remaining: 17,093\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: FILTER - PA MARKET DECOM ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: FILTER - PA MARKET DECOM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate PA MARKET DECOM orders\n",
    "pa_decom_mask = (\n",
    "    all_orders_df[\"Project_Group__c\"]\n",
    "    .fillna(\"\")\n",
    "    .str.contains(PA_DECOM_FILTER, case=False)\n",
    ")\n",
    "excluded_pa_decom_df = all_orders_df[pa_decom_mask].copy()\n",
    "orders_after_pa_filter = all_orders_df[~pa_decom_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š PA MARKET DECOM Filter:\")\n",
    "print(f\"   Before: {len(all_orders_df):,}\")\n",
    "print(f\"   âŒ Excluded (PA MARKET DECOM): {len(excluded_pa_decom_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_pa_filter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FILTER - RECORD TYPE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Record Type Filter:\n",
      "   Before: 17,093\n",
      "   âŒ Excluded (Not 'Service Order Agreement'): 3,484\n",
      "   âœ… Remaining: 13,609\n",
      "\n",
      "   Excluded Record Types:\n",
      "Service_Order_Record_Type__c\n",
      "Work Order    3484\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: FILTER - RECORD TYPE (Service Order Agreement only) ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FILTER - RECORD TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Keep only Service Order Agreement\n",
    "record_type_mask = (\n",
    "    orders_after_pa_filter[\"Service_Order_Record_Type__c\"] == VALID_RECORD_TYPE\n",
    ")\n",
    "excluded_work_orders_df = orders_after_pa_filter[~record_type_mask].copy()\n",
    "orders_after_rt_filter = orders_after_pa_filter[record_type_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Record Type Filter:\")\n",
    "print(f\"   Before: {len(orders_after_pa_filter):,}\")\n",
    "print(f\"   âŒ Excluded (Not '{VALID_RECORD_TYPE}'): {len(excluded_work_orders_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_rt_filter):,}\")\n",
    "\n",
    "# Show breakdown of what was excluded\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    print(f\"\\n   Excluded Record Types:\")\n",
    "    print(\n",
    "        excluded_work_orders_df[\"Service_Order_Record_Type__c\"].value_counts(\n",
    "            dropna=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\n",
      "================================================================================\n",
      "\n",
      "   Work Orders with OSS_Order__c: 156\n",
      "   Work Orders without OSS_Order__c: 3,328\n",
      "   Unique workorder IDs to query: 156\n",
      "   Chunk 1: Retrieved 84 workorders\n",
      "\n",
      "âœ… Enriched 84 work orders with OSS data\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3B: ENRICH WORK ORDERS WITH OSS DATA ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    # Get work orders with OSS_Order__c (maps to workorder_id)\n",
    "    has_wo_oss = excluded_work_orders_df[\"OSS_Order__c\"].notna() & (\n",
    "        excluded_work_orders_df[\"OSS_Order__c\"] != \"\"\n",
    "    )\n",
    "    work_orders_with_oss = excluded_work_orders_df[has_wo_oss].copy()\n",
    "\n",
    "    print(f\"\\n   Work Orders with OSS_Order__c: {len(work_orders_with_oss):,}\")\n",
    "    print(f\"   Work Orders without OSS_Order__c: {(~has_wo_oss).sum():,}\")\n",
    "\n",
    "    if len(work_orders_with_oss) > 0:\n",
    "        workorder_ids = (\n",
    "            work_orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "        )\n",
    "        print(f\"   Unique workorder IDs to query: {len(workorder_ids):,}\")\n",
    "\n",
    "        # Query workorders.workorders\n",
    "        chunk_size = 5000\n",
    "        oss_workorders = []\n",
    "\n",
    "        for i in range(0, len(workorder_ids), chunk_size):\n",
    "            chunk = workorder_ids[i : i + chunk_size]\n",
    "            ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "            wo_query = f\"\"\"\n",
    "            SELECT \n",
    "                workorder_id,\n",
    "                order_nm,\n",
    "                order_id,\n",
    "                workorder_type_cd,\n",
    "                workorder_state_cd,\n",
    "                description,\n",
    "                start_date,\n",
    "                end_date,\n",
    "                disabled\n",
    "            FROM workorders.workorders\n",
    "            WHERE workorder_id IN ({ids_str})\n",
    "              AND disabled = 'infinity'\n",
    "            \"\"\"\n",
    "\n",
    "            with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "                cur.execute(wo_query)\n",
    "                rows = cur.fetchall()\n",
    "                oss_workorders.extend([dict(row) for row in rows])\n",
    "\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} workorders\")\n",
    "\n",
    "        if len(oss_workorders) > 0:\n",
    "            oss_wo_df = pd.DataFrame(oss_workorders)\n",
    "\n",
    "            # Convert date columns - strip timezone for Excel compatibility\n",
    "            for date_col in [\"start_date\", \"end_date\"]:\n",
    "                if date_col in oss_wo_df.columns:\n",
    "                    oss_wo_df[date_col] = pd.to_datetime(\n",
    "                        oss_wo_df[date_col], utc=True\n",
    "                    ).dt.tz_localize(None)\n",
    "\n",
    "            # Add descriptions\n",
    "            oss_wo_df[\"workorder_type_desc\"] = oss_wo_df[\"workorder_type_cd\"].map(\n",
    "                WORKORDER_TYPES\n",
    "            )\n",
    "            oss_wo_df[\"workorder_state_desc\"] = oss_wo_df[\"workorder_state_cd\"].map(\n",
    "                WORKORDER_STATES\n",
    "            )\n",
    "\n",
    "            # Merge with excluded work orders\n",
    "            excluded_work_orders_df[\"OSS_Order_ID\"] = pd.to_numeric(\n",
    "                excluded_work_orders_df[\"OSS_Order__c\"], errors=\"coerce\"\n",
    "            )\n",
    "            excluded_work_orders_df = excluded_work_orders_df.merge(\n",
    "                oss_wo_df, left_on=\"OSS_Order_ID\", right_on=\"workorder_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"\\nâœ… Enriched {oss_wo_df['workorder_id'].notna().sum():,} work orders with OSS data\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ No active workorders found in OSS\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No work orders have OSS_Order__c populated\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No work orders to enrich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š OSS_Order__c Population:\n",
      "   With OSS_Order__c: 11,525\n",
      "   Without OSS_Order__c: 2,084\n",
      "\n",
      "   Unique OSS Order IDs to query: 11,518\n",
      "   Chunk 1: Retrieved 5000 orders\n",
      "   Chunk 2: Retrieved 4999 orders\n",
      "   Chunk 3: Retrieved 1518 orders\n",
      "\n",
      "âœ… Total OSS orders retrieved: 11,517\n",
      "   OSS Order IDs not found in OSS: 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: CHECK OSS ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify orders with OSS_Order__c\n",
    "has_oss_id = orders_after_rt_filter[\"OSS_Order__c\"].notna() & (\n",
    "    orders_after_rt_filter[\"OSS_Order__c\"] != \"\"\n",
    ")\n",
    "orders_with_oss = orders_after_rt_filter[has_oss_id].copy()\n",
    "orders_without_oss = orders_after_rt_filter[~has_oss_id].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š OSS_Order__c Population:\")\n",
    "print(f\"   With OSS_Order__c: {len(orders_with_oss):,}\")\n",
    "print(f\"   Without OSS_Order__c: {len(orders_without_oss):,}\")\n",
    "\n",
    "# Query OSS for orders with OSS_Order__c\n",
    "if len(orders_with_oss) > 0:\n",
    "    oss_order_ids = (\n",
    "        orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "    )\n",
    "    print(f\"\\n   Unique OSS Order IDs to query: {len(oss_order_ids):,}\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 5000\n",
    "    oss_orders = []\n",
    "\n",
    "    for i in range(0, len(oss_order_ids), chunk_size):\n",
    "        chunk = oss_order_ids[i : i + chunk_size]\n",
    "        ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "        oss_query = f\"\"\"\n",
    "        SELECT \n",
    "            order_id,\n",
    "            order_state_cd,\n",
    "            order_type_cd,\n",
    "            bill_start_date,\n",
    "            bill_end_date,\n",
    "            circuit_active_date,\n",
    "            account_id,\n",
    "            service_id\n",
    "        FROM om.orders\n",
    "        WHERE order_id IN ({ids_str})\n",
    "        \"\"\"\n",
    "\n",
    "        with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(oss_query)\n",
    "            rows = cur.fetchall()\n",
    "            oss_orders.extend([dict(row) for row in rows])\n",
    "\n",
    "        print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} orders\")\n",
    "\n",
    "    oss_orders_df = pd.DataFrame(oss_orders)\n",
    "    print(f\"\\nâœ… Total OSS orders retrieved: {len(oss_orders_df):,}\")\n",
    "\n",
    "    # Check for OSS IDs not found\n",
    "    found_ids = (\n",
    "        set(oss_orders_df[\"order_id\"].tolist()) if len(oss_orders_df) > 0 else set()\n",
    "    )\n",
    "    not_found_ids = set(oss_order_ids) - found_ids\n",
    "    print(f\"   OSS Order IDs not found in OSS: {len(not_found_ids):,}\")\n",
    "else:\n",
    "    oss_orders_df = pd.DataFrame()\n",
    "    not_found_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: DETERMINING ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "Today's date for comparison: 2026-01-08\n",
      "Active OSS States: ['CL', 'OA']\n",
      "\n",
      "ðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\n",
      "   ACTIVELY BILLING: 11,490 (99.7%)\n",
      "   CL - Bill End Passed: 23 (0.2%)\n",
      "   Not Active State (CA): 6 (0.1%)\n",
      "   CL - Bill Start Future: 5 (0.0%)\n",
      "   No OSS Match: 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: DETERMINE ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: DETERMINING ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "today = pd.Timestamp.now().normalize()\n",
    "print(f\"\\nToday's date for comparison: {today.date()}\")\n",
    "print(f\"Active OSS States: {ACTIVE_OSS_STATES}\")\n",
    "\n",
    "# Merge SF orders with OSS data\n",
    "if len(orders_with_oss) > 0:\n",
    "    orders_with_oss[\"OSS_Order_ID\"] = orders_with_oss[\"OSS_Order__c\"].astype(int)\n",
    "\n",
    "if len(oss_orders_df) > 0:\n",
    "    # Convert date columns\n",
    "    oss_orders_df[\"bill_start_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_start_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "    oss_orders_df[\"bill_end_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_end_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged_df = orders_with_oss.merge(\n",
    "        oss_orders_df, left_on=\"OSS_Order_ID\", right_on=\"order_id\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    merged_df = orders_with_oss.copy()\n",
    "    merged_df[\"order_id\"] = None\n",
    "    merged_df[\"order_state_cd\"] = None\n",
    "    merged_df[\"bill_start_date\"] = None\n",
    "    merged_df[\"bill_end_date\"] = None\n",
    "\n",
    "\n",
    "# Determine actively billing status\n",
    "def get_billing_status(row):\n",
    "    if pd.isna(row.get(\"order_id\")):\n",
    "        return \"No OSS Match\"\n",
    "\n",
    "    state = row[\"order_state_cd\"].strip() if row.get(\"order_state_cd\") else None\n",
    "    bill_start = row.get(\"bill_start_date\")\n",
    "    bill_end = row.get(\"bill_end_date\")\n",
    "\n",
    "    # Check if state is in allowed active states (CL or OA)\n",
    "    if state not in ACTIVE_OSS_STATES:\n",
    "        return f\"Not Active State ({state})\"\n",
    "\n",
    "    if pd.isna(bill_start):\n",
    "        return f\"{state} - No Bill Start Date\"\n",
    "\n",
    "    if bill_start > today:\n",
    "        return f\"{state} - Bill Start Future\"\n",
    "\n",
    "    if pd.notna(bill_end) and bill_end <= today:\n",
    "        return f\"{state} - Bill End Passed\"\n",
    "\n",
    "    return \"ACTIVELY BILLING\"\n",
    "\n",
    "\n",
    "merged_df[\"OSS_Billing_Status\"] = merged_df.apply(get_billing_status, axis=1)\n",
    "\n",
    "# Add state description\n",
    "merged_df[\"OSS_State_Desc\"] = merged_df[\"order_state_cd\"].map(OSS_ORDER_STATES)\n",
    "\n",
    "# Also handle orders without OSS link\n",
    "orders_without_oss[\"OSS_Billing_Status\"] = \"No OSS_Order__c in SF\"\n",
    "orders_without_oss[\"order_state_cd\"] = None\n",
    "orders_without_oss[\"OSS_State_Desc\"] = None\n",
    "orders_without_oss[\"bill_start_date\"] = None\n",
    "orders_without_oss[\"bill_end_date\"] = None\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\")\n",
    "if len(merged_df) > 0:\n",
    "    billing_status_counts = merged_df[\"OSS_Billing_Status\"].value_counts()\n",
    "    for status, count in billing_status_counts.items():\n",
    "        pct = 100 * count / len(merged_df)\n",
    "        print(f\"   {status}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FILTER - ACTIVELY BILLING ONLY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Actively Billing Filter:\n",
      "   Before: 13,609\n",
      "   âŒ Excluded (Not Actively Billing): 2,119\n",
      "   âœ… Confirmed Actively Billing: 11,490\n",
      "\n",
      "   Breakdown of excluded:\n",
      "      No OSS_Order__c in SF: 2,084\n",
      "      CL - Bill End Passed: 23\n",
      "      Not Active State (CA): 6\n",
      "      CL - Bill Start Future: 5\n",
      "      No OSS Match: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_29448\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n",
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_29448\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: FILTER - ACTIVELY BILLING ONLY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: FILTER - ACTIVELY BILLING ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get common columns\n",
    "common_cols = [col for col in orders_after_rt_filter.columns]\n",
    "oss_cols = [\n",
    "    \"order_state_cd\",\n",
    "    \"OSS_State_Desc\",\n",
    "    \"bill_start_date\",\n",
    "    \"bill_end_date\",\n",
    "    \"OSS_Billing_Status\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "for col in oss_cols:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = None\n",
    "    if col not in orders_without_oss.columns:\n",
    "        orders_without_oss[col] = None\n",
    "\n",
    "all_cols = common_cols + oss_cols\n",
    "\n",
    "# Combine orders with and without OSS\n",
    "all_checked_orders = pd.concat(\n",
    "    [\n",
    "        merged_df[[c for c in all_cols if c in merged_df.columns]],\n",
    "        orders_without_oss[[c for c in all_cols if c in orders_without_oss.columns]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Split into actively billing vs not\n",
    "actively_billing_mask = all_checked_orders[\"OSS_Billing_Status\"] == \"ACTIVELY BILLING\"\n",
    "orders_actively_billing = all_checked_orders[actively_billing_mask].copy()\n",
    "excluded_not_billing_df = all_checked_orders[~actively_billing_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Actively Billing Filter:\")\n",
    "print(f\"   Before: {len(all_checked_orders):,}\")\n",
    "print(f\"   âŒ Excluded (Not Actively Billing): {len(excluded_not_billing_df):,}\")\n",
    "print(f\"   âœ… Confirmed Actively Billing: {len(orders_actively_billing):,}\")\n",
    "\n",
    "# Show breakdown of excluded\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    print(f\"\\n   Breakdown of excluded:\")\n",
    "    for status, count in (\n",
    "        excluded_not_billing_df[\"OSS_Billing_Status\"].value_counts().items()\n",
    "    ):\n",
    "        print(f\"      {status}: {count:,}\")\n",
    "\n",
    "# Set this as our active orders for the rest of the pipeline\n",
    "active_orders_df = orders_actively_billing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: QUERYING NEW BBF BANS\n",
      "================================================================================\n",
      "Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\n",
      "\n",
      "âœ… Found 2,505 new BBF BANs\n",
      "   Legacy BAN to BBF BAN mappings: 2,505\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: GET NEW BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: QUERYING NEW BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bbf_ban_query = \"\"\"\n",
    "SELECT \n",
    "    Id,\n",
    "    Name,\n",
    "    Account__c,\n",
    "    Account__r.Name,\n",
    "    Legacy_ES_Id__c,\n",
    "    BBF_Ban__c,\n",
    "    Billing_Address_1__c,\n",
    "    Billing_City__c,\n",
    "    Billing_State__c,\n",
    "    Billing_ZIP__c,\n",
    "    Payment_Terms__c,\n",
    "    Active_Billing__c\n",
    "FROM Billing_Invoice__c\n",
    "WHERE BBF_Ban__c = true\n",
    "  AND Legacy_ES_Id__c != null\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\")\n",
    "result = es_sf.query_all(bbf_ban_query)\n",
    "bbf_bans_df = pd.DataFrame(result[\"records\"])\n",
    "\n",
    "if len(bbf_bans_df) > 0:\n",
    "    # Flatten Account name\n",
    "    if \"Account__r\" in bbf_bans_df.columns:\n",
    "        bbf_bans_df[\"Account_Name\"] = bbf_bans_df[\"Account__r\"].apply(\n",
    "            lambda x: x[\"Name\"] if x else None\n",
    "        )\n",
    "\n",
    "    # Clean up\n",
    "    bbf_bans_df = bbf_bans_df.drop(\n",
    "        columns=[\"attributes\", \"Account__r\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Found {len(bbf_bans_df):,} new BBF BANs\")\n",
    "\n",
    "    # Create lookup from legacy BAN ID to new BBF BAN\n",
    "    legacy_to_bbf_ban = {}\n",
    "    for _, ban in bbf_bans_df.iterrows():\n",
    "        legacy_id = ban.get(\"Legacy_ES_Id__c\")\n",
    "        if legacy_id:\n",
    "            legacy_to_bbf_ban[legacy_id] = {\n",
    "                \"Id\": ban[\"Id\"],\n",
    "                \"Name\": ban[\"Name\"],\n",
    "                \"Account__c\": ban.get(\"Account__c\"),\n",
    "                \"Account_Name\": ban.get(\"Account_Name\"),\n",
    "            }\n",
    "    print(f\"   Legacy BAN to BBF BAN mappings: {len(legacy_to_bbf_ban):,}\")\n",
    "else:\n",
    "    legacy_to_bbf_ban = {}\n",
    "    print(\"âš ï¸ No BBF BANs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: MAPPING ORDERS TO BBF BANS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š BAN Mapping Results:\n",
      "   âœ… Ready to migrate (has BBF BAN): 11,476\n",
      "   âš ï¸ Missing BBF BAN mapping: 12\n",
      "   âŒ Missing ANY BAN: 2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: MAP ORDERS TO BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: MAPPING ORDERS TO BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track which orders can be mapped\n",
    "orders_with_bbf_ban = []\n",
    "orders_missing_bbf_ban = []\n",
    "orders_missing_any_ban = []\n",
    "\n",
    "for _, order in active_orders_df.iterrows():\n",
    "    legacy_ban_id = order.get(\"Billing_Invoice__c\")\n",
    "    order_dict = order.to_dict()\n",
    "\n",
    "    if not legacy_ban_id:\n",
    "        # Order has no BAN at all\n",
    "        orders_missing_any_ban.append(order_dict)\n",
    "    elif legacy_ban_id in legacy_to_bbf_ban:\n",
    "        # Order can be mapped to new BBF BAN\n",
    "        bbf_ban = legacy_to_bbf_ban[legacy_ban_id]\n",
    "        order_dict[\"New_BBF_BAN_Id\"] = bbf_ban[\"Id\"]\n",
    "        order_dict[\"New_BBF_BAN_Name\"] = bbf_ban[\"Name\"]\n",
    "        order_dict[\"New_BBF_BAN_Account__c\"] = bbf_ban[\"Account__c\"]\n",
    "        order_dict[\"New_BBF_BAN_Account_Name\"] = bbf_ban[\"Account_Name\"]\n",
    "        orders_with_bbf_ban.append(order_dict)\n",
    "    else:\n",
    "        # Order has legacy BAN but no BBF BAN mapping\n",
    "        orders_missing_bbf_ban.append(order_dict)\n",
    "\n",
    "orders_ready_df = pd.DataFrame(orders_with_bbf_ban)\n",
    "orders_no_bbf_ban_df = pd.DataFrame(orders_missing_bbf_ban)\n",
    "orders_no_ban_df = pd.DataFrame(orders_missing_any_ban)\n",
    "\n",
    "print(f\"\\nðŸ“Š BAN Mapping Results:\")\n",
    "print(f\"   âœ… Ready to migrate (has BBF BAN): {len(orders_ready_df):,}\")\n",
    "print(f\"   âš ï¸ Missing BBF BAN mapping: {len(orders_no_bbf_ban_df):,}\")\n",
    "print(f\"   âŒ Missing ANY BAN: {len(orders_no_ban_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Found 2,225 unique Accounts from new BBF BANs\n",
      "\n",
      "âœ… Accounts to migrate: 2,225\n"
     ]
    }
   ],
   "source": [
    "# === STEP 9: IDENTIFY ACCOUNTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    unique_account_ids = (\n",
    "        orders_ready_df[\"New_BBF_BAN_Account__c\"].dropna().unique().tolist()\n",
    "    )\n",
    "    print(f\"\\nFound {len(unique_account_ids):,} unique Accounts from new BBF BANs\")\n",
    "\n",
    "    if unique_account_ids:\n",
    "        chunk_size = 150\n",
    "        all_accounts = []\n",
    "\n",
    "        for i in range(0, len(unique_account_ids), chunk_size):\n",
    "            chunk = unique_account_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            account_query = f\"\"\"\n",
    "            SELECT Id, Name, Type, Industry, \n",
    "                   BillingStreet, BillingCity, BillingState, BillingPostalCode, BillingCountry,\n",
    "                   Phone, Website, BBF_New_Id__c\n",
    "            FROM Account\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(account_query)\n",
    "            all_accounts.extend(result[\"records\"])\n",
    "\n",
    "        accounts_df = pd.DataFrame(all_accounts)\n",
    "        if len(accounts_df) > 0:\n",
    "            accounts_df = accounts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Accounts to migrate: {len(accounts_df):,}\")\n",
    "    else:\n",
    "        accounts_df = pd.DataFrame()\n",
    "else:\n",
    "    accounts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Account query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: IDENTIFYING CONTACTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Contacts to migrate: 15,581\n"
     ]
    }
   ],
   "source": [
    "# === STEP 10: IDENTIFY CONTACTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 10: IDENTIFYING CONTACTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(accounts_df) > 0:\n",
    "    account_ids = accounts_df[\"Id\"].tolist()\n",
    "    chunk_size = 150\n",
    "    all_contacts = []\n",
    "\n",
    "    for i in range(0, len(account_ids), chunk_size):\n",
    "        chunk = account_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        contact_query = f\"\"\"\n",
    "        SELECT Id, AccountId, FirstName, LastName, Email, Phone, Title,\n",
    "               MailingStreet, MailingCity, MailingState, MailingPostalCode,\n",
    "               BBF_New_Id__c\n",
    "        FROM Contact\n",
    "        WHERE AccountId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(contact_query)\n",
    "        all_contacts.extend(result[\"records\"])\n",
    "\n",
    "    contacts_df = pd.DataFrame(all_contacts)\n",
    "    if len(contacts_df) > 0:\n",
    "        contacts_df = contacts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "    print(f\"\\nâœ… Contacts to migrate: {len(contacts_df):,}\")\n",
    "else:\n",
    "    contacts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No accounts to migrate, skipping Contact query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Unique locations referenced by orders:\n",
      "   Address_A: 723\n",
      "   Address_Z: 9,818\n",
      "   Combined unique: 10,176\n",
      "\n",
      "âœ… Locations to migrate: 10,176\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11: IDENTIFY LOCATIONS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    address_a_ids = orders_ready_df[\"Address_A__c\"].dropna().unique().tolist()\n",
    "    address_z_ids = orders_ready_df[\"Address_Z__c\"].dropna().unique().tolist()\n",
    "    all_address_ids = list(set(address_a_ids + address_z_ids))\n",
    "\n",
    "    print(f\"\\nUnique locations referenced by orders:\")\n",
    "    print(f\"   Address_A: {len(address_a_ids):,}\")\n",
    "    print(f\"   Address_Z: {len(address_z_ids):,}\")\n",
    "    print(f\"   Combined unique: {len(all_address_ids):,}\")\n",
    "\n",
    "    if all_address_ids:\n",
    "        chunk_size = 150\n",
    "        all_locations = []\n",
    "\n",
    "        for i in range(0, len(all_address_ids), chunk_size):\n",
    "            chunk = all_address_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            location_query = f\"\"\"\n",
    "            SELECT Id, Name, Address__c, City__c, State__c, County__c, Zip__c,\n",
    "                   Complete_Address__c, CLLI__c, Building_Status__c, On_Net__c,\n",
    "                   BBF_New_Id__c\n",
    "            FROM Address__c\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(location_query)\n",
    "            all_locations.extend(result[\"records\"])\n",
    "\n",
    "        locations_df = pd.DataFrame(all_locations)\n",
    "        if len(locations_df) > 0:\n",
    "            locations_df = locations_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Locations to migrate: {len(locations_df):,}\")\n",
    "    else:\n",
    "        locations_df = pd.DataFrame()\n",
    "else:\n",
    "    locations_df = pd.DataFrame()\n",
    "    all_address_ids = []\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Location query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Off_Net records to migrate: 2,157\n"
     ]
    }
   ],
   "source": [
    "# === STEP 12: IDENTIFY OFF_NET RECORDS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_address_ids) > 0:\n",
    "    chunk_size = 100\n",
    "    all_offnet = []\n",
    "\n",
    "    for i in range(0, len(all_address_ids), chunk_size):\n",
    "        chunk = all_address_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        offnet_query = f\"\"\"\n",
    "        SELECT Id, Name, \n",
    "               Location_1__c, Location_1_Address__c,\n",
    "               Location_2__c, Location_2_Address__c,\n",
    "               Off_Net_Vendor__c, Vendor_Name__c,\n",
    "               Vendor_circuit_Id__c, Internal_Circuit_Id__c,\n",
    "               Cost_MRC__c, Cost_NRC__c, Invoice_MRC__c,\n",
    "               LEC_Order_Status__c, Off_Net_Type__c,\n",
    "               Bandwidth__c, Circuit_Type__c, Term__c,\n",
    "               Term_Agreement_Start_Date__c, Term_Agreement_End_Date__c,\n",
    "               Vendor_Bill_Start_Date__c, Vendor_Bill_Stop_Date__c,\n",
    "               SOF1__c\n",
    "        FROM Off_Net__c\n",
    "        WHERE Location_1__c IN ('{ids_str}')\n",
    "           OR Location_2__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(offnet_query)\n",
    "        all_offnet.extend(result[\"records\"])\n",
    "\n",
    "    offnet_df = pd.DataFrame(all_offnet)\n",
    "    if len(offnet_df) > 0:\n",
    "        offnet_df = offnet_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        offnet_df = offnet_df.drop_duplicates(subset=[\"Id\"])\n",
    "    print(f\"\\nâœ… Off_Net records to migrate: {len(offnet_df):,}\")\n",
    "else:\n",
    "    offnet_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No locations to migrate, skipping Off_Net query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13: DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\n",
      "   âœ… Has Billing_Start_Date__c (primary):  11,475 (100.0%)\n",
      "   âš ï¸ Needs OSS bill_start_date (fallback): 1 (0.0%)\n",
      "      â””â”€ OSS covers: 1 of 1\n",
      "\n",
      "ðŸ“Š Data Quality Issues:\n",
      "   [HIGH] Orders missing Address_A__c: 2 (0.0%)\n",
      "         â†’ Cannot set A_Location__c on BBF Service__c\n",
      "   [LOW] Orders missing Node__c: 9785 (85.3%)\n",
      "         â†’ Can fix post-migration - A_Node__c/Z_Node__c optional\n",
      "   [LOW] Orders missing Billing_Start_Date__c: 1 (0.0%)\n",
      "         â†’ OSS bill_start_date covers 1 of 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13: DATA QUALITY ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_quality_issues = []\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    total_ready = len(orders_ready_df)\n",
    "\n",
    "    # Check for missing Address_A__c\n",
    "    missing_addr_a = orders_ready_df[\"Address_A__c\"].isna().sum()\n",
    "    if missing_addr_a > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Address_A__c\",\n",
    "                \"Count\": missing_addr_a,\n",
    "                \"Percentage\": f\"{missing_addr_a/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"HIGH\",\n",
    "                \"Impact\": \"Cannot set A_Location__c on BBF Service__c\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check for missing Node__c\n",
    "    missing_node = orders_ready_df[\"Node__c\"].isna().sum()\n",
    "    if missing_node > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Node__c\",\n",
    "                \"Count\": missing_node,\n",
    "                \"Percentage\": f\"{missing_node/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can fix post-migration - A_Node__c/Z_Node__c optional\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check Active Date Coverage: Billing_Start_Date__c (primary) â†’ OSS bill_start_date (fallback)\n",
    "    # Note: Service_Start_Date__c is historical (when service originally started), not used for Active_Date__c\n",
    "    sf_bill_start_null = orders_ready_df[\"Billing_Start_Date__c\"].isna()\n",
    "    oss_start_exists = orders_ready_df[\"bill_start_date\"].notna()\n",
    "\n",
    "    # Count coverage\n",
    "    has_bill_start = (~sf_bill_start_null).sum()\n",
    "    needs_oss = sf_bill_start_null.sum()\n",
    "    oss_covers = (sf_bill_start_null & oss_start_exists).sum()\n",
    "    no_date = (sf_bill_start_null & ~oss_start_exists).sum()\n",
    "\n",
    "    print(f\"\\nðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\")\n",
    "    print(\n",
    "        f\"   âœ… Has Billing_Start_Date__c (primary):  {has_bill_start:,} ({100*has_bill_start/total_ready:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   âš ï¸ Needs OSS bill_start_date (fallback): {needs_oss:,} ({100*needs_oss/total_ready:.1f}%)\"\n",
    "    )\n",
    "    if needs_oss > 0:\n",
    "        print(f\"      â””â”€ OSS covers: {oss_covers:,} of {needs_oss:,}\")\n",
    "    if no_date > 0:\n",
    "        print(f\"   ðŸ”´ No date available: {no_date:,}\")\n",
    "\n",
    "    if sf_bill_start_null.sum() > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Billing_Start_Date__c\",\n",
    "                \"Count\": sf_bill_start_null.sum(),\n",
    "                \"Percentage\": f\"{sf_bill_start_null.sum()/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\" if oss_covers == needs_oss else \"MEDIUM\",\n",
    "                \"Impact\": f\"OSS bill_start_date covers {oss_covers:,} of {needs_oss:,}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "data_quality_df = pd.DataFrame(data_quality_issues)\n",
    "\n",
    "if len(data_quality_issues) > 0:\n",
    "    print(\"\\nðŸ“Š Data Quality Issues:\")\n",
    "    for issue in data_quality_issues:\n",
    "        print(\n",
    "            f\"   [{issue['Severity']}] {issue['Issue']}: {issue['Count']} ({issue['Percentage']})\"\n",
    "        )\n",
    "        print(f\"         â†’ {issue['Impact']}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No significant data quality issues found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13B: QUERYING ORDER ITEMS\n",
      "================================================================================\n",
      "\n",
      "Querying OrderItems for 11,476 orders...\n",
      "   Chunk 10: 2,565 items so far...\n",
      "   Chunk 20: 5,539 items so far...\n",
      "   Chunk 30: 7,943 items so far...\n",
      "   Chunk 40: 10,002 items so far...\n",
      "   Chunk 50: 13,157 items so far...\n",
      "   Chunk 60: 16,266 items so far...\n",
      "   Chunk 70: 18,627 items so far...\n",
      "\n",
      "âœ… Total OrderItems retrieved: 20,608\n",
      "\n",
      "ðŸ“Š OrderItem Statistics:\n",
      "   Orders with items: 11,476\n",
      "   Min items per order: 1\n",
      "   Max items per order: 18\n",
      "   Avg items per order: 1.8\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13B: QUERY ORDER ITEMS FOR MIGRATION SCOPE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13B: QUERYING ORDER ITEMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    # Get all Order IDs from orders ready to migrate\n",
    "    order_ids = orders_ready_df[\"Id\"].unique().tolist()\n",
    "    print(f\"\\nQuerying OrderItems for {len(order_ids):,} orders...\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 150\n",
    "    all_order_items = []\n",
    "\n",
    "    for i in range(0, len(order_ids), chunk_size):\n",
    "        chunk = order_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        orderitem_query = f\"\"\"\n",
    "        SELECT \n",
    "            Id,\n",
    "            OrderId,\n",
    "            OrderItemNumber,\n",
    "            Product2Id,\n",
    "            Product_Name__c,\n",
    "            Product_Family__c,\n",
    "            Quantity,\n",
    "            UnitPrice,\n",
    "            TotalPrice,\n",
    "            ListPrice,\n",
    "            Total_MRC_Amortized__c,\n",
    "            NRC_IRU_FEE__c,\n",
    "            NRC_Non_Amortized__c,\n",
    "            Vendor_Fees_Monthly__c,\n",
    "            Vendor_NRC__c,\n",
    "            ServiceDate,\n",
    "            EndDate,\n",
    "            Contract_End_Month__c,\n",
    "            Description,\n",
    "            Bandwidth_NEW__c,\n",
    "            Bandwidth_Numerical__c,\n",
    "            Term__c,\n",
    "            Product_Service_Term__c,\n",
    "            Cancelled__c,\n",
    "            Last_Mile_Carrier__c,\n",
    "            Vendor_Last_Mile_CID__c,\n",
    "            SBQQ__ChargeType__c,\n",
    "            SBQQ__BillingFrequency__c,\n",
    "            SBQQ__Status__c,\n",
    "            OFF_NET_IDs__c,\n",
    "            CreatedDate,\n",
    "            LastModifiedDate\n",
    "        FROM OrderItem\n",
    "        WHERE OrderId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "\n",
    "        result = es_sf.query_all(orderitem_query)\n",
    "        all_order_items.extend(result[\"records\"])\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"   Chunk {i//chunk_size + 1}: {len(all_order_items):,} items so far...\"\n",
    "            )\n",
    "\n",
    "    order_items_df = pd.DataFrame(all_order_items)\n",
    "    if len(order_items_df) > 0:\n",
    "        order_items_df = order_items_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "\n",
    "    print(f\"\\nâœ… Total OrderItems retrieved: {len(order_items_df):,}\")\n",
    "\n",
    "    # Show basic stats\n",
    "    if len(order_items_df) > 0:\n",
    "        items_per_order = order_items_df.groupby(\"OrderId\").size()\n",
    "        print(f\"\\nðŸ“Š OrderItem Statistics:\")\n",
    "        print(f\"   Orders with items: {len(items_per_order):,}\")\n",
    "        print(f\"   Min items per order: {items_per_order.min()}\")\n",
    "        print(f\"   Max items per order: {items_per_order.max()}\")\n",
    "        print(f\"   Avg items per order: {items_per_order.mean():.1f}\")\n",
    "else:\n",
    "    order_items_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping OrderItem query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13C: ORDER ITEM ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Product Family Breakdown:\n",
      "   Point-to-Point (PTPS): 4,987 (24.2%)\n",
      "   Dedicated Internet Access (DIAS): 3,461 (16.8%)\n",
      "   IP: 3,323 (16.1%)\n",
      "   Hosted Voice (VOIC): 1,648 (8.0%)\n",
      "   Dark Fiber (DFBR): 1,467 (7.1%)\n",
      "   Point-to-MultiPoint (PMPS): 1,340 (6.5%)\n",
      "   Promotions: 875 (4.2%)\n",
      "   Managed Service (MSP): 771 (3.7%)\n",
      "   Handoff Type: 575 (2.8%)\n",
      "   Diversity: 497 (2.4%)\n",
      "   Additional Port: 277 (1.3%)\n",
      "   Tagged / Untagged: 234 (1.1%)\n",
      "   Logical Attributes: 230 (1.1%)\n",
      "   Routing: 125 (0.6%)\n",
      "   Managed Wave (MWAV): 106 (0.5%)\n",
      "   ... and 34 more families\n",
      "\n",
      "ðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\n",
      "   (null): 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Cancelled Status:\n",
      "   Cancelled=False: 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Financial Fields:\n",
      "   Total_MRC_Amortized__c: 20,608 populated, sum=$9,648,124.07\n",
      "   NRC_IRU_FEE__c: 8,378 populated, sum=$14,307,429.82\n",
      "   Vendor_Fees_Monthly__c: 17 populated, sum=$8,780.00\n",
      "\n",
      "ðŸ“Š OrderItem Data Quality:\n",
      "\n",
      "âœ… OrderItem analysis complete\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13C: ORDER ITEM ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13C: ORDER ITEM ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "orderitem_summary = []\n",
    "orderitem_data_quality = []\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    total_items = len(order_items_df)\n",
    "\n",
    "    # === PRODUCT FAMILY BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Product Family Breakdown:\")\n",
    "    family_counts = order_items_df[\"Product_Family__c\"].value_counts(dropna=False)\n",
    "    for family, count in family_counts.head(15).items():\n",
    "        pct = 100 * count / total_items\n",
    "        family_name = family if family else \"(null)\"\n",
    "        print(f\"   {family_name}: {count:,} ({pct:.1f}%)\")\n",
    "    if len(family_counts) > 15:\n",
    "        print(f\"   ... and {len(family_counts) - 15} more families\")\n",
    "\n",
    "    # === CHARGE TYPE BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\")\n",
    "    charge_counts = order_items_df[\"SBQQ__ChargeType__c\"].value_counts(dropna=False)\n",
    "    for charge, count in charge_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        charge_name = charge if charge else \"(null)\"\n",
    "        print(f\"   {charge_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === CANCELLED BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Cancelled Status:\")\n",
    "    cancelled_counts = order_items_df[\"Cancelled__c\"].value_counts(dropna=False)\n",
    "    for status, count in cancelled_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        status_name = str(status) if status is not None else \"(null)\"\n",
    "        print(f\"   Cancelled={status_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === MRC/NRC ANALYSIS ===\n",
    "    print(\"\\nðŸ“Š Financial Fields:\")\n",
    "\n",
    "    # Total MRC\n",
    "    mrc_populated = order_items_df[\"Total_MRC_Amortized__c\"].notna().sum()\n",
    "    mrc_sum = order_items_df[\"Total_MRC_Amortized__c\"].sum()\n",
    "    print(\n",
    "        f\"   Total_MRC_Amortized__c: {mrc_populated:,} populated, sum=${mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # NRC\n",
    "    nrc_populated = order_items_df[\"NRC_IRU_FEE__c\"].notna().sum()\n",
    "    nrc_sum = order_items_df[\"NRC_IRU_FEE__c\"].sum()\n",
    "    print(f\"   NRC_IRU_FEE__c: {nrc_populated:,} populated, sum=${nrc_sum:,.2f}\")\n",
    "\n",
    "    # Vendor costs\n",
    "    vendor_mrc_populated = order_items_df[\"Vendor_Fees_Monthly__c\"].notna().sum()\n",
    "    vendor_mrc_sum = order_items_df[\"Vendor_Fees_Monthly__c\"].sum()\n",
    "    print(\n",
    "        f\"   Vendor_Fees_Monthly__c: {vendor_mrc_populated:,} populated, sum=${vendor_mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # === DATA QUALITY CHECKS ===\n",
    "    print(\"\\nðŸ“Š OrderItem Data Quality:\")\n",
    "\n",
    "    # Missing Product Name\n",
    "    missing_product = order_items_df[\"Product_Name__c\"].isna().sum()\n",
    "    if missing_product > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Name__c\",\n",
    "                \"Count\": missing_product,\n",
    "                \"Percentage\": f\"{100*missing_product/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need product mapping for Service_Charge__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Name__c: {missing_product:,} ({100*missing_product/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Product Family\n",
    "    missing_family = order_items_df[\"Product_Family__c\"].isna().sum()\n",
    "    if missing_family > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Family__c\",\n",
    "                \"Count\": missing_family,\n",
    "                \"Percentage\": f\"{100*missing_family/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Product_Name__c as fallback\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Family__c: {missing_family:,} ({100*missing_family/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Unit Price\n",
    "    missing_price = order_items_df[\"UnitPrice\"].isna().sum()\n",
    "    if missing_price > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing UnitPrice\",\n",
    "                \"Count\": missing_price,\n",
    "                \"Percentage\": f\"{100*missing_price/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need price for Service_Charge__c.Amount__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing UnitPrice: {missing_price:,} ({100*missing_price/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Cancelled items\n",
    "    cancelled_items = order_items_df[\"Cancelled__c\"].fillna(False).sum()\n",
    "    if cancelled_items > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems marked as Cancelled\",\n",
    "                \"Count\": int(cancelled_items),\n",
    "                \"Percentage\": f\"{100*cancelled_items/total_items:.1f}%\",\n",
    "                \"Severity\": \"INFO\",\n",
    "                \"Impact\": \"Consider excluding from migration\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   â„¹ï¸ Cancelled items: {int(cancelled_items):,} ({100*cancelled_items/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing ServiceDate\n",
    "    missing_svc_date = order_items_df[\"ServiceDate\"].isna().sum()\n",
    "    if missing_svc_date > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing ServiceDate\",\n",
    "                \"Count\": missing_svc_date,\n",
    "                \"Percentage\": f\"{100*missing_svc_date/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Order-level date or OSS bill_start_date\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing ServiceDate: {missing_svc_date:,} ({100*missing_svc_date/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # === BUILD SUMMARY ===\n",
    "    orderitem_summary = [\n",
    "        {\"Metric\": \"Total OrderItems\", \"Value\": total_items},\n",
    "        {\"Metric\": \"Unique Orders\", \"Value\": order_items_df[\"OrderId\"].nunique()},\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Families\",\n",
    "            \"Value\": order_items_df[\"Product_Family__c\"].nunique(),\n",
    "        },\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Names\",\n",
    "            \"Value\": order_items_df[\"Product_Name__c\"].nunique(),\n",
    "        },\n",
    "        {\"Metric\": \"Total MRC (sum)\", \"Value\": f\"${mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total NRC (sum)\", \"Value\": f\"${nrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total Vendor MRC (sum)\", \"Value\": f\"${vendor_mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Cancelled Items\", \"Value\": int(cancelled_items)},\n",
    "        {\n",
    "            \"Metric\": \"Active Items (not cancelled)\",\n",
    "            \"Value\": total_items - int(cancelled_items),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"\\nâœ… OrderItem analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No OrderItems to analyze\")\n",
    "\n",
    "orderitem_summary_df = pd.DataFrame(orderitem_summary)\n",
    "orderitem_dq_df = pd.DataFrame(orderitem_data_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13D: PRODUCT FAMILY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Product Family Summary created: 49 families\n",
      "\n",
      "Top 10 by Item Count:\n",
      "                      Product_Family  Item_Count  Order_Count   Total_MRC\n",
      "33             Point-to-Point (PTPS)        4987         4970  3963654.84\n",
      "11  Dedicated Internet Access (DIAS)        3461         3431  2929647.50\n",
      "21                                IP        3323         3165     4473.25\n",
      "20               Hosted Voice (VOIC)        1648          417   189779.34\n",
      "6                  Dark Fiber (DFBR)        1467          771   601983.15\n",
      "32        Point-to-MultiPoint (PMPS)        1340         1340   970607.99\n",
      "37                        Promotions         875          867    -9700.00\n",
      "26             Managed Service (MSP)         771          270   126825.24\n",
      "18                      Handoff Type         575          573        0.00\n",
      "13                         Diversity         497          247       20.00\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13D: PRODUCT FAMILY SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13D: PRODUCT FAMILY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    # Create product family summary with financials\n",
    "    product_summary = (\n",
    "        order_items_df.groupby(\"Product_Family__c\", dropna=False)\n",
    "        .agg(\n",
    "            {\n",
    "                \"Id\": \"count\",\n",
    "                \"OrderId\": \"nunique\",\n",
    "                \"Total_MRC_Amortized__c\": \"sum\",\n",
    "                \"NRC_IRU_FEE__c\": \"sum\",\n",
    "                \"Vendor_Fees_Monthly__c\": \"sum\",\n",
    "                \"UnitPrice\": \"mean\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    product_summary.columns = [\n",
    "        \"Product_Family\",\n",
    "        \"Item_Count\",\n",
    "        \"Order_Count\",\n",
    "        \"Total_MRC\",\n",
    "        \"Total_NRC\",\n",
    "        \"Total_Vendor_MRC\",\n",
    "        \"Avg_UnitPrice\",\n",
    "    ]\n",
    "\n",
    "    # Sort by item count\n",
    "    product_summary = product_summary.sort_values(\"Item_Count\", ascending=False)\n",
    "\n",
    "    # Fill nulls for display\n",
    "    product_summary[\"Product_Family\"] = product_summary[\"Product_Family\"].fillna(\n",
    "        \"(null)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Product Family Summary created: {len(product_summary)} families\")\n",
    "    print(\"\\nTop 10 by Item Count:\")\n",
    "    print(\n",
    "        product_summary.head(10)[\n",
    "            [\"Product_Family\", \"Item_Count\", \"Order_Count\", \"Total_MRC\"]\n",
    "        ].to_string()\n",
    "    )\n",
    "else:\n",
    "    product_summary = pd.DataFrame()\n",
    "\n",
    "product_summary_df = product_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13E: QUERYING NODE__C RECORDS FOR MIGRATION SCOPE (v6)\n",
      "================================================================================\n",
      "\n",
      "Searching for Nodes that reference 11,476 migrating Orders...\n",
      "   Query: Node__c.Service_Order_Agreement__c IN (migrating Order IDs)\n",
      "   Query: Node__c.Service_Order_Agreement_Billing__c IN (migrating Order IDs)\n",
      "   Processed 1,500 orders... found 102 nodes so far\n",
      "   Processed 3,000 orders... found 322 nodes so far\n",
      "   Processed 4,500 orders... found 674 nodes so far\n",
      "   Processed 6,000 orders... found 1,331 nodes so far\n",
      "   Processed 7,500 orders... found 1,522 nodes so far\n",
      "   Processed 9,000 orders... found 1,626 nodes so far\n",
      "   Processed 10,500 orders... found 1,920 nodes so far\n",
      "\n",
      "âœ… Found 2,021 Node__c records referencing migrating Orders\n",
      "   - Via Service_Order_Agreement__c (Latest): 2,021\n",
      "   - Via Service_Order_Agreement_Billing__c: 398\n",
      "\n",
      "2. Building Order-Node mapping...\n",
      "   âœ… Created mapping for 2,021 Node-Order pairs\n",
      "\n",
      "ðŸ“Š Coverage Comparison:\n",
      "   Orders with Node (via Node.Service_Order_Agreement__c): 2,014\n",
      "   Orders with Node (via Order.Node__c field): 1,691\n",
      "\n",
      "ðŸ“Š Total unique Node__c records for migration: 2,021\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13E: QUERY NODE__C FOR MIGRATION SCOPE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13E: QUERYING NODE__C RECORDS FOR MIGRATION SCOPE (v6)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PRIMARY APPROACH: Query Nodes that have a lookup TO migrating Orders\n",
    "# Node__c.Service_Order_Agreement__c points to Order\n",
    "# This should have much better coverage than Order.Node__c (which was 84.8% null)\n",
    "\n",
    "nodes_df = pd.DataFrame()\n",
    "node_order_map_df = pd.DataFrame()\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    order_ids = orders_ready_df[\"Id\"].tolist()\n",
    "    print(\n",
    "        f\"\\nSearching for Nodes that reference {len(order_ids):,} migrating Orders...\"\n",
    "    )\n",
    "    print(\"   Query: Node__c.Service_Order_Agreement__c IN (migrating Order IDs)\")\n",
    "    print(\n",
    "        \"   Query: Node__c.Service_Order_Agreement_Billing__c IN (migrating Order IDs)\"\n",
    "    )\n",
    "\n",
    "    all_nodes = []\n",
    "    chunk_size = 150\n",
    "\n",
    "    for i in range(0, len(order_ids), chunk_size):\n",
    "        chunk = order_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT \n",
    "                Id,\n",
    "                Name,\n",
    "                Node_ID__c,\n",
    "                Address__c,\n",
    "                Site_Name__c,\n",
    "                Ring__c,\n",
    "                Ring_Type__c,\n",
    "                East_Neighbor__c,\n",
    "                West_Neighbor__c,\n",
    "                Distance_to_East__c,\n",
    "                Distance_to_West__c,\n",
    "                Service_Order_Agreement__c,\n",
    "                Service_Order_Agreement_Billing__c,\n",
    "                Service_ID__c,\n",
    "                Maintenance_IP_Address__c,\n",
    "                Node_Order_List__c,\n",
    "                CreatedDate,\n",
    "                LastModifiedDate\n",
    "            FROM Node__c\n",
    "            WHERE Service_Order_Agreement__c IN ('{ids_str}')\n",
    "               OR Service_Order_Agreement_Billing__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "\n",
    "        result = es_sf.query_all(query)\n",
    "        all_nodes.extend(result[\"records\"])\n",
    "\n",
    "        if (i + chunk_size) % 500 == 0:\n",
    "            print(\n",
    "                f\"   Processed {i + chunk_size:,} orders... found {len(all_nodes):,} nodes so far\"\n",
    "            )\n",
    "\n",
    "    if len(all_nodes) > 0:\n",
    "        nodes_df = pd.DataFrame(all_nodes)\n",
    "        if \"attributes\" in nodes_df.columns:\n",
    "            nodes_df = nodes_df.drop(columns=[\"attributes\"])\n",
    "        print(\n",
    "            f\"\\nâœ… Found {len(nodes_df):,} Node__c records referencing migrating Orders\"\n",
    "        )\n",
    "\n",
    "        # Show breakdown\n",
    "        soa_count = nodes_df[\"Service_Order_Agreement__c\"].notna().sum()\n",
    "        soa_billing_count = nodes_df[\"Service_Order_Agreement_Billing__c\"].notna().sum()\n",
    "        print(f\"   - Via Service_Order_Agreement__c (Latest): {soa_count:,}\")\n",
    "        print(f\"   - Via Service_Order_Agreement_Billing__c: {soa_billing_count:,}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No Node__c records found referencing migrating Orders\")\n",
    "\n",
    "    # Build Order-Node mapping for A/Z analysis\n",
    "    print(\"\\n2. Building Order-Node mapping...\")\n",
    "    if len(nodes_df) > 0:\n",
    "        mapping_data = []\n",
    "\n",
    "        for idx, node in nodes_df.iterrows():\n",
    "            # Get the Order this node references\n",
    "            order_id = node.get(\"Service_Order_Agreement__c\") or node.get(\n",
    "                \"Service_Order_Agreement_Billing__c\"\n",
    "            )\n",
    "\n",
    "            if pd.notna(order_id):\n",
    "                # Find the order in our ready list\n",
    "                order_row = orders_ready_df[orders_ready_df[\"Id\"] == order_id]\n",
    "                if len(order_row) > 0:\n",
    "                    order = order_row.iloc[0]\n",
    "                    mapping_data.append(\n",
    "                        {\n",
    "                            \"Order_Id\": order_id,\n",
    "                            \"Service_ID__c\": order.get(\"Service_ID__c\"),\n",
    "                            \"Node_Id\": node[\"Id\"],\n",
    "                            \"Node_Name\": node.get(\"Name\"),\n",
    "                            \"Node_Address__c\": node.get(\"Address__c\"),\n",
    "                            \"Node_Site_Name__c\": node.get(\"Site_Name__c\"),\n",
    "                            \"Node_Service_Order_Agreement__c\": node.get(\n",
    "                                \"Service_Order_Agreement__c\"\n",
    "                            ),\n",
    "                            \"Node_Service_Order_Agreement_Billing__c\": node.get(\n",
    "                                \"Service_Order_Agreement_Billing__c\"\n",
    "                            ),\n",
    "                            \"Order_Address_A__c\": order.get(\"Address_A__c\"),\n",
    "                            \"Order_Address_Z__c\": order.get(\"Address_Z__c\"),\n",
    "                            \"Order_Node__c\": order.get(\n",
    "                                \"Node__c\"\n",
    "                            ),  # For comparison - the reverse lookup\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if mapping_data:\n",
    "            node_order_map_df = pd.DataFrame(mapping_data)\n",
    "            print(\n",
    "                f\"   âœ… Created mapping for {len(node_order_map_df):,} Node-Order pairs\"\n",
    "            )\n",
    "\n",
    "            # Check how many orders have nodes vs Order.Node__c field\n",
    "            orders_with_nodes = node_order_map_df[\"Order_Id\"].nunique()\n",
    "            orders_with_node_field = orders_ready_df[\"Node__c\"].notna().sum()\n",
    "            print(f\"\\nðŸ“Š Coverage Comparison:\")\n",
    "            print(\n",
    "                f\"   Orders with Node (via Node.Service_Order_Agreement__c): {orders_with_nodes:,}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"   Orders with Node (via Order.Node__c field): {orders_with_node_field:,}\"\n",
    "            )\n",
    "\n",
    "node_count = len(nodes_df) if len(nodes_df) > 0 else 0\n",
    "print(f\"\\nðŸ“Š Total unique Node__c records for migration: {node_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13F: NODE A/Z DETERMINATION ANALYSIS (v6)\n",
      "================================================================================\n",
      "\n",
      "Analyzing Node address matching to determine A vs Z...\n",
      "\n",
      "1. Querying 2,076 Address__c records for comparison...\n",
      "   âœ… Retrieved 2,076 Address records\n",
      "\n",
      "2. Performing A/Z determination based on address matching...\n",
      "\n",
      "ðŸ“Š A/Z Determination Results:\n",
      "   Total Node-Order pairs analyzed: 2,021\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   A_NODE (â†’ BBF A_Node__c):    0\n",
      "   Z_NODE (â†’ BBF Z_Node__c):    1,899\n",
      "   BOTH_MATCH (same location):  122\n",
      "   NO_MATCH (review needed):    0\n",
      "   NO_NODE_ADDRESS:             0\n",
      "\n",
      "âœ… Node A/Z analysis complete\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13F: NODE A/Z DETERMINATION ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13F: NODE A/Z DETERMINATION ANALYSIS (v6)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "node_az_analysis = []\n",
    "node_az_summary = {}\n",
    "node_az_df = pd.DataFrame()\n",
    "\n",
    "if len(node_order_map_df) > 0:\n",
    "    print(\"\\nAnalyzing Node address matching to determine A vs Z...\")\n",
    "\n",
    "    # First, collect all Address IDs we need to query\n",
    "    all_addr_ids = set()\n",
    "    for idx, row in node_order_map_df.iterrows():\n",
    "        if pd.notna(row.get(\"Order_Address_A__c\")):\n",
    "            all_addr_ids.add(row[\"Order_Address_A__c\"])\n",
    "        if pd.notna(row.get(\"Order_Address_Z__c\")):\n",
    "            all_addr_ids.add(row[\"Order_Address_Z__c\"])\n",
    "\n",
    "    print(f\"\\n1. Querying {len(all_addr_ids):,} Address__c records for comparison...\")\n",
    "\n",
    "    addr_lookup = {}\n",
    "    if all_addr_ids:\n",
    "        addr_list = list(all_addr_ids)\n",
    "        chunk_size = 150\n",
    "\n",
    "        for i in range(0, len(addr_list), chunk_size):\n",
    "            chunk = addr_list[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT Id, Name, Address__c, City__c, State__c, Zip__c, Complete_Address__c\n",
    "                FROM Address__c\n",
    "                WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(query)\n",
    "            for rec in result[\"records\"]:\n",
    "                addr_lookup[rec[\"Id\"]] = {\n",
    "                    \"Name\": rec.get(\"Name\", \"\"),\n",
    "                    \"Address__c\": rec.get(\"Address__c\", \"\"),\n",
    "                    \"City__c\": rec.get(\"City__c\", \"\"),\n",
    "                    \"State__c\": rec.get(\"State__c\", \"\"),\n",
    "                    \"Zip__c\": rec.get(\"Zip__c\", \"\"),\n",
    "                    \"Complete_Address__c\": rec.get(\"Complete_Address__c\", \"\"),\n",
    "                }\n",
    "\n",
    "        print(f\"   âœ… Retrieved {len(addr_lookup):,} Address records\")\n",
    "\n",
    "    # Now perform A/Z matching\n",
    "    print(\"\\n2. Performing A/Z determination based on address matching...\")\n",
    "\n",
    "    a_match_count = 0\n",
    "    z_match_count = 0\n",
    "    both_match_count = 0\n",
    "    no_match_count = 0\n",
    "    no_node_address_count = 0\n",
    "\n",
    "    for idx, row in node_order_map_df.iterrows():\n",
    "        node_addr = (\n",
    "            str(row.get(\"Node_Address__c\", \"\")).strip().lower()\n",
    "            if pd.notna(row.get(\"Node_Address__c\"))\n",
    "            else \"\"\n",
    "        )\n",
    "        node_site = (\n",
    "            str(row.get(\"Node_Site_Name__c\", \"\")).strip().lower()\n",
    "            if pd.notna(row.get(\"Node_Site_Name__c\"))\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        addr_a_id = row.get(\"Order_Address_A__c\")\n",
    "        addr_z_id = row.get(\"Order_Address_Z__c\")\n",
    "\n",
    "        addr_a_info = addr_lookup.get(addr_a_id, {}) if pd.notna(addr_a_id) else {}\n",
    "        addr_z_info = addr_lookup.get(addr_z_id, {}) if pd.notna(addr_z_id) else {}\n",
    "\n",
    "        # Build comparison strings\n",
    "        addr_a_str = (\n",
    "            \" \".join(\n",
    "                [\n",
    "                    str(addr_a_info.get(\"Address__c\", \"\")),\n",
    "                    str(addr_a_info.get(\"City__c\", \"\")),\n",
    "                    str(addr_a_info.get(\"State__c\", \"\")),\n",
    "                    str(addr_a_info.get(\"Complete_Address__c\", \"\")),\n",
    "                ]\n",
    "            )\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "\n",
    "        addr_z_str = (\n",
    "            \" \".join(\n",
    "                [\n",
    "                    str(addr_z_info.get(\"Address__c\", \"\")),\n",
    "                    str(addr_z_info.get(\"City__c\", \"\")),\n",
    "                    str(addr_z_info.get(\"State__c\", \"\")),\n",
    "                    str(addr_z_info.get(\"Complete_Address__c\", \"\")),\n",
    "                ]\n",
    "            )\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "\n",
    "        # Determine A/Z\n",
    "        az_determination = \"UNKNOWN\"\n",
    "        match_reason = \"\"\n",
    "\n",
    "        if not node_addr and not node_site:\n",
    "            az_determination = \"NO_NODE_ADDRESS\"\n",
    "            match_reason = \"Node has no Address__c or Site_Name__c\"\n",
    "            no_node_address_count += 1\n",
    "        else:\n",
    "            a_match = False\n",
    "            z_match = False\n",
    "\n",
    "            # Check node address against A and Z\n",
    "            if node_addr:\n",
    "                # Check for partial match in either direction\n",
    "                if addr_a_str and (\n",
    "                    node_addr in addr_a_str\n",
    "                    or any(\n",
    "                        part in node_addr\n",
    "                        for part in addr_a_str.split()\n",
    "                        if len(part) > 3\n",
    "                    )\n",
    "                ):\n",
    "                    a_match = True\n",
    "                if addr_z_str and (\n",
    "                    node_addr in addr_z_str\n",
    "                    or any(\n",
    "                        part in node_addr\n",
    "                        for part in addr_z_str.split()\n",
    "                        if len(part) > 3\n",
    "                    )\n",
    "                ):\n",
    "                    z_match = True\n",
    "\n",
    "            # Also check site name\n",
    "            if node_site:\n",
    "                if addr_a_str and (\n",
    "                    node_site in addr_a_str\n",
    "                    or any(\n",
    "                        part in node_site\n",
    "                        for part in addr_a_str.split()\n",
    "                        if len(part) > 3\n",
    "                    )\n",
    "                ):\n",
    "                    a_match = True\n",
    "                if addr_z_str and (\n",
    "                    node_site in addr_z_str\n",
    "                    or any(\n",
    "                        part in node_site\n",
    "                        for part in addr_z_str.split()\n",
    "                        if len(part) > 3\n",
    "                    )\n",
    "                ):\n",
    "                    z_match = True\n",
    "\n",
    "            if a_match and z_match:\n",
    "                az_determination = \"BOTH_MATCH\"\n",
    "                match_reason = \"Node address matches both A and Z locations\"\n",
    "                both_match_count += 1\n",
    "            elif a_match:\n",
    "                az_determination = \"A_NODE\"\n",
    "                match_reason = \"Node address matches Address A\"\n",
    "                a_match_count += 1\n",
    "            elif z_match:\n",
    "                az_determination = \"Z_NODE\"\n",
    "                match_reason = \"Node address matches Address Z\"\n",
    "                z_match_count += 1\n",
    "            else:\n",
    "                az_determination = \"NO_MATCH\"\n",
    "                match_reason = \"No address match - manual review needed\"\n",
    "                no_match_count += 1\n",
    "\n",
    "        node_az_analysis.append(\n",
    "            {\n",
    "                \"Order_Id\": row[\"Order_Id\"],\n",
    "                \"Service_ID__c\": row[\"Service_ID__c\"],\n",
    "                \"Node_Id\": row[\"Node_Id\"],\n",
    "                \"Node_Name\": row[\"Node_Name\"],\n",
    "                \"Node_Address__c\": row.get(\"Node_Address__c\"),\n",
    "                \"Node_Site_Name__c\": row.get(\"Node_Site_Name__c\"),\n",
    "                \"Order_Address_A_Id\": addr_a_id,\n",
    "                \"Order_Address_Z_Id\": addr_z_id,\n",
    "                \"Address_A_Street\": addr_a_info.get(\"Address__c\", \"\"),\n",
    "                \"Address_A_City\": addr_a_info.get(\"City__c\", \"\"),\n",
    "                \"Address_Z_Street\": addr_z_info.get(\"Address__c\", \"\"),\n",
    "                \"Address_Z_City\": addr_z_info.get(\"City__c\", \"\"),\n",
    "                \"AZ_Determination\": az_determination,\n",
    "                \"Match_Reason\": match_reason,\n",
    "                \"Order_Node__c_Field\": row.get(\"Order_Node__c\"),  # For comparison\n",
    "            }\n",
    "        )\n",
    "\n",
    "    node_az_df = pd.DataFrame(node_az_analysis)\n",
    "\n",
    "    print(f\"\\nðŸ“Š A/Z Determination Results:\")\n",
    "    print(f\"   Total Node-Order pairs analyzed: {len(node_az_df):,}\")\n",
    "    print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"   A_NODE (â†’ BBF A_Node__c):    {a_match_count:,}\")\n",
    "    print(f\"   Z_NODE (â†’ BBF Z_Node__c):    {z_match_count:,}\")\n",
    "    print(f\"   BOTH_MATCH (same location):  {both_match_count:,}\")\n",
    "    print(f\"   NO_MATCH (review needed):    {no_match_count:,}\")\n",
    "    print(f\"   NO_NODE_ADDRESS:             {no_node_address_count:,}\")\n",
    "\n",
    "    node_az_summary = {\n",
    "        \"total_pairs\": len(node_az_df),\n",
    "        \"a_node\": a_match_count,\n",
    "        \"z_node\": z_match_count,\n",
    "        \"both_match\": both_match_count,\n",
    "        \"no_match\": no_match_count,\n",
    "        \"no_node_address\": no_node_address_count,\n",
    "    }\n",
    "\n",
    "    # Show sample of NO_MATCH for manual review\n",
    "    if no_match_count > 0:\n",
    "        print(f\"\\n   Sample NO_MATCH records (first 5):\")\n",
    "        no_match_sample = node_az_df[node_az_df[\"AZ_Determination\"] == \"NO_MATCH\"].head(\n",
    "            5\n",
    "        )\n",
    "        for idx, row in no_match_sample.iterrows():\n",
    "            print(f\"      Node: {row['Node_Name']} | Addr: {row['Node_Address__c']}\")\n",
    "            print(\n",
    "                f\"         Order A: {row['Address_A_Street']}, {row['Address_A_City']}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"         Order Z: {row['Address_Z_Street']}, {row['Address_Z_City']}\"\n",
    "            )\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No Node-Order mappings to analyze\")\n",
    "\n",
    "print(f\"\\nâœ… Node A/Z analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 14: CREATING BAN MAPPING TABLE\n",
      "================================================================================\n",
      "\n",
      "âœ… BAN mappings with orders: 2,441\n"
     ]
    }
   ],
   "source": [
    "# === STEP 14: CREATE BAN MAPPING TABLE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 14: CREATING BAN MAPPING TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    ban_mapping_data = []\n",
    "    legacy_ban_counts = orders_ready_df.groupby(\"Billing_Invoice__c\").size().to_dict()\n",
    "\n",
    "    for legacy_id, bbf_ban in legacy_to_bbf_ban.items():\n",
    "        order_count = legacy_ban_counts.get(legacy_id, 0)\n",
    "        if order_count > 0:\n",
    "            ban_mapping_data.append(\n",
    "                {\n",
    "                    \"Legacy_BAN_Id\": legacy_id,\n",
    "                    \"New_BBF_BAN_Id\": bbf_ban[\"Id\"],\n",
    "                    \"New_BBF_BAN_Name\": bbf_ban[\"Name\"],\n",
    "                    \"Account__c\": bbf_ban[\"Account__c\"],\n",
    "                    \"Account_Name\": bbf_ban[\"Account_Name\"],\n",
    "                    \"Order_Count\": order_count,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    ban_mapping_df = pd.DataFrame(ban_mapping_data)\n",
    "    ban_mapping_df = ban_mapping_df.sort_values(\"Order_Count\", ascending=False)\n",
    "    print(f\"\\nâœ… BAN mappings with orders: {len(ban_mapping_df):,}\")\n",
    "else:\n",
    "    ban_mapping_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 15: GENERATING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š MIGRATION SCOPE SUMMARY:\n",
      "   Category                          Metric  Count                                          Notes\n",
      "     ORDERS      Total Active Status Orders  17980              All orders with qualifying status\n",
      "     ORDERS      Excluded - PA MARKET DECOM    887                 Orders in decommission project\n",
      "     ORDERS          Excluded - Work Orders   3484         Record Type != Service Order Agreement\n",
      "     ORDERS Excluded - Not Actively Billing   2119 OSS state not in ['CL', 'OA'] or dates invalid\n",
      "     ORDERS      Confirmed Actively Billing  11490                             Passed all filters\n",
      "     ORDERS             Ready (has BBF BAN)  11476                            Can be migrated now\n",
      "     ORDERS                 Missing BBF BAN     12                 Need new BBF BAN created first\n",
      "     ORDERS                 Missing ANY BAN      2                  CRITICAL - no BAN association\n",
      "ORDER ITEMS                Total OrderItems  20608                    For orders ready to migrate\n",
      "ORDER ITEMS         Unique Product Families     49                    Distinct product categories\n",
      "       BANS          Unique BANs to Migrate   2441                     Billing_Invoice__c records\n",
      "   ACCOUNTS             Accounts to Migrate   2225                   Unique from BANs with orders\n",
      "   CONTACTS             Contacts to Migrate  15581             Associated with migration accounts\n",
      "  LOCATIONS            Locations to Migrate  10176                       Address__c â†’ Location__c\n",
      "      NODES                Nodes to Migrate   2021           Node__c records referenced by orders\n",
      "      NODES             Node-Order Mappings   2021              Order-Node pairs for A/Z analysis\n",
      "    OFF-NET                 Off-Net Records   2157                    Carrier/vendor circuit info\n",
      "   NODE A/Z            Determined as A_Node      0           Node address matches Order Address A\n",
      "   NODE A/Z            Determined as Z_Node   1899           Node address matches Order Address Z\n",
      "   NODE A/Z                      Both Match    122                      Node matches both A and Z\n",
      "   NODE A/Z               No Match (Review)      0                           Manual review needed\n",
      "   NODE A/Z                 No Node Address      0                 Node has no address to compare\n"
     ]
    }
   ],
   "source": [
    "# === STEP 15: GENERATE SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 15: GENERATING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build summary data\n",
    "summary_data = [\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Total Active Status Orders\",\n",
    "        \"Count\": len(all_orders_df),\n",
    "        \"Notes\": \"All orders with qualifying status\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Excluded - PA MARKET DECOM\",\n",
    "        \"Count\": len(excluded_pa_decom_df),\n",
    "        \"Notes\": \"Orders in decommission project\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Excluded - Work Orders\",\n",
    "        \"Count\": len(excluded_work_orders_df),\n",
    "        \"Notes\": \"Record Type != Service Order Agreement\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Excluded - Not Actively Billing\",\n",
    "        \"Count\": len(excluded_not_billing_df),\n",
    "        \"Notes\": f\"OSS state not in {ACTIVE_OSS_STATES} or dates invalid\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Confirmed Actively Billing\",\n",
    "        \"Count\": len(active_orders_df),\n",
    "        \"Notes\": \"Passed all filters\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Ready (has BBF BAN)\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"Can be migrated now\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Missing BBF BAN\",\n",
    "        \"Count\": len(orders_no_bbf_ban_df),\n",
    "        \"Notes\": \"Need new BBF BAN created first\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDERS\",\n",
    "        \"Metric\": \"Missing ANY BAN\",\n",
    "        \"Count\": len(orders_no_ban_df),\n",
    "        \"Notes\": \"CRITICAL - no BAN association\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDER ITEMS\",\n",
    "        \"Metric\": \"Total OrderItems\",\n",
    "        \"Count\": (\n",
    "            len(order_items_df)\n",
    "            if \"order_items_df\" in dir() and len(order_items_df) > 0\n",
    "            else 0\n",
    "        ),\n",
    "        \"Notes\": \"For orders ready to migrate\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ORDER ITEMS\",\n",
    "        \"Metric\": \"Unique Product Families\",\n",
    "        \"Count\": (\n",
    "            len(product_summary_df)\n",
    "            if \"product_summary_df\" in dir() and len(product_summary_df) > 0\n",
    "            else 0\n",
    "        ),\n",
    "        \"Notes\": \"Distinct product categories\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"BANS\",\n",
    "        \"Metric\": \"Unique BANs to Migrate\",\n",
    "        \"Count\": len(ban_mapping_df),\n",
    "        \"Notes\": \"Billing_Invoice__c records\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"ACCOUNTS\",\n",
    "        \"Metric\": \"Accounts to Migrate\",\n",
    "        \"Count\": len(accounts_df),\n",
    "        \"Notes\": \"Unique from BANs with orders\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"CONTACTS\",\n",
    "        \"Metric\": \"Contacts to Migrate\",\n",
    "        \"Count\": len(contacts_df),\n",
    "        \"Notes\": \"Associated with migration accounts\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"LOCATIONS\",\n",
    "        \"Metric\": \"Locations to Migrate\",\n",
    "        \"Count\": len(locations_df),\n",
    "        \"Notes\": \"Address__c â†’ Location__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"NODES\",\n",
    "        \"Metric\": \"Nodes to Migrate\",\n",
    "        \"Count\": len(nodes_df) if \"nodes_df\" in dir() and len(nodes_df) > 0 else 0,\n",
    "        \"Notes\": \"Node__c records referenced by orders\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"NODES\",\n",
    "        \"Metric\": \"Node-Order Mappings\",\n",
    "        \"Count\": (\n",
    "            len(node_order_map_df)\n",
    "            if \"node_order_map_df\" in dir() and len(node_order_map_df) > 0\n",
    "            else 0\n",
    "        ),\n",
    "        \"Notes\": \"Order-Node pairs for A/Z analysis\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"OFF-NET\",\n",
    "        \"Metric\": \"Off-Net Records\",\n",
    "        \"Count\": len(offnet_df),\n",
    "        \"Notes\": \"Carrier/vendor circuit info\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Add Node A/Z summary if available\n",
    "if \"node_az_summary\" in dir() and node_az_summary:\n",
    "    summary_data.extend(\n",
    "        [\n",
    "            {\n",
    "                \"Category\": \"NODE A/Z\",\n",
    "                \"Metric\": \"Determined as A_Node\",\n",
    "                \"Count\": node_az_summary.get(\"a_node\", 0),\n",
    "                \"Notes\": \"Node address matches Order Address A\",\n",
    "            },\n",
    "            {\n",
    "                \"Category\": \"NODE A/Z\",\n",
    "                \"Metric\": \"Determined as Z_Node\",\n",
    "                \"Count\": node_az_summary.get(\"z_node\", 0),\n",
    "                \"Notes\": \"Node address matches Order Address Z\",\n",
    "            },\n",
    "            {\n",
    "                \"Category\": \"NODE A/Z\",\n",
    "                \"Metric\": \"Both Match\",\n",
    "                \"Count\": node_az_summary.get(\"both_match\", 0),\n",
    "                \"Notes\": \"Node matches both A and Z\",\n",
    "            },\n",
    "            {\n",
    "                \"Category\": \"NODE A/Z\",\n",
    "                \"Metric\": \"No Match (Review)\",\n",
    "                \"Count\": node_az_summary.get(\"no_match\", 0),\n",
    "                \"Notes\": \"Manual review needed\",\n",
    "            },\n",
    "            {\n",
    "                \"Category\": \"NODE A/Z\",\n",
    "                \"Metric\": \"No Node Address\",\n",
    "                \"Count\": node_az_summary.get(\"no_node_address\", 0),\n",
    "                \"Notes\": \"Node has no address to compare\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nðŸ“Š MIGRATION SCOPE SUMMARY:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 16: EXPORTING TO EXCEL\n",
      "================================================================================\n",
      "   âœ… Legend\n",
      "   âœ… Summary\n",
      "   âœ… Active_Orders (11,476 records)\n",
      "   âœ… OrderItems (20,608 records)\n",
      "   âœ… OrderItem_Summary\n",
      "   âœ… Product_Family_Summary\n",
      "   âœ… BAN_Mapping (2,441 records)\n",
      "   âœ… Accounts (2,225 records)\n",
      "   âœ… Contacts (15,581 records)\n",
      "   âœ… Locations (10,176 records)\n",
      "   âœ… Nodes (2,021 records)\n",
      "   âœ… Node_AZ_Analysis (2,021 records)\n",
      "   âœ… Off_Net (2,157 records)\n",
      "   âœ… Data_Quality (3 issues)\n",
      "   âœ… Orders_Missing_BBF_BAN (12 records)\n",
      "   âœ… Excluded_PA_DECOM (887 records)\n",
      "   âœ… Excluded_Work_Orders (3,484 records)\n",
      "   âœ… Excluded_Not_Billing (2,119 records)\n",
      "\n",
      "   Creating Field Reference sheets...\n",
      "   âœ… Fields_Order\n",
      "   âœ… Fields_Node (v6 NEW)\n",
      "   âœ… Fields_OrderItem\n",
      "   âœ… Fields_BAN\n",
      "   âœ… Fields_Account\n",
      "   âœ… Fields_Contact\n",
      "   âœ… Fields_Location\n",
      "\n",
      "âœ… Excel file saved: es_bbf_migration_analysis_v6_20260108_121628.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === STEP 16: EXPORT TO EXCEL ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 16: EXPORTING TO EXCEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "wb = Workbook()\n",
    "\n",
    "# Styles\n",
    "header_font = Font(bold=True, size=12, color=\"FFFFFF\")\n",
    "header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "section_fill = PatternFill(start_color=\"1F4E79\", end_color=\"1F4E79\", fill_type=\"solid\")\n",
    "section_font = Font(bold=True, size=11, color=\"FFFFFF\")\n",
    "bold_font = Font(bold=True)\n",
    "thin_border = Border(\n",
    "    left=Side(style=\"thin\"),\n",
    "    right=Side(style=\"thin\"),\n",
    "    top=Side(style=\"thin\"),\n",
    "    bottom=Side(style=\"thin\"),\n",
    ")\n",
    "\n",
    "\n",
    "def write_df_to_sheet(ws, df, start_row=1):\n",
    "    \"\"\"Write dataframe to worksheet with formatting\"\"\"\n",
    "    for r_idx, row in enumerate(\n",
    "        dataframe_to_rows(df, index=False, header=True), start=start_row\n",
    "    ):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == start_row:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            cell.border = thin_border\n",
    "\n",
    "\n",
    "def write_section_header(ws, row, text):\n",
    "    \"\"\"Write a section header with dark blue background spanning columns A-E\"\"\"\n",
    "    ws.merge_cells(f\"A{row}:E{row}\")\n",
    "    cell = ws[f\"A{row}\"]\n",
    "    cell.value = text\n",
    "    cell.font = section_font\n",
    "    cell.fill = section_fill\n",
    "    return row + 1\n",
    "\n",
    "\n",
    "def write_label_value(ws, row, label, value, col_c=None, col_d=None):\n",
    "    \"\"\"Write a label:value pair\"\"\"\n",
    "    ws[f\"A{row}\"] = label\n",
    "    ws[f\"A{row}\"].font = bold_font\n",
    "    ws[f\"B{row}\"] = value\n",
    "    if col_c:\n",
    "        ws[f\"C{row}\"] = col_c\n",
    "    if col_d:\n",
    "        ws[f\"D{row}\"] = col_d\n",
    "    return row + 1\n",
    "\n",
    "\n",
    "def write_table_header(ws, row, headers):\n",
    "    \"\"\"Write table header row with bold text\"\"\"\n",
    "    for i, h in enumerate(headers):\n",
    "        col = chr(65 + i)\n",
    "        ws[f\"{col}{row}\"] = h\n",
    "        ws[f\"{col}{row}\"].font = bold_font\n",
    "    return row + 1\n",
    "\n",
    "\n",
    "def write_table_row(ws, row, values):\n",
    "    \"\"\"Write a table data row\"\"\"\n",
    "    for i, v in enumerate(values):\n",
    "        col = chr(65 + i)\n",
    "        ws[f\"{col}{row}\"] = v\n",
    "    return row + 1\n",
    "\n",
    "\n",
    "# Get counts\n",
    "orderitem_count = (\n",
    "    len(order_items_df) if \"order_items_df\" in dir() and len(order_items_df) > 0 else 0\n",
    ")\n",
    "node_count = len(nodes_df) if \"nodes_df\" in dir() and len(nodes_df) > 0 else 0\n",
    "node_az_count = len(node_az_df) if \"node_az_df\" in dir() and len(node_az_df) > 0 else 0\n",
    "\n",
    "# === LEGEND SHEET ===\n",
    "ws = wb.active\n",
    "ws.title = \"Legend\"\n",
    "\n",
    "row = 1\n",
    "ws[\"A1\"] = \"ES â†’ BBF Migration Analysis Legend\"\n",
    "ws[\"A1\"].font = Font(bold=True, size=14)\n",
    "row = 3\n",
    "\n",
    "# DOCUMENT OVERVIEW\n",
    "row = write_section_header(ws, row, \"DOCUMENT OVERVIEW\")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Purpose:\",\n",
    "    \"Comprehensive analysis of ES Salesforce data required for migration to BBF Salesforce\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Driving Principle:\",\n",
    "    \"Everything is driven from Active ES Orders - only data needed to support active services is migrated\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Source:\",\n",
    "    \"es_bbf_migration_data_analysis_v6 notebook run against ES Salesforce production\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws, row, \"Generated:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "row += 1\n",
    "\n",
    "# SHEET DESCRIPTIONS\n",
    "row = write_section_header(ws, row, \"SHEET DESCRIPTIONS\")\n",
    "row = write_table_header(\n",
    "    ws, row, [\"Sheet Name\", \"Record Count\", \"Purpose\", \"Key Columns\"]\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Summary\",\n",
    "        \"~30 metrics\",\n",
    "        \"High-level overview of migration scope with counts by category\",\n",
    "        \"Category, Metric, Count, Notes\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Active_Orders\",\n",
    "        len(orders_ready_df),\n",
    "        \"ES Orders ready for migration (have BBF BAN)\",\n",
    "        \"Id, Service_ID__c, Status, New_BBF_BAN_Id, SOF_MRC__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"BAN_Mapping\",\n",
    "        len(ban_mapping_df),\n",
    "        \"Mapping from ES Billing_Invoice__c to new BBF BAN__c records\",\n",
    "        \"BAN_Id, New_BBF_BAN_Id, Account__c, Order_Count\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Accounts\",\n",
    "        len(accounts_df),\n",
    "        \"ES Accounts to migrate (unique from BANs with orders)\",\n",
    "        \"Id, Name, Type, Industry, BBF_New_Id__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Contacts\",\n",
    "        len(contacts_df),\n",
    "        \"ES Contacts associated with migration accounts\",\n",
    "        \"Id, AccountId, Name, Email, Phone, BBF_New_Id__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Locations\",\n",
    "        len(locations_df),\n",
    "        \"ES Address__c records (A and Z endpoints from orders)\",\n",
    "        \"Id, Name, Address__c, City__c, State__c, CLLI__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Nodes\",\n",
    "        node_count,\n",
    "        \"ES Node__c where Service_Order_Agreement__c = migrating Order (v6)\",\n",
    "        \"Id, Name, Node_ID__c, Address__c, Ring__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Node_AZ_Analysis\",\n",
    "        node_az_count,\n",
    "        \"Analysis of which Node maps to A vs Z (v6 NEW)\",\n",
    "        \"Order_Id, Node_Id, AZ_Determination\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Off_Net\",\n",
    "        len(offnet_df),\n",
    "        \"Off_Net__c carrier/vendor records for migration locations\",\n",
    "        \"Id, Location fields, Vendor info, Cost_MRC__c\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"OrderItems\",\n",
    "        orderitem_count,\n",
    "        \"ES OrderItems for orders ready to migrate\",\n",
    "        \"Id, OrderId, Product fields, UnitPrice, MRC, NRC\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Data_Quality\",\n",
    "        \"issues\",\n",
    "        \"Data quality issues affecting migration\",\n",
    "        \"Issue, Count, Percentage, Severity, Impact\",\n",
    "    ],\n",
    ")\n",
    "row += 1\n",
    "\n",
    "# ACTIVE ORDER CRITERIA\n",
    "row = write_section_header(ws, row, \"ACTIVE ORDER CRITERIA\")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Included Statuses:\",\n",
    "    \"Activated, Disconnect in Progress, Suspended (Late Payment)\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws, row, \"Excluded:\", \"Orders where Project_Group__c contains 'PA MARKET DECOM'\"\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Requirement:\",\n",
    "    \"Must have Billing_Invoice__c linked to a BBF_Ban__c = true record\",\n",
    ")\n",
    "row += 1\n",
    "\n",
    "# MIGRATION SCOPE BREAKDOWN\n",
    "row = write_section_header(ws, row, \"MIGRATION SCOPE BREAKDOWN\")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Total Active Status Orders:\",\n",
    "    len(all_orders_df),\n",
    "    \"All orders with qualifying status\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Excluded (PA MARKET DECOM):\",\n",
    "    len(excluded_pa_decom_df),\n",
    "    \"Orders in decommission project - not migrated\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Excluded (Work Orders):\",\n",
    "    len(excluded_work_orders_df),\n",
    "    \"Service_Order_Record_Type__c != Service Order Agreement\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Excluded (Not Actively Billing):\",\n",
    "    len(excluded_not_billing_df),\n",
    "    f\"OSS: not in {ACTIVE_OSS_STATES} or billing dates invalid\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"In Scope for Migration:\",\n",
    "    len(active_orders_df),\n",
    "    \"Active orders not in PA MARKET DECOM\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws, row, \"Ready (have BBF BAN):\", len(orders_ready_df), \"Can be migrated now\"\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Missing BBF BAN:\",\n",
    "    len(orders_no_bbf_ban_df),\n",
    "    \"Need new BBF BAN created first\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"Missing ANY BAN:\",\n",
    "    len(orders_no_ban_df),\n",
    "    \"CRITICAL - cannot migrate, no BAN association\",\n",
    ")\n",
    "row += 1\n",
    "\n",
    "# NODE A/Z ANALYSIS (v6 NEW)\n",
    "row = write_section_header(ws, row, \"NODE A/Z ANALYSIS (v6 NEW)\")\n",
    "if \"node_az_summary\" in dir() and node_az_summary:\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"Total Node-Order Pairs:\",\n",
    "        node_az_summary.get(\"total_pairs\", 0),\n",
    "        \"Orders with Node__c reference\",\n",
    "    )\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"Determined A_Node:\",\n",
    "        node_az_summary.get(\"a_node\", 0),\n",
    "        \"Node address matches Order Address A â†’ map to BBF A_Node__c\",\n",
    "    )\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"Determined Z_Node:\",\n",
    "        node_az_summary.get(\"z_node\", 0),\n",
    "        \"Node address matches Order Address Z â†’ map to BBF Z_Node__c\",\n",
    "    )\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"Both Match:\",\n",
    "        node_az_summary.get(\"both_match\", 0),\n",
    "        \"Node matches both A and Z addresses\",\n",
    "    )\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"No Match (Review):\",\n",
    "        node_az_summary.get(\"no_match\", 0),\n",
    "        \"Manual review needed\",\n",
    "    )\n",
    "    row = write_label_value(\n",
    "        ws,\n",
    "        row,\n",
    "        \"No Node Address:\",\n",
    "        node_az_summary.get(\"no_node_address\", 0),\n",
    "        \"Node has no address to compare\",\n",
    "    )\n",
    "else:\n",
    "    row = write_label_value(ws, row, \"Status:\", \"No Node data analyzed\", \"\")\n",
    "row += 1\n",
    "\n",
    "# DATA QUALITY ISSUES\n",
    "row = write_section_header(ws, row, \"DATA QUALITY ISSUES\")\n",
    "row = write_table_header(ws, row, [\"Severity\", \"Issue\", \"Count\", \"Impact\"])\n",
    "for issue in data_quality_issues:\n",
    "    row = write_table_row(\n",
    "        ws,\n",
    "        row,\n",
    "        [\n",
    "            issue.get(\"Severity\", \"\"),\n",
    "            issue.get(\"Issue\", \"\"),\n",
    "            issue.get(\"Percentage\", \"\"),\n",
    "            issue.get(\"Impact\", \"\"),\n",
    "        ],\n",
    "    )\n",
    "row += 1\n",
    "\n",
    "# KEY TRACKING FIELDS\n",
    "row = write_section_header(ws, row, \"KEY TRACKING FIELDS\")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"BBF_New_Id__c\",\n",
    "    \"On ES records\",\n",
    "    \"Stores the BBF Salesforce Id after successful migration\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"New_BBF_BAN_Id\",\n",
    "    \"In Active_Orders\",\n",
    "    \"The BBF BAN__c Id that will be the parent for the migrated service\",\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws, row, \"Legacy_BAN_Id\", \"In BAN_Mapping\", \"Original ES Billing_Invoice__c Id\"\n",
    ")\n",
    "row = write_label_value(\n",
    "    ws,\n",
    "    row,\n",
    "    \"ES_Legacy_ID__c\",\n",
    "    \"On BBF records\",\n",
    "    \"Stores the original ES Salesforce Id for traceability\",\n",
    ")\n",
    "row += 1\n",
    "\n",
    "# ES â†’ BBF OBJECT MAPPING\n",
    "row = write_section_header(ws, row, \"ES â†’ BBF OBJECT MAPPING\")\n",
    "row = write_table_header(ws, row, [\"ES Object\", \"BBF Object\", \"Notes\", \"\"])\n",
    "row = write_table_row(\n",
    "    ws, row, [\"Account\", \"Account\", \"Direct migration with field mapping\", \"\"]\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws, row, [\"Contact\", \"Contact\", \"Direct migration, requires Account parent\", \"\"]\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws, row, [\"Billing_Invoice__c\", \"BAN__c\", \"Custom object to custom object\", \"\"]\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws, row, [\"Address__c\", \"Location__c\", \"Custom object to custom object\", \"\"]\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"Node__c\",\n",
    "        \"Node__c\",\n",
    "        \"Query via Node.Service_Order_Agreement__c â†’ Order (v6)\",\n",
    "        \"\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\"Order\", \"Service__c\", \"Standard to custom - data transformation required\", \"\"],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws,\n",
    "    row,\n",
    "    [\n",
    "        \"OrderItem\",\n",
    "        \"Service_Charge__c\",\n",
    "        \"Standard to custom - data transformation required\",\n",
    "        \"\",\n",
    "    ],\n",
    ")\n",
    "row = write_table_row(\n",
    "    ws, row, [\"Off_Net__c\", \"Off_Net__c\", \"Same object name, field mapping needed\", \"\"]\n",
    ")\n",
    "\n",
    "# Column widths\n",
    "ws.column_dimensions[\"A\"].width = 28\n",
    "ws.column_dimensions[\"B\"].width = 18\n",
    "ws.column_dimensions[\"C\"].width = 60\n",
    "ws.column_dimensions[\"D\"].width = 50\n",
    "\n",
    "print(\"   âœ… Legend\")\n",
    "\n",
    "# --- SHEET 2: Summary ---\n",
    "ws_summary = wb.create_sheet(\"Summary\")\n",
    "ws_summary.append([\"ES â†’ BBF Migration Analysis (v6)\"])\n",
    "ws_summary[\"A1\"].font = Font(bold=True, size=16)\n",
    "ws_summary.append([f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"])\n",
    "ws_summary.append([f\"Version 6: Added Node__c analysis with A/Z determination\"])\n",
    "ws_summary.append([])\n",
    "write_df_to_sheet(ws_summary, summary_df, start_row=5)\n",
    "\n",
    "ws_summary.column_dimensions[\"A\"].width = 15\n",
    "ws_summary.column_dimensions[\"B\"].width = 35\n",
    "ws_summary.column_dimensions[\"C\"].width = 12\n",
    "ws_summary.column_dimensions[\"D\"].width = 55\n",
    "print(\"   âœ… Summary\")\n",
    "\n",
    "# --- SHEET 3: Active Orders (Ready) ---\n",
    "if len(orders_ready_df) > 0:\n",
    "    ws = wb.create_sheet(\"Active_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"New_BBF_BAN_Id\",\n",
    "        \"New_BBF_BAN_Name\",\n",
    "        \"Address_A__c\",\n",
    "        \"Address_Z__c\",\n",
    "        \"Node__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "        \"Service_Start_Date__c\",\n",
    "        \"Billing_Start_Date__c\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_ready_df.columns]\n",
    "    write_df_to_sheet(ws, orders_ready_df[export_cols])\n",
    "    print(f\"   âœ… Active_Orders ({len(orders_ready_df):,} records)\")\n",
    "\n",
    "# --- SHEET 4: Order Items ---\n",
    "if \"order_items_df\" in dir() and len(order_items_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItems\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"OrderId\",\n",
    "        \"OrderItemNumber\",\n",
    "        \"Product_Name__c\",\n",
    "        \"Product_Family__c\",\n",
    "        \"Quantity\",\n",
    "        \"UnitPrice\",\n",
    "        \"TotalPrice\",\n",
    "        \"Total_MRC_Amortized__c\",\n",
    "        \"NRC_IRU_FEE__c\",\n",
    "        \"Vendor_Fees_Monthly__c\",\n",
    "        \"ServiceDate\",\n",
    "        \"EndDate\",\n",
    "        \"Bandwidth_NEW__c\",\n",
    "        \"Term__c\",\n",
    "        \"Cancelled__c\",\n",
    "        \"SBQQ__ChargeType__c\",\n",
    "        \"Last_Mile_Carrier__c\",\n",
    "        \"OFF_NET_IDs__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in order_items_df.columns]\n",
    "    write_df_to_sheet(ws, order_items_df[export_cols])\n",
    "    print(f\"   âœ… OrderItems ({len(order_items_df):,} records)\")\n",
    "\n",
    "# --- SHEET 5: OrderItem Summary ---\n",
    "if \"orderitem_summary_df\" in dir() and len(orderitem_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItem_Summary\")\n",
    "    write_df_to_sheet(ws, orderitem_summary_df)\n",
    "    print(f\"   âœ… OrderItem_Summary\")\n",
    "\n",
    "# --- SHEET 6: Product Family Summary ---\n",
    "if \"product_summary_df\" in dir() and len(product_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"Product_Family_Summary\")\n",
    "    write_df_to_sheet(ws, product_summary_df)\n",
    "    print(f\"   âœ… Product_Family_Summary\")\n",
    "\n",
    "# --- SHEET 7: BAN Mapping ---\n",
    "if len(ban_mapping_df) > 0:\n",
    "    ws = wb.create_sheet(\"BAN_Mapping\")\n",
    "    write_df_to_sheet(ws, ban_mapping_df)\n",
    "    print(f\"   âœ… BAN_Mapping ({len(ban_mapping_df):,} records)\")\n",
    "\n",
    "# --- SHEET 8: Accounts ---\n",
    "if len(accounts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Accounts\")\n",
    "    write_df_to_sheet(ws, accounts_df)\n",
    "    print(f\"   âœ… Accounts ({len(accounts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 9: Contacts ---\n",
    "if len(contacts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Contacts\")\n",
    "    write_df_to_sheet(ws, contacts_df)\n",
    "    print(f\"   âœ… Contacts ({len(contacts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 10: Locations ---\n",
    "if len(locations_df) > 0:\n",
    "    ws = wb.create_sheet(\"Locations\")\n",
    "    write_df_to_sheet(ws, locations_df)\n",
    "    print(f\"   âœ… Locations ({len(locations_df):,} records)\")\n",
    "\n",
    "# --- SHEET 11: Nodes (v6 NEW) ---\n",
    "if \"nodes_df\" in dir() and len(nodes_df) > 0:\n",
    "    ws = wb.create_sheet(\"Nodes\")\n",
    "    write_df_to_sheet(ws, nodes_df)\n",
    "    print(f\"   âœ… Nodes ({len(nodes_df):,} records)\")\n",
    "\n",
    "# --- SHEET 12: Node A/Z Analysis (v6 NEW) ---\n",
    "if \"node_az_df\" in dir() and len(node_az_df) > 0:\n",
    "    ws = wb.create_sheet(\"Node_AZ_Analysis\")\n",
    "    write_df_to_sheet(ws, node_az_df)\n",
    "    print(f\"   âœ… Node_AZ_Analysis ({len(node_az_df):,} records)\")\n",
    "\n",
    "# --- SHEET 13: Off_Net ---\n",
    "if len(offnet_df) > 0:\n",
    "    ws = wb.create_sheet(\"Off_Net\")\n",
    "    write_df_to_sheet(ws, offnet_df)\n",
    "    print(f\"   âœ… Off_Net ({len(offnet_df):,} records)\")\n",
    "\n",
    "# --- SHEET 14: Data Quality ---\n",
    "combined_dq = data_quality_issues.copy()\n",
    "if \"orderitem_data_quality\" in dir() and len(orderitem_data_quality) > 0:\n",
    "    combined_dq.extend(orderitem_data_quality)\n",
    "\n",
    "if len(combined_dq) > 0:\n",
    "    ws = wb.create_sheet(\"Data_Quality\")\n",
    "    dq_df = pd.DataFrame(combined_dq)\n",
    "    write_df_to_sheet(ws, dq_df)\n",
    "    print(f\"   âœ… Data_Quality ({len(combined_dq):,} issues)\")\n",
    "\n",
    "# --- SHEET 15: Orders Missing BBF BAN ---\n",
    "if len(orders_no_bbf_ban_df) > 0:\n",
    "    ws = wb.create_sheet(\"Orders_Missing_BBF_BAN\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_no_bbf_ban_df.columns]\n",
    "    write_df_to_sheet(ws, orders_no_bbf_ban_df[export_cols])\n",
    "    print(f\"   âœ… Orders_Missing_BBF_BAN ({len(orders_no_bbf_ban_df):,} records)\")\n",
    "\n",
    "# --- SHEET 16: Excluded - PA MARKET DECOM ---\n",
    "if len(excluded_pa_decom_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_PA_DECOM\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Project_Group__c\",\n",
    "        \"Account_Name\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_pa_decom_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_pa_decom_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_PA_DECOM ({len(excluded_pa_decom_df):,} records)\")\n",
    "\n",
    "# --- SHEET 17: Excluded - Work Orders ---\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Work_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Service_Order_Record_Type__c\",\n",
    "        \"Account_Name\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_work_orders_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_work_orders_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Work_Orders ({len(excluded_work_orders_df):,} records)\")\n",
    "\n",
    "# --- SHEET 18: Excluded - Not Actively Billing ---\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Not_Billing\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"OSS_State_Desc\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "        \"OSS_Billing_Status\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_not_billing_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_not_billing_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Not_Billing ({len(excluded_not_billing_df):,} records)\")\n",
    "\n",
    "# === FIELD REFERENCE SHEETS ===\n",
    "print(\"\\n   Creating Field Reference sheets...\")\n",
    "\n",
    "# --- Fields: Order â†’ Service__c ---\n",
    "ws = wb.create_sheet(\"Fields_Order\")\n",
    "order_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Service_ID__c\", \"BBF Field\": \"Name\", \"Notes\": \"Service identifier\"},\n",
    "    {\n",
    "        \"ES Field\": \"AccountId\",\n",
    "        \"BBF Field\": \"Account__c\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Invoice__c\",\n",
    "        \"BBF Field\": \"Billing_Account_Number__c\",\n",
    "        \"Notes\": \"Lookup via BAN migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_A__c\",\n",
    "        \"BBF Field\": \"A_Location__c\",\n",
    "        \"Notes\": \"Lookup via Location migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_Z__c\",\n",
    "        \"BBF Field\": \"Z_Location__c\",\n",
    "        \"Notes\": \"Lookup via Location migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Node__c\",\n",
    "        \"BBF Field\": \"A_Node__c OR Z_Node__c\",\n",
    "        \"Notes\": \"Based on A/Z analysis - see Node_AZ_Analysis sheet\",\n",
    "    },\n",
    "    {\"ES Field\": \"Status\", \"BBF Field\": \"Status__c\", \"Notes\": \"Value mapping required\"},\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Start_Date__c\",\n",
    "        \"BBF Field\": \"Active_Date__c\",\n",
    "        \"Notes\": \"PRIMARY date source\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_End_Date__c\",\n",
    "        \"BBF Field\": \"Disconnect_Date__c\",\n",
    "        \"Notes\": \"Direct copy if present\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_Provided__c\",\n",
    "        \"BBF Field\": \"Bandwidth__c\",\n",
    "        \"Notes\": \"Service bandwidth in Mbps\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"SOF_MRC__c\",\n",
    "        \"BBF Field\": \"mrc__c\",\n",
    "        \"Notes\": \"Monthly recurring charge\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Primary_Product_Family__c\",\n",
    "        \"BBF Field\": \"Product_Type__c\",\n",
    "        \"Notes\": \"Value mapping required\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_Circuit_ID__c\",\n",
    "        \"BBF Field\": \"Vendor_Circuit_ID__c\",\n",
    "        \"Notes\": \"Direct copy\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(order_fields))\n",
    "ws.column_dimensions[\"A\"].width = 25\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 50\n",
    "print(\"   âœ… Fields_Order\")\n",
    "\n",
    "# --- Fields: Node__c â†’ Node__c (v6 NEW) ---\n",
    "ws = wb.create_sheet(\"Fields_Node\")\n",
    "node_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"Node name\"},\n",
    "    {\"ES Field\": \"Node_ID__c\", \"BBF Field\": \"Node_Id__c\", \"Notes\": \"Node identifier\"},\n",
    "    {\n",
    "        \"ES Field\": \"Address__c\",\n",
    "        \"BBF Field\": \"Street_Address__c\",\n",
    "        \"Notes\": \"String address â†’ BBF uses Location__c lookup\",\n",
    "    },\n",
    "    {\"ES Field\": \"Site_Name__c\", \"BBF Field\": \"POP_Name__c\", \"Notes\": \"Site/POP name\"},\n",
    "    {\n",
    "        \"ES Field\": \"Ring__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no Ring__c object - SKIP or store in notes\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Ring_Type__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"East_Neighbor__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"Ring topology - BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"West_Neighbor__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"Ring topology - BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Distance_to_East__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"Ring topology - BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Distance_to_West__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"Ring topology - BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_Order_Agreement__c\",\n",
    "        \"BBF Field\": \"(reverse lookup)\",\n",
    "        \"Notes\": \"In BBF, Service__c.A_Node__c/Z_Node__c points to Node\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_ID__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"Service ID - use for validation\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Maintenance_IP_Address__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no equivalent\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"(derive from Location)\",\n",
    "        \"BBF Field\": \"Location__c\",\n",
    "        \"Notes\": \"BBF Node links to Location - need to create/map\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"(none)\",\n",
    "        \"BBF Field\": \"City__c\",\n",
    "        \"Notes\": \"BBF has address fields on Node - derive from ES Address__c\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"(none)\",\n",
    "        \"BBF Field\": \"State__c\",\n",
    "        \"Notes\": \"BBF has address fields on Node - derive from ES Address__c\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"(none)\",\n",
    "        \"BBF Field\": \"Zip__c\",\n",
    "        \"Notes\": \"BBF has address fields on Node - derive from ES Address__c\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(node_fields))\n",
    "ws.column_dimensions[\"A\"].width = 30\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 55\n",
    "print(\"   âœ… Fields_Node (v6 NEW)\")\n",
    "\n",
    "# --- Fields: OrderItem â†’ Service_Charge__c ---\n",
    "ws = wb.create_sheet(\"Fields_OrderItem\")\n",
    "orderitem_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"OrderId\",\n",
    "        \"BBF Field\": \"Service__c\",\n",
    "        \"Notes\": \"Lookup via Orderâ†’Service migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Product_Name__c\",\n",
    "        \"BBF Field\": \"Product_Simple__c\",\n",
    "        \"Notes\": \"Value mapping required\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Product_Family__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"Used for product mapping\",\n",
    "    },\n",
    "    {\"ES Field\": \"UnitPrice\", \"BBF Field\": \"Unit_Rate__c\", \"Notes\": \"Unit price\"},\n",
    "    {\n",
    "        \"ES Field\": \"Total_MRC_Amortized__c\",\n",
    "        \"BBF Field\": \"Amount__c\",\n",
    "        \"Notes\": \"MRC amount\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"NRC_IRU_FEE__c\",\n",
    "        \"BBF Field\": \"NRC__c\",\n",
    "        \"Notes\": \"Non-recurring charge\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_Fees_Monthly__c\",\n",
    "        \"BBF Field\": \"MRC_COGS__c\",\n",
    "        \"Notes\": \"Vendor monthly cost\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_NRC__c\",\n",
    "        \"BBF Field\": \"NRC_COGS__c\",\n",
    "        \"Notes\": \"Vendor NRC cost\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"ServiceDate\",\n",
    "        \"BBF Field\": \"Start_Date__c\",\n",
    "        \"Notes\": \"Service start date\",\n",
    "    },\n",
    "    {\"ES Field\": \"EndDate\", \"BBF Field\": \"End_Date__c\", \"Notes\": \"Service end date\"},\n",
    "    {\"ES Field\": \"Quantity\", \"BBF Field\": \"Units__c\", \"Notes\": \"Quantity\"},\n",
    "    {\n",
    "        \"ES Field\": \"SBQQ__ChargeType__c\",\n",
    "        \"BBF Field\": \"Charge_Class__c\",\n",
    "        \"Notes\": \"One-Timeâ†’NONRECUR, Recurringâ†’RECUR\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Cancelled__c\",\n",
    "        \"BBF Field\": \"Charge_Active__c\",\n",
    "        \"Notes\": \"Inverse - Active if not cancelled\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Last_Mile_Carrier__c\",\n",
    "        \"BBF Field\": \"Aloc_COGS_Provider__c\",\n",
    "        \"Notes\": \"Vendor lookup\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(orderitem_fields))\n",
    "ws.column_dimensions[\"A\"].width = 25\n",
    "ws.column_dimensions[\"B\"].width = 25\n",
    "ws.column_dimensions[\"C\"].width = 50\n",
    "print(\"   âœ… Fields_OrderItem\")\n",
    "\n",
    "# --- Fields: Billing_Invoice__c â†’ BAN__c ---\n",
    "ws = wb.create_sheet(\"Fields_BAN\")\n",
    "ban_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"BAN identifier\"},\n",
    "    {\n",
    "        \"ES Field\": \"Account__c\",\n",
    "        \"BBF Field\": \"Account__c\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Account_Number__c\",\n",
    "        \"BBF Field\": \"Account_Number__c\",\n",
    "        \"Notes\": \"Account number\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Address_1__c\",\n",
    "        \"BBF Field\": \"Billing_Address_1__c\",\n",
    "        \"Notes\": \"Billing address line 1\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_City__c\",\n",
    "        \"BBF Field\": \"Billing_City__c\",\n",
    "        \"Notes\": \"Billing city\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_State__c\",\n",
    "        \"BBF Field\": \"Billing_State__c\",\n",
    "        \"Notes\": \"Billing state\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_ZIP__c\",\n",
    "        \"BBF Field\": \"Billing_Zip__c\",\n",
    "        \"Notes\": \"Billing ZIP code\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_E_mail__c\",\n",
    "        \"BBF Field\": \"Billing_Email__c\",\n",
    "        \"Notes\": \"Billing email\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Payment_Terms__c\",\n",
    "        \"BBF Field\": \"Payment_Terms__c\",\n",
    "        \"Notes\": \"Payment terms\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(ban_fields))\n",
    "ws.column_dimensions[\"A\"].width = 28\n",
    "ws.column_dimensions[\"B\"].width = 28\n",
    "ws.column_dimensions[\"C\"].width = 35\n",
    "print(\"   âœ… Fields_BAN\")\n",
    "\n",
    "# --- Fields: Account â†’ Account ---\n",
    "ws = wb.create_sheet(\"Fields_Account\")\n",
    "account_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"Account name\"},\n",
    "    {\"ES Field\": \"Type\", \"BBF Field\": \"Type\", \"Notes\": \"Account type\"},\n",
    "    {\"ES Field\": \"Industry\", \"BBF Field\": \"Industry\", \"Notes\": \"Industry\"},\n",
    "    {\n",
    "        \"ES Field\": \"BillingStreet\",\n",
    "        \"BBF Field\": \"BillingStreet\",\n",
    "        \"Notes\": \"Billing address\",\n",
    "    },\n",
    "    {\"ES Field\": \"BillingCity\", \"BBF Field\": \"BillingCity\", \"Notes\": \"Billing city\"},\n",
    "    {\"ES Field\": \"BillingState\", \"BBF Field\": \"BillingState\", \"Notes\": \"Billing state\"},\n",
    "    {\n",
    "        \"ES Field\": \"BillingPostalCode\",\n",
    "        \"BBF Field\": \"BillingPostalCode\",\n",
    "        \"Notes\": \"Billing postal code\",\n",
    "    },\n",
    "    {\"ES Field\": \"Phone\", \"BBF Field\": \"Phone\", \"Notes\": \"Phone number\"},\n",
    "    {\"ES Field\": \"Website\", \"BBF Field\": \"Website\", \"Notes\": \"Website URL\"},\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(account_fields))\n",
    "ws.column_dimensions[\"A\"].width = 22\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 30\n",
    "print(\"   âœ… Fields_Account\")\n",
    "\n",
    "# --- Fields: Contact â†’ Contact ---\n",
    "ws = wb.create_sheet(\"Fields_Contact\")\n",
    "contact_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"AccountId\",\n",
    "        \"BBF Field\": \"AccountId\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\"ES Field\": \"FirstName\", \"BBF Field\": \"FirstName\", \"Notes\": \"First name\"},\n",
    "    {\"ES Field\": \"LastName\", \"BBF Field\": \"LastName\", \"Notes\": \"Last name\"},\n",
    "    {\"ES Field\": \"Email\", \"BBF Field\": \"Email\", \"Notes\": \"Email address\"},\n",
    "    {\"ES Field\": \"Phone\", \"BBF Field\": \"Phone\", \"Notes\": \"Phone number\"},\n",
    "    {\"ES Field\": \"Title\", \"BBF Field\": \"Title\", \"Notes\": \"Job title\"},\n",
    "    {\"ES Field\": \"MobilePhone\", \"BBF Field\": \"MobilePhone\", \"Notes\": \"Mobile phone\"},\n",
    "    {\"ES Field\": \"Department\", \"BBF Field\": \"Department\", \"Notes\": \"Department\"},\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(contact_fields))\n",
    "ws.column_dimensions[\"A\"].width = 22\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 30\n",
    "print(\"   âœ… Fields_Contact\")\n",
    "\n",
    "# --- Fields: Address__c â†’ Location__c ---\n",
    "ws = wb.create_sheet(\"Fields_Location\")\n",
    "location_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"Location name\"},\n",
    "    {\"ES Field\": \"Address__c\", \"BBF Field\": \"Street__c\", \"Notes\": \"Street address\"},\n",
    "    {\"ES Field\": \"City__c\", \"BBF Field\": \"City__c\", \"Notes\": \"City\"},\n",
    "    {\"ES Field\": \"State__c\", \"BBF Field\": \"State__c\", \"Notes\": \"State\"},\n",
    "    {\"ES Field\": \"County__c\", \"BBF Field\": \"County__c\", \"Notes\": \"County\"},\n",
    "    {\"ES Field\": \"Zip__c\", \"BBF Field\": \"PostalCode__c\", \"Notes\": \"ZIP code\"},\n",
    "    {\"ES Field\": \"CLLI__c\", \"BBF Field\": \"CLLICode__c\", \"Notes\": \"CLLI code\"},\n",
    "    {\"ES Field\": \"Geocode_Lat_Long__c\", \"BBF Field\": \"Loc__c\", \"Notes\": \"Geolocation\"},\n",
    "    {\n",
    "        \"ES Field\": \"Building_Status__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no equivalent - SKIP\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Building_Type__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no equivalent - SKIP\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"On_Net__c\",\n",
    "        \"BBF Field\": \"(none)\",\n",
    "        \"Notes\": \"BBF has no equivalent - SKIP\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(location_fields))\n",
    "ws.column_dimensions[\"A\"].width = 25\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 40\n",
    "print(\"   âœ… Fields_Location\")\n",
    "\n",
    "# Save\n",
    "wb.save(OUTPUT_FILE)\n",
    "print(f\"\\nâœ… Excel file saved: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANUP\n",
      "================================================================================\n",
      "âœ… OSS connection closed\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Output file: es_bbf_migration_analysis_v6_20260108_121628.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CLEANUP ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "oss_conn.close()\n",
    "print(\"âœ… OSS connection closed\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## v6 Changes Summary\n",
    "\n",
    "### NEW: Node__c Analysis\n",
    "- Queries Node__c records referenced by migration orders\n",
    "- Gets nodes via Order.Node__c field\n",
    "- Gets nodes via Node.Service_Order_Agreement__c lookup\n",
    "- Creates Order-Node mapping for A/Z analysis\n",
    "\n",
    "### NEW: Node A/Z Determination\n",
    "- Compares Node.Address__c to Order.Address_A__c and Address_Z__c\n",
    "- Determines if ES Node should map to BBF A_Node__c or Z_Node__c\n",
    "- Categories: A_NODE, Z_NODE, BOTH_MATCH, NO_MATCH\n",
    "\n",
    "### NEW: Node Excel Sheets\n",
    "- **Nodes**: All Node__c records for migration\n",
    "- **Node_AZ_Analysis**: A/Z determination for each Order-Node pair\n",
    "- **Fields_Node**: ES â†’ BBF field mapping for Node__c\n",
    "\n",
    "### Inherited from v5\n",
    "- Comprehensive Legend sheet with full documentation\n",
    "- Field reference sheets for all objects\n",
    "\n",
    "### Inherited from v4\n",
    "- OrderItem analysis for migration scope orders\n",
    "- Active Date uses Billing_Start_Date__c (primary) â†’ OSS bill_start_date (fallback)\n",
    "\n",
    "### Inherited from v3\n",
    "- OSS actively billing validation\n",
    "- Work Orders enriched with workorders.workorders data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
