{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES â†’ BBF Migration Data Analysis (v4)\n",
    "\n",
    "This notebook analyzes all data required for the ES to BBF Salesforce migration.\n",
    "\n",
    "## Version 4 Changes\n",
    "- **NEW**: OrderItem analysis for migration scope orders\n",
    "- **NEW**: OrderItem data quality checks\n",
    "- **NEW**: Product/Family breakdown analysis\n",
    "- **NEW**: Proposed field mapping reference\n",
    "- **FIX**: Added `Billing_Start_Date__c` to date cascade (Service_Start_Date__c â†’ Billing_Start_Date__c â†’ OSS bill_start_date)\n",
    "\n",
    "## Version 3 Changes (inherited)\n",
    "- Include OA (Accepted) orders in addition to CL (Closed)\n",
    "- Work Orders enriched with data from `workorders.workorders` table\n",
    "\n",
    "## Driving Principle\n",
    "**Everything is driven from Active ES Orders that are ACTUALLY BILLING in OSS** - we migrate only the data needed to support truly active services.\n",
    "\n",
    "## Filter Pipeline\n",
    "1. Status IN ('Activated', 'Suspended (Late Payment)', 'Disconnect in Progress')\n",
    "2. Project_Group__c NOT LIKE '%PA MARKET DECOM%'\n",
    "3. Service_Order_Record_Type__c = 'Service Order Agreement' (excludes Work Orders)\n",
    "4. OSS Actively Billing:\n",
    "   - order_state_cd IN ('CL', 'OA')\n",
    "   - bill_start_date <= today\n",
    "   - bill_end_date IS NULL or > today\n",
    "5. Has BBF BAN mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\vjero\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe\n",
      "Pandas: 2.2.3\n",
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "# === SETUP & IMPORTS ===\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from simple_salesforce import Salesforce\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration loaded\n",
      "   Active Statuses: ['Activated', 'Suspended (Late Payment)', 'Disconnect in Progress']\n",
      "   Record Type: Service Order Agreement\n",
      "   Excluding: Project_Group__c LIKE '%PA MARKET DECOM%'\n",
      "   OSS Active States: ['CL', 'OA']\n",
      "   Output: es_bbf_migration_analysis_v4_20260107_212249.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# ES (Source) Salesforce Credentials\n",
    "ES_USERNAME = \"sfdcapi@everstream.net\"\n",
    "ES_PASSWORD = \"pV4CAxns8DQtJsBq!\"\n",
    "ES_TOKEN = \"r1uoYiusK19RbrflARydi86TA\"\n",
    "ES_DOMAIN = \"login\"  # 'login' for production\n",
    "\n",
    "# OSS Database Credentials\n",
    "OSS_HOST = \"pg01.comlink.net\"\n",
    "OSS_PORT = \"5432\"\n",
    "OSS_DB = \"GLC\"\n",
    "OSS_USER = \"oss_server\"\n",
    "OSS_PASSWORD = \"3wU3uB28X?!r2?@ebrUg\"\n",
    "\n",
    "# Active Order Status Filter\n",
    "ACTIVE_STATUSES = [\"Activated\", \"Suspended (Late Payment)\", \"Disconnect in Progress\"]\n",
    "\n",
    "# Record Type Filter\n",
    "VALID_RECORD_TYPE = \"Service Order Agreement\"\n",
    "\n",
    "# PA Market Decom Exclusion\n",
    "PA_DECOM_FILTER = \"PA MARKET DECOM\"\n",
    "\n",
    "# OSS Order States that qualify as \"actively billing\"\n",
    "ACTIVE_OSS_STATES = [\"CL\", \"OA\"]  # Closed and Accepted\n",
    "\n",
    "# OSS Order States Reference\n",
    "OSS_ORDER_STATES = {\n",
    "    \"CL\": \"Closed (Active/Billing)\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"OS\": \"Submitted\",\n",
    "    \"OC\": \"Created\",\n",
    "    \"PN\": \"Pending\",\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"OR\": \"Rejected\",\n",
    "    \"OV\": \"Validated (Disabled)\",\n",
    "}\n",
    "\n",
    "# OSS Work Order Types Reference\n",
    "WORKORDER_TYPES = {\n",
    "    \"IT\": \"Professional Services\",\n",
    "    \"MR\": \"Maintenance/Repair\",\n",
    "    \"OS\": \"Other Service\",\n",
    "    \"VS\": \"Voice Service\",\n",
    "}\n",
    "\n",
    "# OSS Work Order States Reference\n",
    "WORKORDER_STATES = {\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"CL\": \"Closed\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"PN\": \"Pending\",\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_FILE = f\"es_bbf_migration_analysis_v4_{TIMESTAMP}.xlsx\"\n",
    "\n",
    "print(\"ðŸ“‹ Configuration loaded\")\n",
    "print(f\"   Active Statuses: {ACTIVE_STATUSES}\")\n",
    "print(f\"   Record Type: {VALID_RECORD_TYPE}\")\n",
    "print(f\"   Excluding: Project_Group__c LIKE '%{PA_DECOM_FILTER}%'\")\n",
    "print(f\"   OSS Active States: {ACTIVE_OSS_STATES}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONNECTING TO ES SALESFORCE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to ES...\n",
      "âœ… Connected to ES: everstream.my.salesforce.com\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO ES SALESFORCE ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONNECTING TO ES SALESFORCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to ES...\")\n",
    "es_sf = Salesforce(\n",
    "    username=ES_USERNAME,\n",
    "    password=ES_PASSWORD,\n",
    "    security_token=ES_TOKEN,\n",
    "    domain=ES_DOMAIN,\n",
    ")\n",
    "print(f\"âœ… Connected to ES: {es_sf.sf_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONNECTING TO OSS DATABASE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to OSS...\n",
      "âœ… Connected to OSS: pg01.comlink.net/GLC\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO OSS DATABASE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONNECTING TO OSS DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to OSS...\")\n",
    "oss_conn = psycopg2.connect(\n",
    "    dbname=OSS_DB,\n",
    "    user=OSS_USER,\n",
    "    password=OSS_PASSWORD,\n",
    "    host=OSS_HOST,\n",
    "    port=OSS_PORT,\n",
    ")\n",
    "print(f\"âœ… Connected to OSS: {OSS_HOST}/{OSS_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: QUERYING ACTIVE ORDERS\n",
      "================================================================================\n",
      "Querying all orders with active statuses...\n",
      "\n",
      "âœ… Total orders with active status: 17,972\n",
      "\n",
      "ðŸ“Š Record Type Breakdown:\n",
      "Service_Order_Record_Type__c\n",
      "Service Order Agreement    14058\n",
      "Work Order                  3914\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: QUERY ALL ACTIVE ORDERS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: QUERYING ACTIVE ORDERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status_filter = \"','\".join(ACTIVE_STATUSES)\n",
    "\n",
    "orders_query = f\"\"\"\n",
    "SELECT \n",
    "    Id, \n",
    "    Name,\n",
    "    Service_ID__c,\n",
    "    Status,\n",
    "    AccountId,\n",
    "    Account.Name,\n",
    "    Billing_Invoice__c,\n",
    "    Address_A__c,\n",
    "    Address_Z__c,\n",
    "    Node__c,\n",
    "    OpportunityId,\n",
    "    Service_Start_Date__c,\n",
    "    Billing_Start_Date__c,\n",
    "    Service_End_Date__c,\n",
    "    Service_Provided__c,\n",
    "    SOF_MRC__c,\n",
    "    OSS_Order__c,\n",
    "    OSS_Service_ID__c,\n",
    "    Vendor_Circuit_ID__c,\n",
    "    Primary_Product_Family__c,\n",
    "    Primary_Product_Name__c,\n",
    "    Project_Group__c,\n",
    "    Service_Order_Record_Type__c,\n",
    "    CreatedDate,\n",
    "    LastModifiedDate\n",
    "FROM Order\n",
    "WHERE Status IN ('{status_filter}')\n",
    "ORDER BY Service_ID__c\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying all orders with active statuses...\")\n",
    "result = es_sf.query_all(orders_query)\n",
    "orders_raw = result[\"records\"]\n",
    "\n",
    "# Flatten results\n",
    "all_orders = []\n",
    "for order in orders_raw:\n",
    "    all_orders.append(\n",
    "        {\n",
    "            \"Id\": order[\"Id\"],\n",
    "            \"Name\": order.get(\"Name\"),\n",
    "            \"Service_ID__c\": order.get(\"Service_ID__c\"),\n",
    "            \"Status\": order[\"Status\"],\n",
    "            \"AccountId\": order.get(\"AccountId\"),\n",
    "            \"Account_Name\": order[\"Account\"][\"Name\"] if order.get(\"Account\") else None,\n",
    "            \"Billing_Invoice__c\": order.get(\"Billing_Invoice__c\"),\n",
    "            \"Address_A__c\": order.get(\"Address_A__c\"),\n",
    "            \"Address_Z__c\": order.get(\"Address_Z__c\"),\n",
    "            \"Node__c\": order.get(\"Node__c\"),\n",
    "            \"OpportunityId\": order.get(\"OpportunityId\"),\n",
    "            \"Service_Start_Date__c\": order.get(\"Service_Start_Date__c\"),\n",
    "            \"Billing_Start_Date__c\": order.get(\"Billing_Start_Date__c\"),\n",
    "            \"Service_End_Date__c\": order.get(\"Service_End_Date__c\"),\n",
    "            \"Service_Provided__c\": order.get(\"Service_Provided__c\"),\n",
    "            \"SOF_MRC__c\": order.get(\"SOF_MRC__c\"),\n",
    "            \"OSS_Order__c\": order.get(\"OSS_Order__c\"),\n",
    "            \"OSS_Service_ID__c\": order.get(\"OSS_Service_ID__c\"),\n",
    "            \"Vendor_Circuit_ID__c\": order.get(\"Vendor_Circuit_ID__c\"),\n",
    "            \"Primary_Product_Family__c\": order.get(\"Primary_Product_Family__c\"),\n",
    "            \"Primary_Product_Name__c\": order.get(\"Primary_Product_Name__c\"),\n",
    "            \"Project_Group__c\": order.get(\"Project_Group__c\"),\n",
    "            \"Service_Order_Record_Type__c\": order.get(\"Service_Order_Record_Type__c\"),\n",
    "            \"CreatedDate\": order.get(\"CreatedDate\"),\n",
    "            \"LastModifiedDate\": order.get(\"LastModifiedDate\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_orders_df = pd.DataFrame(all_orders)\n",
    "print(f\"\\nâœ… Total orders with active status: {len(all_orders_df):,}\")\n",
    "\n",
    "# Show record type breakdown\n",
    "print(f\"\\nðŸ“Š Record Type Breakdown:\")\n",
    "print(all_orders_df[\"Service_Order_Record_Type__c\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FILTER - PA MARKET DECOM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PA MARKET DECOM Filter:\n",
      "   Before: 17,972\n",
      "   âŒ Excluded (PA MARKET DECOM): 887\n",
      "   âœ… Remaining: 17,085\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: FILTER - PA MARKET DECOM ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: FILTER - PA MARKET DECOM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate PA MARKET DECOM orders\n",
    "pa_decom_mask = (\n",
    "    all_orders_df[\"Project_Group__c\"]\n",
    "    .fillna(\"\")\n",
    "    .str.contains(PA_DECOM_FILTER, case=False)\n",
    ")\n",
    "excluded_pa_decom_df = all_orders_df[pa_decom_mask].copy()\n",
    "orders_after_pa_filter = all_orders_df[~pa_decom_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š PA MARKET DECOM Filter:\")\n",
    "print(f\"   Before: {len(all_orders_df):,}\")\n",
    "print(f\"   âŒ Excluded (PA MARKET DECOM): {len(excluded_pa_decom_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_pa_filter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FILTER - RECORD TYPE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Record Type Filter:\n",
      "   Before: 17,085\n",
      "   âŒ Excluded (Not 'Service Order Agreement'): 3,477\n",
      "   âœ… Remaining: 13,608\n",
      "\n",
      "   Excluded Record Types:\n",
      "Service_Order_Record_Type__c\n",
      "Work Order    3477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: FILTER - RECORD TYPE (Service Order Agreement only) ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FILTER - RECORD TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Keep only Service Order Agreement\n",
    "record_type_mask = (\n",
    "    orders_after_pa_filter[\"Service_Order_Record_Type__c\"] == VALID_RECORD_TYPE\n",
    ")\n",
    "excluded_work_orders_df = orders_after_pa_filter[~record_type_mask].copy()\n",
    "orders_after_rt_filter = orders_after_pa_filter[record_type_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Record Type Filter:\")\n",
    "print(f\"   Before: {len(orders_after_pa_filter):,}\")\n",
    "print(f\"   âŒ Excluded (Not '{VALID_RECORD_TYPE}'): {len(excluded_work_orders_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_rt_filter):,}\")\n",
    "\n",
    "# Show breakdown of what was excluded\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    print(f\"\\n   Excluded Record Types:\")\n",
    "    print(\n",
    "        excluded_work_orders_df[\"Service_Order_Record_Type__c\"].value_counts(\n",
    "            dropna=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\n",
      "================================================================================\n",
      "\n",
      "   Work Orders with OSS_Order__c: 156\n",
      "   Work Orders without OSS_Order__c: 3,321\n",
      "   Unique workorder IDs to query: 156\n",
      "   Chunk 1: Retrieved 84 workorders\n",
      "\n",
      "âœ… Enriched 84 work orders with OSS data\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3B: ENRICH WORK ORDERS WITH OSS DATA ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    # Get work orders with OSS_Order__c (maps to workorder_id)\n",
    "    has_wo_oss = excluded_work_orders_df[\"OSS_Order__c\"].notna() & (\n",
    "        excluded_work_orders_df[\"OSS_Order__c\"] != \"\"\n",
    "    )\n",
    "    work_orders_with_oss = excluded_work_orders_df[has_wo_oss].copy()\n",
    "\n",
    "    print(f\"\\n   Work Orders with OSS_Order__c: {len(work_orders_with_oss):,}\")\n",
    "    print(f\"   Work Orders without OSS_Order__c: {(~has_wo_oss).sum():,}\")\n",
    "\n",
    "    if len(work_orders_with_oss) > 0:\n",
    "        workorder_ids = (\n",
    "            work_orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "        )\n",
    "        print(f\"   Unique workorder IDs to query: {len(workorder_ids):,}\")\n",
    "\n",
    "        # Query workorders.workorders\n",
    "        chunk_size = 5000\n",
    "        oss_workorders = []\n",
    "\n",
    "        for i in range(0, len(workorder_ids), chunk_size):\n",
    "            chunk = workorder_ids[i : i + chunk_size]\n",
    "            ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "            wo_query = f\"\"\"\n",
    "            SELECT \n",
    "                workorder_id,\n",
    "                order_nm,\n",
    "                order_id,\n",
    "                workorder_type_cd,\n",
    "                workorder_state_cd,\n",
    "                description,\n",
    "                start_date,\n",
    "                end_date,\n",
    "                disabled\n",
    "            FROM workorders.workorders\n",
    "            WHERE workorder_id IN ({ids_str})\n",
    "              AND disabled = 'infinity'\n",
    "            \"\"\"\n",
    "\n",
    "            with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "                cur.execute(wo_query)\n",
    "                rows = cur.fetchall()\n",
    "                oss_workorders.extend([dict(row) for row in rows])\n",
    "\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} workorders\")\n",
    "\n",
    "        if len(oss_workorders) > 0:\n",
    "            oss_wo_df = pd.DataFrame(oss_workorders)\n",
    "\n",
    "            # Convert date columns - strip timezone for Excel compatibility\n",
    "            for date_col in [\"start_date\", \"end_date\"]:\n",
    "                if date_col in oss_wo_df.columns:\n",
    "                    oss_wo_df[date_col] = pd.to_datetime(\n",
    "                        oss_wo_df[date_col], utc=True\n",
    "                    ).dt.tz_localize(None)\n",
    "\n",
    "            # Add descriptions\n",
    "            oss_wo_df[\"workorder_type_desc\"] = oss_wo_df[\"workorder_type_cd\"].map(\n",
    "                WORKORDER_TYPES\n",
    "            )\n",
    "            oss_wo_df[\"workorder_state_desc\"] = oss_wo_df[\"workorder_state_cd\"].map(\n",
    "                WORKORDER_STATES\n",
    "            )\n",
    "\n",
    "            # Merge with excluded work orders\n",
    "            excluded_work_orders_df[\"OSS_Order_ID\"] = pd.to_numeric(\n",
    "                excluded_work_orders_df[\"OSS_Order__c\"], errors=\"coerce\"\n",
    "            )\n",
    "            excluded_work_orders_df = excluded_work_orders_df.merge(\n",
    "                oss_wo_df, left_on=\"OSS_Order_ID\", right_on=\"workorder_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"\\nâœ… Enriched {oss_wo_df['workorder_id'].notna().sum():,} work orders with OSS data\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ No active workorders found in OSS\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No work orders have OSS_Order__c populated\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No work orders to enrich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š OSS_Order__c Population:\n",
      "   With OSS_Order__c: 11,525\n",
      "   Without OSS_Order__c: 2,083\n",
      "\n",
      "   Unique OSS Order IDs to query: 11,518\n",
      "   Chunk 1: Retrieved 5000 orders\n",
      "   Chunk 2: Retrieved 4999 orders\n",
      "   Chunk 3: Retrieved 1518 orders\n",
      "\n",
      "âœ… Total OSS orders retrieved: 11,517\n",
      "   OSS Order IDs not found in OSS: 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: CHECK OSS ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify orders with OSS_Order__c\n",
    "has_oss_id = orders_after_rt_filter[\"OSS_Order__c\"].notna() & (\n",
    "    orders_after_rt_filter[\"OSS_Order__c\"] != \"\"\n",
    ")\n",
    "orders_with_oss = orders_after_rt_filter[has_oss_id].copy()\n",
    "orders_without_oss = orders_after_rt_filter[~has_oss_id].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š OSS_Order__c Population:\")\n",
    "print(f\"   With OSS_Order__c: {len(orders_with_oss):,}\")\n",
    "print(f\"   Without OSS_Order__c: {len(orders_without_oss):,}\")\n",
    "\n",
    "# Query OSS for orders with OSS_Order__c\n",
    "if len(orders_with_oss) > 0:\n",
    "    oss_order_ids = (\n",
    "        orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "    )\n",
    "    print(f\"\\n   Unique OSS Order IDs to query: {len(oss_order_ids):,}\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 5000\n",
    "    oss_orders = []\n",
    "\n",
    "    for i in range(0, len(oss_order_ids), chunk_size):\n",
    "        chunk = oss_order_ids[i : i + chunk_size]\n",
    "        ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "        oss_query = f\"\"\"\n",
    "        SELECT \n",
    "            order_id,\n",
    "            order_state_cd,\n",
    "            order_type_cd,\n",
    "            bill_start_date,\n",
    "            bill_end_date,\n",
    "            circuit_active_date,\n",
    "            account_id,\n",
    "            service_id\n",
    "        FROM om.orders\n",
    "        WHERE order_id IN ({ids_str})\n",
    "        \"\"\"\n",
    "\n",
    "        with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(oss_query)\n",
    "            rows = cur.fetchall()\n",
    "            oss_orders.extend([dict(row) for row in rows])\n",
    "\n",
    "        print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} orders\")\n",
    "\n",
    "    oss_orders_df = pd.DataFrame(oss_orders)\n",
    "    print(f\"\\nâœ… Total OSS orders retrieved: {len(oss_orders_df):,}\")\n",
    "\n",
    "    # Check for OSS IDs not found\n",
    "    found_ids = (\n",
    "        set(oss_orders_df[\"order_id\"].tolist()) if len(oss_orders_df) > 0 else set()\n",
    "    )\n",
    "    not_found_ids = set(oss_order_ids) - found_ids\n",
    "    print(f\"   OSS Order IDs not found in OSS: {len(not_found_ids):,}\")\n",
    "else:\n",
    "    oss_orders_df = pd.DataFrame()\n",
    "    not_found_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: DETERMINING ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "Today's date for comparison: 2026-01-07\n",
      "Active OSS States: ['CL', 'OA']\n",
      "\n",
      "ðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\n",
      "   ACTIVELY BILLING: 11,489 (99.7%)\n",
      "   CL - Bill End Passed: 23 (0.2%)\n",
      "   Not Active State (CA): 6 (0.1%)\n",
      "   CL - Bill Start Future: 6 (0.1%)\n",
      "   No OSS Match: 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: DETERMINE ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: DETERMINING ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "today = pd.Timestamp.now().normalize()\n",
    "print(f\"\\nToday's date for comparison: {today.date()}\")\n",
    "print(f\"Active OSS States: {ACTIVE_OSS_STATES}\")\n",
    "\n",
    "# Merge SF orders with OSS data\n",
    "if len(orders_with_oss) > 0:\n",
    "    orders_with_oss[\"OSS_Order_ID\"] = orders_with_oss[\"OSS_Order__c\"].astype(int)\n",
    "\n",
    "if len(oss_orders_df) > 0:\n",
    "    # Convert date columns\n",
    "    oss_orders_df[\"bill_start_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_start_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "    oss_orders_df[\"bill_end_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_end_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged_df = orders_with_oss.merge(\n",
    "        oss_orders_df, left_on=\"OSS_Order_ID\", right_on=\"order_id\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    merged_df = orders_with_oss.copy()\n",
    "    merged_df[\"order_id\"] = None\n",
    "    merged_df[\"order_state_cd\"] = None\n",
    "    merged_df[\"bill_start_date\"] = None\n",
    "    merged_df[\"bill_end_date\"] = None\n",
    "\n",
    "\n",
    "# Determine actively billing status\n",
    "def get_billing_status(row):\n",
    "    if pd.isna(row.get(\"order_id\")):\n",
    "        return \"No OSS Match\"\n",
    "\n",
    "    state = row[\"order_state_cd\"].strip() if row.get(\"order_state_cd\") else None\n",
    "    bill_start = row.get(\"bill_start_date\")\n",
    "    bill_end = row.get(\"bill_end_date\")\n",
    "\n",
    "    # Check if state is in allowed active states (CL or OA)\n",
    "    if state not in ACTIVE_OSS_STATES:\n",
    "        return f\"Not Active State ({state})\"\n",
    "\n",
    "    if pd.isna(bill_start):\n",
    "        return f\"{state} - No Bill Start Date\"\n",
    "\n",
    "    if bill_start > today:\n",
    "        return f\"{state} - Bill Start Future\"\n",
    "\n",
    "    if pd.notna(bill_end) and bill_end <= today:\n",
    "        return f\"{state} - Bill End Passed\"\n",
    "\n",
    "    return \"ACTIVELY BILLING\"\n",
    "\n",
    "\n",
    "merged_df[\"OSS_Billing_Status\"] = merged_df.apply(get_billing_status, axis=1)\n",
    "\n",
    "# Add state description\n",
    "merged_df[\"OSS_State_Desc\"] = merged_df[\"order_state_cd\"].map(OSS_ORDER_STATES)\n",
    "\n",
    "# Also handle orders without OSS link\n",
    "orders_without_oss[\"OSS_Billing_Status\"] = \"No OSS_Order__c in SF\"\n",
    "orders_without_oss[\"order_state_cd\"] = None\n",
    "orders_without_oss[\"OSS_State_Desc\"] = None\n",
    "orders_without_oss[\"bill_start_date\"] = None\n",
    "orders_without_oss[\"bill_end_date\"] = None\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\")\n",
    "if len(merged_df) > 0:\n",
    "    billing_status_counts = merged_df[\"OSS_Billing_Status\"].value_counts()\n",
    "    for status, count in billing_status_counts.items():\n",
    "        pct = 100 * count / len(merged_df)\n",
    "        print(f\"   {status}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FILTER - ACTIVELY BILLING ONLY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Actively Billing Filter:\n",
      "   Before: 13,608\n",
      "   âŒ Excluded (Not Actively Billing): 2,119\n",
      "   âœ… Confirmed Actively Billing: 11,489\n",
      "\n",
      "   Breakdown of excluded:\n",
      "      No OSS_Order__c in SF: 2,083\n",
      "      CL - Bill End Passed: 23\n",
      "      Not Active State (CA): 6\n",
      "      CL - Bill Start Future: 6\n",
      "      No OSS Match: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_41320\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n",
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_41320\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: FILTER - ACTIVELY BILLING ONLY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: FILTER - ACTIVELY BILLING ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get common columns\n",
    "common_cols = [col for col in orders_after_rt_filter.columns]\n",
    "oss_cols = [\n",
    "    \"order_state_cd\",\n",
    "    \"OSS_State_Desc\",\n",
    "    \"bill_start_date\",\n",
    "    \"bill_end_date\",\n",
    "    \"OSS_Billing_Status\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "for col in oss_cols:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = None\n",
    "    if col not in orders_without_oss.columns:\n",
    "        orders_without_oss[col] = None\n",
    "\n",
    "all_cols = common_cols + oss_cols\n",
    "\n",
    "# Combine orders with and without OSS\n",
    "all_checked_orders = pd.concat(\n",
    "    [\n",
    "        merged_df[[c for c in all_cols if c in merged_df.columns]],\n",
    "        orders_without_oss[[c for c in all_cols if c in orders_without_oss.columns]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Split into actively billing vs not\n",
    "actively_billing_mask = all_checked_orders[\"OSS_Billing_Status\"] == \"ACTIVELY BILLING\"\n",
    "orders_actively_billing = all_checked_orders[actively_billing_mask].copy()\n",
    "excluded_not_billing_df = all_checked_orders[~actively_billing_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Actively Billing Filter:\")\n",
    "print(f\"   Before: {len(all_checked_orders):,}\")\n",
    "print(f\"   âŒ Excluded (Not Actively Billing): {len(excluded_not_billing_df):,}\")\n",
    "print(f\"   âœ… Confirmed Actively Billing: {len(orders_actively_billing):,}\")\n",
    "\n",
    "# Show breakdown of excluded\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    print(f\"\\n   Breakdown of excluded:\")\n",
    "    for status, count in (\n",
    "        excluded_not_billing_df[\"OSS_Billing_Status\"].value_counts().items()\n",
    "    ):\n",
    "        print(f\"      {status}: {count:,}\")\n",
    "\n",
    "# Set this as our active orders for the rest of the pipeline\n",
    "active_orders_df = orders_actively_billing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: QUERYING NEW BBF BANS\n",
      "================================================================================\n",
      "Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\n",
      "\n",
      "âœ… Found 2,505 new BBF BANs\n",
      "   Legacy BAN to BBF BAN mappings: 2,505\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: GET NEW BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: QUERYING NEW BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bbf_ban_query = \"\"\"\n",
    "SELECT \n",
    "    Id,\n",
    "    Name,\n",
    "    Account__c,\n",
    "    Account__r.Name,\n",
    "    Legacy_ES_Id__c,\n",
    "    BBF_Ban__c,\n",
    "    Billing_Address_1__c,\n",
    "    Billing_City__c,\n",
    "    Billing_State__c,\n",
    "    Billing_ZIP__c,\n",
    "    Payment_Terms__c,\n",
    "    Active_Billing__c\n",
    "FROM Billing_Invoice__c\n",
    "WHERE BBF_Ban__c = true\n",
    "  AND Legacy_ES_Id__c != null\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\")\n",
    "result = es_sf.query_all(bbf_ban_query)\n",
    "bbf_bans_df = pd.DataFrame(result[\"records\"])\n",
    "\n",
    "if len(bbf_bans_df) > 0:\n",
    "    # Flatten Account name\n",
    "    if \"Account__r\" in bbf_bans_df.columns:\n",
    "        bbf_bans_df[\"Account_Name\"] = bbf_bans_df[\"Account__r\"].apply(\n",
    "            lambda x: x[\"Name\"] if x else None\n",
    "        )\n",
    "\n",
    "    # Clean up\n",
    "    bbf_bans_df = bbf_bans_df.drop(\n",
    "        columns=[\"attributes\", \"Account__r\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Found {len(bbf_bans_df):,} new BBF BANs\")\n",
    "\n",
    "    # Create lookup from legacy BAN ID to new BBF BAN\n",
    "    legacy_to_bbf_ban = {}\n",
    "    for _, ban in bbf_bans_df.iterrows():\n",
    "        legacy_id = ban.get(\"Legacy_ES_Id__c\")\n",
    "        if legacy_id:\n",
    "            legacy_to_bbf_ban[legacy_id] = {\n",
    "                \"Id\": ban[\"Id\"],\n",
    "                \"Name\": ban[\"Name\"],\n",
    "                \"Account__c\": ban.get(\"Account__c\"),\n",
    "                \"Account_Name\": ban.get(\"Account_Name\"),\n",
    "            }\n",
    "    print(f\"   Legacy BAN to BBF BAN mappings: {len(legacy_to_bbf_ban):,}\")\n",
    "else:\n",
    "    legacy_to_bbf_ban = {}\n",
    "    print(\"âš ï¸ No BBF BANs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: MAPPING ORDERS TO BBF BANS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š BAN Mapping Results:\n",
      "   âœ… Ready to migrate (has BBF BAN): 11,475\n",
      "   âš ï¸ Missing BBF BAN mapping: 12\n",
      "   âŒ Missing ANY BAN: 2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: MAP ORDERS TO BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: MAPPING ORDERS TO BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track which orders can be mapped\n",
    "orders_with_bbf_ban = []\n",
    "orders_missing_bbf_ban = []\n",
    "orders_missing_any_ban = []\n",
    "\n",
    "for _, order in active_orders_df.iterrows():\n",
    "    legacy_ban_id = order.get(\"Billing_Invoice__c\")\n",
    "    order_dict = order.to_dict()\n",
    "\n",
    "    if not legacy_ban_id:\n",
    "        # Order has no BAN at all\n",
    "        orders_missing_any_ban.append(order_dict)\n",
    "    elif legacy_ban_id in legacy_to_bbf_ban:\n",
    "        # Order can be mapped to new BBF BAN\n",
    "        bbf_ban = legacy_to_bbf_ban[legacy_ban_id]\n",
    "        order_dict[\"New_BBF_BAN_Id\"] = bbf_ban[\"Id\"]\n",
    "        order_dict[\"New_BBF_BAN_Name\"] = bbf_ban[\"Name\"]\n",
    "        order_dict[\"New_BBF_BAN_Account__c\"] = bbf_ban[\"Account__c\"]\n",
    "        order_dict[\"New_BBF_BAN_Account_Name\"] = bbf_ban[\"Account_Name\"]\n",
    "        orders_with_bbf_ban.append(order_dict)\n",
    "    else:\n",
    "        # Order has legacy BAN but no BBF BAN mapping\n",
    "        orders_missing_bbf_ban.append(order_dict)\n",
    "\n",
    "orders_ready_df = pd.DataFrame(orders_with_bbf_ban)\n",
    "orders_no_bbf_ban_df = pd.DataFrame(orders_missing_bbf_ban)\n",
    "orders_no_ban_df = pd.DataFrame(orders_missing_any_ban)\n",
    "\n",
    "print(f\"\\nðŸ“Š BAN Mapping Results:\")\n",
    "print(f\"   âœ… Ready to migrate (has BBF BAN): {len(orders_ready_df):,}\")\n",
    "print(f\"   âš ï¸ Missing BBF BAN mapping: {len(orders_no_bbf_ban_df):,}\")\n",
    "print(f\"   âŒ Missing ANY BAN: {len(orders_no_ban_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Found 2,224 unique Accounts from new BBF BANs\n",
      "\n",
      "âœ… Accounts to migrate: 2,224\n"
     ]
    }
   ],
   "source": [
    "# === STEP 9: IDENTIFY ACCOUNTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    unique_account_ids = (\n",
    "        orders_ready_df[\"New_BBF_BAN_Account__c\"].dropna().unique().tolist()\n",
    "    )\n",
    "    print(f\"\\nFound {len(unique_account_ids):,} unique Accounts from new BBF BANs\")\n",
    "\n",
    "    if unique_account_ids:\n",
    "        chunk_size = 150\n",
    "        all_accounts = []\n",
    "\n",
    "        for i in range(0, len(unique_account_ids), chunk_size):\n",
    "            chunk = unique_account_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            account_query = f\"\"\"\n",
    "            SELECT Id, Name, Type, Industry, \n",
    "                   BillingStreet, BillingCity, BillingState, BillingPostalCode, BillingCountry,\n",
    "                   Phone, Website, BBF_New_Id__c\n",
    "            FROM Account\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(account_query)\n",
    "            all_accounts.extend(result[\"records\"])\n",
    "\n",
    "        accounts_df = pd.DataFrame(all_accounts)\n",
    "        if len(accounts_df) > 0:\n",
    "            accounts_df = accounts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Accounts to migrate: {len(accounts_df):,}\")\n",
    "    else:\n",
    "        accounts_df = pd.DataFrame()\n",
    "else:\n",
    "    accounts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Account query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: IDENTIFYING CONTACTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Contacts to migrate: 15,576\n"
     ]
    }
   ],
   "source": [
    "# === STEP 10: IDENTIFY CONTACTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 10: IDENTIFYING CONTACTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(accounts_df) > 0:\n",
    "    account_ids = accounts_df[\"Id\"].tolist()\n",
    "    chunk_size = 150\n",
    "    all_contacts = []\n",
    "\n",
    "    for i in range(0, len(account_ids), chunk_size):\n",
    "        chunk = account_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        contact_query = f\"\"\"\n",
    "        SELECT Id, AccountId, FirstName, LastName, Email, Phone, Title,\n",
    "               MailingStreet, MailingCity, MailingState, MailingPostalCode,\n",
    "               BBF_New_Id__c\n",
    "        FROM Contact\n",
    "        WHERE AccountId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(contact_query)\n",
    "        all_contacts.extend(result[\"records\"])\n",
    "\n",
    "    contacts_df = pd.DataFrame(all_contacts)\n",
    "    if len(contacts_df) > 0:\n",
    "        contacts_df = contacts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "    print(f\"\\nâœ… Contacts to migrate: {len(contacts_df):,}\")\n",
    "else:\n",
    "    contacts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No accounts to migrate, skipping Contact query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Unique locations referenced by orders:\n",
      "   Address_A: 723\n",
      "   Address_Z: 9,818\n",
      "   Combined unique: 10,175\n",
      "\n",
      "âœ… Locations to migrate: 10,175\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11: IDENTIFY LOCATIONS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    address_a_ids = orders_ready_df[\"Address_A__c\"].dropna().unique().tolist()\n",
    "    address_z_ids = orders_ready_df[\"Address_Z__c\"].dropna().unique().tolist()\n",
    "    all_address_ids = list(set(address_a_ids + address_z_ids))\n",
    "\n",
    "    print(f\"\\nUnique locations referenced by orders:\")\n",
    "    print(f\"   Address_A: {len(address_a_ids):,}\")\n",
    "    print(f\"   Address_Z: {len(address_z_ids):,}\")\n",
    "    print(f\"   Combined unique: {len(all_address_ids):,}\")\n",
    "\n",
    "    if all_address_ids:\n",
    "        chunk_size = 150\n",
    "        all_locations = []\n",
    "\n",
    "        for i in range(0, len(all_address_ids), chunk_size):\n",
    "            chunk = all_address_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            location_query = f\"\"\"\n",
    "            SELECT Id, Name, Address__c, City__c, State__c, County__c, Zip__c,\n",
    "                   Complete_Address__c, CLLI__c, Building_Status__c, On_Net__c,\n",
    "                   BBF_New_Id__c\n",
    "            FROM Address__c\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(location_query)\n",
    "            all_locations.extend(result[\"records\"])\n",
    "\n",
    "        locations_df = pd.DataFrame(all_locations)\n",
    "        if len(locations_df) > 0:\n",
    "            locations_df = locations_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Locations to migrate: {len(locations_df):,}\")\n",
    "    else:\n",
    "        locations_df = pd.DataFrame()\n",
    "else:\n",
    "    locations_df = pd.DataFrame()\n",
    "    all_address_ids = []\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Location query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Off_Net records to migrate: 2,157\n"
     ]
    }
   ],
   "source": [
    "# === STEP 12: IDENTIFY OFF_NET RECORDS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_address_ids) > 0:\n",
    "    chunk_size = 100\n",
    "    all_offnet = []\n",
    "\n",
    "    for i in range(0, len(all_address_ids), chunk_size):\n",
    "        chunk = all_address_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        offnet_query = f\"\"\"\n",
    "        SELECT Id, Name, \n",
    "               Location_1__c, Location_1_Address__c,\n",
    "               Location_2__c, Location_2_Address__c,\n",
    "               Off_Net_Vendor__c, Vendor_Name__c,\n",
    "               Vendor_circuit_Id__c, Internal_Circuit_Id__c,\n",
    "               Cost_MRC__c, Cost_NRC__c, Invoice_MRC__c,\n",
    "               LEC_Order_Status__c, Off_Net_Type__c,\n",
    "               Bandwidth__c, Circuit_Type__c, Term__c,\n",
    "               Term_Agreement_Start_Date__c, Term_Agreement_End_Date__c,\n",
    "               Vendor_Bill_Start_Date__c, Vendor_Bill_Stop_Date__c,\n",
    "               SOF1__c\n",
    "        FROM Off_Net__c\n",
    "        WHERE Location_1__c IN ('{ids_str}')\n",
    "           OR Location_2__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(offnet_query)\n",
    "        all_offnet.extend(result[\"records\"])\n",
    "\n",
    "    offnet_df = pd.DataFrame(all_offnet)\n",
    "    if len(offnet_df) > 0:\n",
    "        offnet_df = offnet_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        offnet_df = offnet_df.drop_duplicates(subset=[\"Id\"])\n",
    "    print(f\"\\nâœ… Off_Net records to migrate: {len(offnet_df):,}\")\n",
    "else:\n",
    "    offnet_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No locations to migrate, skipping Off_Net query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13: DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\n",
      "   âœ… Has Service_Start_Date__c:           7,050 (61.4%)\n",
      "   âœ… Has Billing_Start_Date__c (fallback): 4,425 (38.6%)\n",
      "   âš ï¸ Needs OSS bill_start_date:           0 (0.0%)\n",
      "\n",
      "ðŸ“Š Data Quality Issues:\n",
      "   [HIGH] Orders missing Address_A__c: 2 (0.0%)\n",
      "         â†’ Cannot set A_Location__c on BBF Service__c\n",
      "   [LOW] Orders missing Node__c: 9784 (85.3%)\n",
      "         â†’ Can fix post-migration - A_Node__c/Z_Node__c optional\n",
      "   [LOW] Orders missing Service_Start_Date__c: 4425 (38.6%)\n",
      "         â†’ Billing_Start_Date__c covers 4,425, OSS covers 0 more\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13: DATA QUALITY ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_quality_issues = []\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    total_ready = len(orders_ready_df)\n",
    "\n",
    "    # Check for missing Address_A__c\n",
    "    missing_addr_a = orders_ready_df[\"Address_A__c\"].isna().sum()\n",
    "    if missing_addr_a > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Address_A__c\",\n",
    "                \"Count\": missing_addr_a,\n",
    "                \"Percentage\": f\"{missing_addr_a/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"HIGH\",\n",
    "                \"Impact\": \"Cannot set A_Location__c on BBF Service__c\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check for missing Node__c\n",
    "    missing_node = orders_ready_df[\"Node__c\"].isna().sum()\n",
    "    if missing_node > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Node__c\",\n",
    "                \"Count\": missing_node,\n",
    "                \"Percentage\": f\"{missing_node/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can fix post-migration - A_Node__c/Z_Node__c optional\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check Billing/Service Date cascade: Service_Start_Date__c â†’ Billing_Start_Date__c â†’ OSS bill_start_date\n",
    "    sf_svc_start_null = orders_ready_df[\"Service_Start_Date__c\"].isna()\n",
    "    sf_bill_start_null = orders_ready_df[\"Billing_Start_Date__c\"].isna()\n",
    "    oss_start_exists = orders_ready_df[\"bill_start_date\"].notna()\n",
    "\n",
    "    # Count coverage at each level\n",
    "    has_svc_start = (~sf_svc_start_null).sum()\n",
    "    has_bill_start_only = (sf_svc_start_null & ~sf_bill_start_null).sum()\n",
    "    needs_oss = (sf_svc_start_null & sf_bill_start_null).sum()\n",
    "    oss_covers = (sf_svc_start_null & sf_bill_start_null & oss_start_exists).sum()\n",
    "\n",
    "    print(f\"\\nðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\")\n",
    "    print(\n",
    "        f\"   âœ… Has Service_Start_Date__c:           {has_svc_start:,} ({100*has_svc_start/total_ready:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   âœ… Has Billing_Start_Date__c (fallback): {has_bill_start_only:,} ({100*has_bill_start_only/total_ready:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   âš ï¸ Needs OSS bill_start_date:           {needs_oss:,} ({100*needs_oss/total_ready:.1f}%)\"\n",
    "    )\n",
    "    if needs_oss > 0:\n",
    "        print(f\"      â””â”€ OSS covers: {oss_covers:,} of {needs_oss:,}\")\n",
    "\n",
    "    if sf_svc_start_null.sum() > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Service_Start_Date__c\",\n",
    "                \"Count\": sf_svc_start_null.sum(),\n",
    "                \"Percentage\": f\"{sf_svc_start_null.sum()/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": f\"Billing_Start_Date__c covers {has_bill_start_only:,}, OSS covers {oss_covers:,} more\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "data_quality_df = pd.DataFrame(data_quality_issues)\n",
    "\n",
    "if len(data_quality_issues) > 0:\n",
    "    print(\"\\nðŸ“Š Data Quality Issues:\")\n",
    "    for issue in data_quality_issues:\n",
    "        print(\n",
    "            f\"   [{issue['Severity']}] {issue['Issue']}: {issue['Count']} ({issue['Percentage']})\"\n",
    "        )\n",
    "        print(f\"         â†’ {issue['Impact']}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No significant data quality issues found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13B: QUERYING ORDER ITEMS\n",
      "================================================================================\n",
      "\n",
      "Querying OrderItems for 11,475 orders...\n",
      "   Chunk 10: 2,565 items so far...\n",
      "   Chunk 20: 5,539 items so far...\n",
      "   Chunk 30: 7,942 items so far...\n",
      "   Chunk 40: 10,003 items so far...\n",
      "   Chunk 50: 13,169 items so far...\n",
      "   Chunk 60: 16,267 items so far...\n",
      "   Chunk 70: 18,628 items so far...\n",
      "\n",
      "âœ… Total OrderItems retrieved: 20,608\n",
      "\n",
      "ðŸ“Š OrderItem Statistics:\n",
      "   Orders with items: 11,475\n",
      "   Min items per order: 1\n",
      "   Max items per order: 18\n",
      "   Avg items per order: 1.8\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13B: QUERY ORDER ITEMS FOR MIGRATION SCOPE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13B: QUERYING ORDER ITEMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    # Get all Order IDs from orders ready to migrate\n",
    "    order_ids = orders_ready_df[\"Id\"].unique().tolist()\n",
    "    print(f\"\\nQuerying OrderItems for {len(order_ids):,} orders...\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 150\n",
    "    all_order_items = []\n",
    "\n",
    "    for i in range(0, len(order_ids), chunk_size):\n",
    "        chunk = order_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        orderitem_query = f\"\"\"\n",
    "        SELECT \n",
    "            Id,\n",
    "            OrderId,\n",
    "            OrderItemNumber,\n",
    "            Product2Id,\n",
    "            Product_Name__c,\n",
    "            Product_Family__c,\n",
    "            Quantity,\n",
    "            UnitPrice,\n",
    "            TotalPrice,\n",
    "            ListPrice,\n",
    "            Total_MRC_Amortized__c,\n",
    "            NRC_IRU_FEE__c,\n",
    "            NRC_Non_Amortized__c,\n",
    "            Vendor_Fees_Monthly__c,\n",
    "            Vendor_NRC__c,\n",
    "            ServiceDate,\n",
    "            EndDate,\n",
    "            Contract_End_Month__c,\n",
    "            Description,\n",
    "            Bandwidth_NEW__c,\n",
    "            Bandwidth_Numerical__c,\n",
    "            Term__c,\n",
    "            Product_Service_Term__c,\n",
    "            Cancelled__c,\n",
    "            Last_Mile_Carrier__c,\n",
    "            Vendor_Last_Mile_CID__c,\n",
    "            SBQQ__ChargeType__c,\n",
    "            SBQQ__BillingFrequency__c,\n",
    "            SBQQ__Status__c,\n",
    "            OFF_NET_IDs__c,\n",
    "            CreatedDate,\n",
    "            LastModifiedDate\n",
    "        FROM OrderItem\n",
    "        WHERE OrderId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "\n",
    "        result = es_sf.query_all(orderitem_query)\n",
    "        all_order_items.extend(result[\"records\"])\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"   Chunk {i//chunk_size + 1}: {len(all_order_items):,} items so far...\"\n",
    "            )\n",
    "\n",
    "    order_items_df = pd.DataFrame(all_order_items)\n",
    "    if len(order_items_df) > 0:\n",
    "        order_items_df = order_items_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "\n",
    "    print(f\"\\nâœ… Total OrderItems retrieved: {len(order_items_df):,}\")\n",
    "\n",
    "    # Show basic stats\n",
    "    if len(order_items_df) > 0:\n",
    "        items_per_order = order_items_df.groupby(\"OrderId\").size()\n",
    "        print(f\"\\nðŸ“Š OrderItem Statistics:\")\n",
    "        print(f\"   Orders with items: {len(items_per_order):,}\")\n",
    "        print(f\"   Min items per order: {items_per_order.min()}\")\n",
    "        print(f\"   Max items per order: {items_per_order.max()}\")\n",
    "        print(f\"   Avg items per order: {items_per_order.mean():.1f}\")\n",
    "else:\n",
    "    order_items_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping OrderItem query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13C: ORDER ITEM ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Product Family Breakdown:\n",
      "   Point-to-Point (PTPS): 4,986 (24.2%)\n",
      "   Dedicated Internet Access (DIAS): 3,461 (16.8%)\n",
      "   IP: 3,323 (16.1%)\n",
      "   Hosted Voice (VOIC): 1,648 (8.0%)\n",
      "   Dark Fiber (DFBR): 1,467 (7.1%)\n",
      "   Point-to-MultiPoint (PMPS): 1,340 (6.5%)\n",
      "   Promotions: 876 (4.3%)\n",
      "   Managed Service (MSP): 771 (3.7%)\n",
      "   Handoff Type: 575 (2.8%)\n",
      "   Diversity: 497 (2.4%)\n",
      "   Additional Port: 277 (1.3%)\n",
      "   Tagged / Untagged: 234 (1.1%)\n",
      "   Logical Attributes: 230 (1.1%)\n",
      "   Routing: 125 (0.6%)\n",
      "   Managed Wave (MWAV): 106 (0.5%)\n",
      "   ... and 34 more families\n",
      "\n",
      "ðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\n",
      "   (null): 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Cancelled Status:\n",
      "   Cancelled=False: 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Financial Fields:\n",
      "   Total_MRC_Amortized__c: 20,608 populated, sum=$9,650,224.07\n",
      "   NRC_IRU_FEE__c: 8,376 populated, sum=$14,307,429.82\n",
      "   Vendor_Fees_Monthly__c: 17 populated, sum=$8,780.00\n",
      "\n",
      "ðŸ“Š OrderItem Data Quality:\n",
      "\n",
      "âœ… OrderItem analysis complete\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13C: ORDER ITEM ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13C: ORDER ITEM ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "orderitem_summary = []\n",
    "orderitem_data_quality = []\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    total_items = len(order_items_df)\n",
    "\n",
    "    # === PRODUCT FAMILY BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Product Family Breakdown:\")\n",
    "    family_counts = order_items_df[\"Product_Family__c\"].value_counts(dropna=False)\n",
    "    for family, count in family_counts.head(15).items():\n",
    "        pct = 100 * count / total_items\n",
    "        family_name = family if family else \"(null)\"\n",
    "        print(f\"   {family_name}: {count:,} ({pct:.1f}%)\")\n",
    "    if len(family_counts) > 15:\n",
    "        print(f\"   ... and {len(family_counts) - 15} more families\")\n",
    "\n",
    "    # === CHARGE TYPE BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\")\n",
    "    charge_counts = order_items_df[\"SBQQ__ChargeType__c\"].value_counts(dropna=False)\n",
    "    for charge, count in charge_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        charge_name = charge if charge else \"(null)\"\n",
    "        print(f\"   {charge_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === CANCELLED BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Cancelled Status:\")\n",
    "    cancelled_counts = order_items_df[\"Cancelled__c\"].value_counts(dropna=False)\n",
    "    for status, count in cancelled_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        status_name = str(status) if status is not None else \"(null)\"\n",
    "        print(f\"   Cancelled={status_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === MRC/NRC ANALYSIS ===\n",
    "    print(\"\\nðŸ“Š Financial Fields:\")\n",
    "\n",
    "    # Total MRC\n",
    "    mrc_populated = order_items_df[\"Total_MRC_Amortized__c\"].notna().sum()\n",
    "    mrc_sum = order_items_df[\"Total_MRC_Amortized__c\"].sum()\n",
    "    print(\n",
    "        f\"   Total_MRC_Amortized__c: {mrc_populated:,} populated, sum=${mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # NRC\n",
    "    nrc_populated = order_items_df[\"NRC_IRU_FEE__c\"].notna().sum()\n",
    "    nrc_sum = order_items_df[\"NRC_IRU_FEE__c\"].sum()\n",
    "    print(f\"   NRC_IRU_FEE__c: {nrc_populated:,} populated, sum=${nrc_sum:,.2f}\")\n",
    "\n",
    "    # Vendor costs\n",
    "    vendor_mrc_populated = order_items_df[\"Vendor_Fees_Monthly__c\"].notna().sum()\n",
    "    vendor_mrc_sum = order_items_df[\"Vendor_Fees_Monthly__c\"].sum()\n",
    "    print(\n",
    "        f\"   Vendor_Fees_Monthly__c: {vendor_mrc_populated:,} populated, sum=${vendor_mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # === DATA QUALITY CHECKS ===\n",
    "    print(\"\\nðŸ“Š OrderItem Data Quality:\")\n",
    "\n",
    "    # Missing Product Name\n",
    "    missing_product = order_items_df[\"Product_Name__c\"].isna().sum()\n",
    "    if missing_product > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Name__c\",\n",
    "                \"Count\": missing_product,\n",
    "                \"Percentage\": f\"{100*missing_product/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need product mapping for Service_Charge__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Name__c: {missing_product:,} ({100*missing_product/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Product Family\n",
    "    missing_family = order_items_df[\"Product_Family__c\"].isna().sum()\n",
    "    if missing_family > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Family__c\",\n",
    "                \"Count\": missing_family,\n",
    "                \"Percentage\": f\"{100*missing_family/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Product_Name__c as fallback\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Family__c: {missing_family:,} ({100*missing_family/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Unit Price\n",
    "    missing_price = order_items_df[\"UnitPrice\"].isna().sum()\n",
    "    if missing_price > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing UnitPrice\",\n",
    "                \"Count\": missing_price,\n",
    "                \"Percentage\": f\"{100*missing_price/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need price for Service_Charge__c.Amount__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing UnitPrice: {missing_price:,} ({100*missing_price/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Cancelled items\n",
    "    cancelled_items = order_items_df[\"Cancelled__c\"].fillna(False).sum()\n",
    "    if cancelled_items > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems marked as Cancelled\",\n",
    "                \"Count\": int(cancelled_items),\n",
    "                \"Percentage\": f\"{100*cancelled_items/total_items:.1f}%\",\n",
    "                \"Severity\": \"INFO\",\n",
    "                \"Impact\": \"Consider excluding from migration\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   â„¹ï¸ Cancelled items: {int(cancelled_items):,} ({100*cancelled_items/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing ServiceDate\n",
    "    missing_svc_date = order_items_df[\"ServiceDate\"].isna().sum()\n",
    "    if missing_svc_date > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing ServiceDate\",\n",
    "                \"Count\": missing_svc_date,\n",
    "                \"Percentage\": f\"{100*missing_svc_date/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Order-level date or OSS bill_start_date\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing ServiceDate: {missing_svc_date:,} ({100*missing_svc_date/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # === BUILD SUMMARY ===\n",
    "    orderitem_summary = [\n",
    "        {\"Metric\": \"Total OrderItems\", \"Value\": total_items},\n",
    "        {\"Metric\": \"Unique Orders\", \"Value\": order_items_df[\"OrderId\"].nunique()},\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Families\",\n",
    "            \"Value\": order_items_df[\"Product_Family__c\"].nunique(),\n",
    "        },\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Names\",\n",
    "            \"Value\": order_items_df[\"Product_Name__c\"].nunique(),\n",
    "        },\n",
    "        {\"Metric\": \"Total MRC (sum)\", \"Value\": f\"${mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total NRC (sum)\", \"Value\": f\"${nrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total Vendor MRC (sum)\", \"Value\": f\"${vendor_mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Cancelled Items\", \"Value\": int(cancelled_items)},\n",
    "        {\n",
    "            \"Metric\": \"Active Items (not cancelled)\",\n",
    "            \"Value\": total_items - int(cancelled_items),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"\\nâœ… OrderItem analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No OrderItems to analyze\")\n",
    "\n",
    "orderitem_summary_df = pd.DataFrame(orderitem_summary)\n",
    "orderitem_dq_df = pd.DataFrame(orderitem_data_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13D: PRODUCT FAMILY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Product Family Summary created: 49 families\n",
      "\n",
      "Top 10 by Item Count:\n",
      "                      Product_Family  Item_Count  Order_Count   Total_MRC\n",
      "33             Point-to-Point (PTPS)        4986         4969  3963154.84\n",
      "11  Dedicated Internet Access (DIAS)        3461         3431  2932247.50\n",
      "21                                IP        3323         3165     4473.25\n",
      "20               Hosted Voice (VOIC)        1648          417   189779.34\n",
      "6                  Dark Fiber (DFBR)        1467          771   601983.15\n",
      "32        Point-to-MultiPoint (PMPS)        1340         1340   970607.99\n",
      "37                        Promotions         876          868    -9700.00\n",
      "26             Managed Service (MSP)         771          270   126825.24\n",
      "18                      Handoff Type         575          573        0.00\n",
      "13                         Diversity         497          247       20.00\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13D: PRODUCT FAMILY SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13D: PRODUCT FAMILY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    # Create product family summary with financials\n",
    "    product_summary = (\n",
    "        order_items_df.groupby(\"Product_Family__c\", dropna=False)\n",
    "        .agg(\n",
    "            {\n",
    "                \"Id\": \"count\",\n",
    "                \"OrderId\": \"nunique\",\n",
    "                \"Total_MRC_Amortized__c\": \"sum\",\n",
    "                \"NRC_IRU_FEE__c\": \"sum\",\n",
    "                \"Vendor_Fees_Monthly__c\": \"sum\",\n",
    "                \"UnitPrice\": \"mean\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    product_summary.columns = [\n",
    "        \"Product_Family\",\n",
    "        \"Item_Count\",\n",
    "        \"Order_Count\",\n",
    "        \"Total_MRC\",\n",
    "        \"Total_NRC\",\n",
    "        \"Total_Vendor_MRC\",\n",
    "        \"Avg_UnitPrice\",\n",
    "    ]\n",
    "\n",
    "    # Sort by item count\n",
    "    product_summary = product_summary.sort_values(\"Item_Count\", ascending=False)\n",
    "\n",
    "    # Fill nulls for display\n",
    "    product_summary[\"Product_Family\"] = product_summary[\"Product_Family\"].fillna(\n",
    "        \"(null)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Product Family Summary created: {len(product_summary)} families\")\n",
    "    print(\"\\nTop 10 by Item Count:\")\n",
    "    print(\n",
    "        product_summary.head(10)[\n",
    "            [\"Product_Family\", \"Item_Count\", \"Order_Count\", \"Total_MRC\"]\n",
    "        ].to_string()\n",
    "    )\n",
    "else:\n",
    "    product_summary = pd.DataFrame()\n",
    "\n",
    "product_summary_df = product_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 14: CREATING BAN MAPPING TABLE\n",
      "================================================================================\n",
      "\n",
      "âœ… BAN mappings with orders: 2,440\n"
     ]
    }
   ],
   "source": [
    "# === STEP 14: CREATE BAN MAPPING TABLE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 14: CREATING BAN MAPPING TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    ban_mapping_data = []\n",
    "    legacy_ban_counts = orders_ready_df.groupby(\"Billing_Invoice__c\").size().to_dict()\n",
    "\n",
    "    for legacy_id, bbf_ban in legacy_to_bbf_ban.items():\n",
    "        order_count = legacy_ban_counts.get(legacy_id, 0)\n",
    "        if order_count > 0:\n",
    "            ban_mapping_data.append(\n",
    "                {\n",
    "                    \"Legacy_BAN_Id\": legacy_id,\n",
    "                    \"New_BBF_BAN_Id\": bbf_ban[\"Id\"],\n",
    "                    \"New_BBF_BAN_Name\": bbf_ban[\"Name\"],\n",
    "                    \"Account__c\": bbf_ban[\"Account__c\"],\n",
    "                    \"Account_Name\": bbf_ban[\"Account_Name\"],\n",
    "                    \"Order_Count\": order_count,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    ban_mapping_df = pd.DataFrame(ban_mapping_data)\n",
    "    ban_mapping_df = ban_mapping_df.sort_values(\"Order_Count\", ascending=False)\n",
    "    print(f\"\\nâœ… BAN mappings with orders: {len(ban_mapping_df):,}\")\n",
    "else:\n",
    "    ban_mapping_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 15: MIGRATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "FILTER PIPELINE:\n",
      "  Total Active Status Orders:     17,972\n",
      "  â”œâ”€ Excluded (PA MARKET DECOM):  887\n",
      "  â”œâ”€ Excluded (Work Orders):      3,477\n",
      "  â”œâ”€ Excluded (Not Billing OSS):  2,119\n",
      "  â””â”€ Confirmed Actively Billing:  11,489\n",
      "\n",
      "BAN MAPPING:\n",
      "  â”œâ”€ Ready to migrate:            11,475\n",
      "  â”œâ”€ Missing BBF BAN:             12\n",
      "  â””â”€ Missing ANY BAN:             2\n",
      "\n",
      "ORDER ITEMS (v4 NEW):\n",
      "  â”œâ”€ Total OrderItems:            20,608\n",
      "  â””â”€ Unique Product Families:     49\n",
      "\n",
      "RECORDS TO MIGRATE:\n",
      "  â”œâ”€ BANs:                        2,440\n",
      "  â”œâ”€ Accounts:                    2,224\n",
      "  â”œâ”€ Contacts:                    15,576\n",
      "  â”œâ”€ Locations:                   10,175\n",
      "  â””â”€ Off_Net:                     2,157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === STEP 15: GENERATE SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 15: MIGRATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get OrderItem count safely\n",
    "orderitem_count = (\n",
    "    len(order_items_df) if \"order_items_df\" in dir() and len(order_items_df) > 0 else 0\n",
    ")\n",
    "product_family_count = (\n",
    "    order_items_df[\"Product_Family__c\"].nunique() if orderitem_count > 0 else 0\n",
    ")\n",
    "\n",
    "summary_data = [\n",
    "    {\"Category\": \"FILTER PIPELINE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"1. Total Active Status Orders\",\n",
    "        \"Count\": len(all_orders_df),\n",
    "        \"Notes\": \"All orders with qualifying status\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"2. Excluded (PA MARKET DECOM)\",\n",
    "        \"Count\": len(excluded_pa_decom_df),\n",
    "        \"Notes\": \"Project_Group__c contains 'PA MARKET DECOM'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"3. Excluded (Work Orders)\",\n",
    "        \"Count\": len(excluded_work_orders_df),\n",
    "        \"Notes\": f\"Service_Order_Record_Type__c != '{VALID_RECORD_TYPE}'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"4. Excluded (Not Actively Billing)\",\n",
    "        \"Count\": len(excluded_not_billing_df),\n",
    "        \"Notes\": f\"OSS: not in {ACTIVE_OSS_STATES} or billing dates invalid\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"5. Confirmed Actively Billing\",\n",
    "        \"Count\": len(active_orders_df),\n",
    "        \"Notes\": \"Passed all filters\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"BAN MAPPING\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Ready (has BBF BAN)\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"Can be migrated now\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing BBF BAN\",\n",
    "        \"Count\": len(orders_no_bbf_ban_df),\n",
    "        \"Notes\": \"Need new BBF BAN created\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing ANY BAN\",\n",
    "        \"Count\": len(orders_no_ban_df),\n",
    "        \"Notes\": \"CRITICAL - no BAN reference\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"ORDER ITEMS (v4)\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Total OrderItems\",\n",
    "        \"Count\": orderitem_count,\n",
    "        \"Notes\": \"For orders ready to migrate\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Unique Product Families\",\n",
    "        \"Count\": product_family_count,\n",
    "        \"Notes\": \"Distinct Product_Family__c values\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"RECORDS TO MIGRATE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"BANs (with orders)\",\n",
    "        \"Count\": len(ban_mapping_df),\n",
    "        \"Notes\": \"Billing_Invoice__c to BAN__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Accounts\",\n",
    "        \"Count\": len(accounts_df),\n",
    "        \"Notes\": \"Unique accounts from BBF BANs\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Contacts\",\n",
    "        \"Count\": len(contacts_df),\n",
    "        \"Notes\": \"Contacts for migration accounts\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Locations\",\n",
    "        \"Count\": len(locations_df),\n",
    "        \"Notes\": \"Address_A + Address_Z from orders\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Off_Net\",\n",
    "        \"Count\": len(offnet_df),\n",
    "        \"Notes\": \"Off_Net for migration locations\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "ðŸ“Š SUMMARY\n",
    "{'â”€'*50}\n",
    "FILTER PIPELINE:\n",
    "  Total Active Status Orders:     {len(all_orders_df):,}\n",
    "  â”œâ”€ Excluded (PA MARKET DECOM):  {len(excluded_pa_decom_df):,}\n",
    "  â”œâ”€ Excluded (Work Orders):      {len(excluded_work_orders_df):,}\n",
    "  â”œâ”€ Excluded (Not Billing OSS):  {len(excluded_not_billing_df):,}\n",
    "  â””â”€ Confirmed Actively Billing:  {len(active_orders_df):,}\n",
    "\n",
    "BAN MAPPING:\n",
    "  â”œâ”€ Ready to migrate:            {len(orders_ready_df):,}\n",
    "  â”œâ”€ Missing BBF BAN:             {len(orders_no_bbf_ban_df):,}\n",
    "  â””â”€ Missing ANY BAN:             {len(orders_no_ban_df):,}\n",
    "\n",
    "ORDER ITEMS (v4 NEW):\n",
    "  â”œâ”€ Total OrderItems:            {orderitem_count:,}\n",
    "  â””â”€ Unique Product Families:     {product_family_count:,}\n",
    "\n",
    "RECORDS TO MIGRATE:\n",
    "  â”œâ”€ BANs:                        {len(ban_mapping_df):,}\n",
    "  â”œâ”€ Accounts:                    {len(accounts_df):,}\n",
    "  â”œâ”€ Contacts:                    {len(contacts_df):,}\n",
    "  â”œâ”€ Locations:                   {len(locations_df):,}\n",
    "  â””â”€ Off_Net:                     {len(offnet_df):,}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 16: EXPORTING TO EXCEL\n",
      "================================================================================\n",
      "   âœ… Summary\n",
      "   âœ… Active_Orders (11,475 records)\n",
      "   âœ… OrderItems (20,608 records)\n",
      "   âœ… OrderItem_Summary (9 metrics)\n",
      "   âœ… Product_Family_Summary (49 families)\n",
      "   âœ… BAN_Mapping (2,440 records)\n",
      "   âœ… Accounts (2,224 records)\n",
      "   âœ… Contacts (15,576 records)\n",
      "   âœ… Locations (10,175 records)\n",
      "   âœ… Off_Net (2,157 records)\n",
      "   âœ… Data_Quality (3 issues)\n",
      "   âœ… Orders_Missing_BBF_BAN (12 records)\n",
      "   âœ… Excluded_PA_DECOM (887 records)\n",
      "   âœ… Excluded_Work_Orders (3,477 records)\n",
      "   âœ… Excluded_Not_Billing (2,119 records)\n",
      "\n",
      "âœ… Excel file saved: es_bbf_migration_analysis_v4_20260107_212249.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === STEP 16: EXPORT TO EXCEL ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 16: EXPORTING TO EXCEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "wb = Workbook()\n",
    "ws_summary = wb.active\n",
    "ws_summary.title = \"Summary\"\n",
    "\n",
    "# Styles\n",
    "header_font = Font(bold=True, size=12, color=\"FFFFFF\")\n",
    "header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "thin_border = Border(\n",
    "    left=Side(style=\"thin\"),\n",
    "    right=Side(style=\"thin\"),\n",
    "    top=Side(style=\"thin\"),\n",
    "    bottom=Side(style=\"thin\"),\n",
    ")\n",
    "\n",
    "\n",
    "def write_df_to_sheet(ws, df, start_row=1):\n",
    "    \"\"\"Write dataframe to worksheet with formatting\"\"\"\n",
    "    for r_idx, row in enumerate(\n",
    "        dataframe_to_rows(df, index=False, header=True), start=start_row\n",
    "    ):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == start_row:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            cell.border = thin_border\n",
    "\n",
    "\n",
    "# --- SHEET 1: Summary ---\n",
    "ws_summary.append([\"ES â†’ BBF Migration Analysis (v4)\"])\n",
    "ws_summary[\"A1\"].font = Font(bold=True, size=16)\n",
    "ws_summary.append([f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"])\n",
    "ws_summary.append(\n",
    "    [\n",
    "        f\"OSS Active States: {ACTIVE_OSS_STATES} | Record Type filter | OSS Billing validation | OrderItem Analysis\"\n",
    "    ]\n",
    ")\n",
    "ws_summary.append([])\n",
    "write_df_to_sheet(ws_summary, summary_df, start_row=5)\n",
    "\n",
    "ws_summary.column_dimensions[\"A\"].width = 20\n",
    "ws_summary.column_dimensions[\"B\"].width = 40\n",
    "ws_summary.column_dimensions[\"C\"].width = 12\n",
    "ws_summary.column_dimensions[\"D\"].width = 55\n",
    "print(\"   âœ… Summary\")\n",
    "\n",
    "# --- SHEET 2: Active Orders (Ready) ---\n",
    "if len(orders_ready_df) > 0:\n",
    "    ws = wb.create_sheet(\"Active_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"New_BBF_BAN_Id\",\n",
    "        \"New_BBF_BAN_Name\",\n",
    "        \"Address_A__c\",\n",
    "        \"Address_Z__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "        \"Service_Start_Date__c\",\n",
    "        \"Billing_Start_Date__c\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_ready_df.columns]\n",
    "    write_df_to_sheet(ws, orders_ready_df[export_cols])\n",
    "    print(f\"   âœ… Active_Orders ({len(orders_ready_df):,} records)\")\n",
    "\n",
    "# --- SHEET 3: Order Items (NEW in v4) ---\n",
    "if \"order_items_df\" in dir() and len(order_items_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItems\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"OrderId\",\n",
    "        \"OrderItemNumber\",\n",
    "        \"Product_Name__c\",\n",
    "        \"Product_Family__c\",\n",
    "        \"Quantity\",\n",
    "        \"UnitPrice\",\n",
    "        \"TotalPrice\",\n",
    "        \"Total_MRC_Amortized__c\",\n",
    "        \"NRC_IRU_FEE__c\",\n",
    "        \"Vendor_Fees_Monthly__c\",\n",
    "        \"ServiceDate\",\n",
    "        \"EndDate\",\n",
    "        \"Bandwidth_NEW__c\",\n",
    "        \"Term__c\",\n",
    "        \"Cancelled__c\",\n",
    "        \"SBQQ__ChargeType__c\",\n",
    "        \"Last_Mile_Carrier__c\",\n",
    "        \"OFF_NET_IDs__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in order_items_df.columns]\n",
    "    write_df_to_sheet(ws, order_items_df[export_cols])\n",
    "    print(f\"   âœ… OrderItems ({len(order_items_df):,} records)\")\n",
    "\n",
    "# --- SHEET 4: OrderItem Summary (NEW in v4) ---\n",
    "if \"orderitem_summary_df\" in dir() and len(orderitem_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItem_Summary\")\n",
    "    write_df_to_sheet(ws, orderitem_summary_df)\n",
    "    print(f\"   âœ… OrderItem_Summary ({len(orderitem_summary_df):,} metrics)\")\n",
    "\n",
    "# --- SHEET 5: Product Family Summary (NEW in v4) ---\n",
    "if \"product_summary_df\" in dir() and len(product_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"Product_Family_Summary\")\n",
    "    write_df_to_sheet(ws, product_summary_df)\n",
    "    print(f\"   âœ… Product_Family_Summary ({len(product_summary_df):,} families)\")\n",
    "\n",
    "# --- SHEET 6: BAN Mapping ---\n",
    "if len(ban_mapping_df) > 0:\n",
    "    ws = wb.create_sheet(\"BAN_Mapping\")\n",
    "    write_df_to_sheet(ws, ban_mapping_df)\n",
    "    print(f\"   âœ… BAN_Mapping ({len(ban_mapping_df):,} records)\")\n",
    "\n",
    "# --- SHEET 7: Accounts ---\n",
    "if len(accounts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Accounts\")\n",
    "    write_df_to_sheet(ws, accounts_df)\n",
    "    print(f\"   âœ… Accounts ({len(accounts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 8: Contacts ---\n",
    "if len(contacts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Contacts\")\n",
    "    write_df_to_sheet(ws, contacts_df)\n",
    "    print(f\"   âœ… Contacts ({len(contacts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 9: Locations ---\n",
    "if len(locations_df) > 0:\n",
    "    ws = wb.create_sheet(\"Locations\")\n",
    "    write_df_to_sheet(ws, locations_df)\n",
    "    print(f\"   âœ… Locations ({len(locations_df):,} records)\")\n",
    "\n",
    "# --- SHEET 10: Off_Net ---\n",
    "if len(offnet_df) > 0:\n",
    "    ws = wb.create_sheet(\"Off_Net\")\n",
    "    write_df_to_sheet(ws, offnet_df)\n",
    "    print(f\"   âœ… Off_Net ({len(offnet_df):,} records)\")\n",
    "\n",
    "# --- SHEET 11: Data Quality (combined Order + OrderItem) ---\n",
    "combined_dq = data_quality_issues.copy()\n",
    "if \"orderitem_data_quality\" in dir() and len(orderitem_data_quality) > 0:\n",
    "    combined_dq.extend(orderitem_data_quality)\n",
    "\n",
    "if len(combined_dq) > 0:\n",
    "    ws = wb.create_sheet(\"Data_Quality\")\n",
    "    dq_df = pd.DataFrame(combined_dq)\n",
    "    write_df_to_sheet(ws, dq_df)\n",
    "    print(f\"   âœ… Data_Quality ({len(combined_dq):,} issues)\")\n",
    "\n",
    "# --- SHEET 12: Orders Missing BBF BAN ---\n",
    "if len(orders_no_bbf_ban_df) > 0:\n",
    "    ws = wb.create_sheet(\"Orders_Missing_BBF_BAN\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_no_bbf_ban_df.columns]\n",
    "    write_df_to_sheet(ws, orders_no_bbf_ban_df[export_cols])\n",
    "    print(f\"   âœ… Orders_Missing_BBF_BAN ({len(orders_no_bbf_ban_df):,} records)\")\n",
    "\n",
    "# --- SHEET 13: Excluded - PA MARKET DECOM ---\n",
    "if len(excluded_pa_decom_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_PA_DECOM\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Project_Group__c\",\n",
    "        \"Account_Name\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_pa_decom_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_pa_decom_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_PA_DECOM ({len(excluded_pa_decom_df):,} records)\")\n",
    "\n",
    "# --- SHEET 14: Excluded - Work Orders (enriched with OSS data) ---\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Work_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Service_Order_Record_Type__c\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_nm\",\n",
    "        \"order_id\",\n",
    "        \"workorder_type_cd\",\n",
    "        \"workorder_type_desc\",\n",
    "        \"workorder_state_cd\",\n",
    "        \"workorder_state_desc\",\n",
    "        \"description\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_work_orders_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_work_orders_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Work_Orders ({len(excluded_work_orders_df):,} records)\")\n",
    "\n",
    "# --- SHEET 15: Excluded - Not Actively Billing ---\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Not_Billing\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"OSS_State_Desc\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "        \"OSS_Billing_Status\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_not_billing_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_not_billing_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Not_Billing ({len(excluded_not_billing_df):,} records)\")\n",
    "\n",
    "# --- SHEET 16: OrderItem Data Quality (NEW in v4) ---\n",
    "if \"orderitem_dq_df\" in dir() and len(orderitem_dq_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItem_Data_Quality\")\n",
    "    write_df_to_sheet(ws, orderitem_dq_df)\n",
    "    print(f\"   âœ… OrderItem_Data_Quality ({len(orderitem_dq_df):,} issues)\")\n",
    "\n",
    "# Save\n",
    "wb.save(OUTPUT_FILE)\n",
    "print(f\"\\nâœ… Excel file saved: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANUP\n",
      "================================================================================\n",
      "âœ… OSS connection closed\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Output file: es_bbf_migration_analysis_v4_20260107_212249.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CLEANUP ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "oss_conn.close()\n",
    "print(\"âœ… OSS connection closed\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## v4 Changes Summary\n",
    "\n",
    "### NEW: OrderItem Analysis\n",
    "- Queries all OrderItems for orders in migration scope\n",
    "- Product Family breakdown with financial totals\n",
    "- Data quality checks on OrderItems\n",
    "- New sheets: OrderItems, OrderItem_Summary, Product_Family_Summary, OrderItem_Data_Quality\n",
    "\n",
    "### Inherited from v3\n",
    "- OSS Active States: CL (Closed) and OA (Accepted)\n",
    "- OSS `bill_start_date` available when SF `Service_Start_Date__c` is missing\n",
    "- Work Orders enriched with OSS workorder data\n",
    "\n",
    "### Field Mapping Reference\n",
    "See companion document: `ES_BBF_Field_Mapping_Proposal.md` for:\n",
    "- ES Order â†’ BBF Service__c mapping\n",
    "- ES OrderItem â†’ BBF Service_Charge__c mapping\n",
    "- Status value mappings\n",
    "- Product/Charge type mappings\n",
    "\n",
    "### Next Steps\n",
    "1. Review field mapping proposal\n",
    "2. Create Product mapping table (ES Product_Family â†’ BBF Product_Simple)\n",
    "3. Create Status mapping table (ES Status â†’ BBF Status__c)\n",
    "4. Create Service__c migration notebook\n",
    "5. Create Service_Charge__c migration notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
