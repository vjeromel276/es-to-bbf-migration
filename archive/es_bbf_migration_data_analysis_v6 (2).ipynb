{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES â†’ BBF Migration Data Analysis (v6)\n",
    "\n",
    "This notebook analyzes all data required for the ES to BBF Salesforce migration.\n",
    "\n",
    "## Version 6 Changes\n",
    "- **NEW**: Node analysis for A/Z endpoint determination\n",
    "- **NEW**: Ring data query and analysis\n",
    "- **NEW**: Node position analysis (East_POP vs West_POP vs NEITHER)\n",
    "- **NEW**: Node-to-Address text matching\n",
    "- **NEW**: Cross-tab analysis of Ring Position vs Order A/Z Position\n",
    "- **NEW**: Summary for process owner discussion\n",
    "\n",
    "## Version 5 Changes (inherited)\n",
    "- Comprehensive Legend sheet with full documentation\n",
    "- Field reference sheets for each object showing migration fields\n",
    "\n",
    "## Version 4 Changes (inherited)\n",
    "- OrderItem analysis for migration scope orders\n",
    "- OrderItem data quality checks\n",
    "- Product/Family breakdown analysis\n",
    "- Active Date uses `Billing_Start_Date__c` (primary) â†’ OSS `bill_start_date` (fallback)\n",
    "\n",
    "## Version 3 Changes (inherited)\n",
    "- Include OA (Accepted) orders in addition to CL (Closed)\n",
    "- Work Orders enriched with data from `workorders.workorders` table\n",
    "\n",
    "## Driving Principle\n",
    "**Everything is driven from Active ES Orders that are ACTUALLY BILLING in OSS** - we migrate only the data needed to support truly active services.\n",
    "\n",
    "## Filter Pipeline\n",
    "1. Status IN ('Activated', 'Suspended (Late Payment)', 'Disconnect in Progress')\n",
    "2. Project_Group__c NOT LIKE '%PA MARKET DECOM%'\n",
    "3. Service_Order_Record_Type__c = 'Service Order Agreement' (excludes Work Orders)\n",
    "4. OSS Actively Billing:\n",
    "   - order_state_cd IN ('CL', 'OA')\n",
    "   - bill_start_date <= today\n",
    "   - bill_end_date IS NULL or > today\n",
    "5. Has BBF BAN mapping\n",
    "\n",
    "## Node Analysis (NEW in v6)\n",
    "- Query Nodes via Service_Order_Agreement__c lookup to migrating Orders\n",
    "- Analyze Ring position (East_POP vs West_POP)\n",
    "- Cross-reference Node.Address__c with Order.Address_A/Z\n",
    "- Produces data for process owner discussion on A vs Z endpoint determination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\vjero\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe\n",
      "Pandas: 2.2.3\n",
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "# === SETUP & IMPORTS ===\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from simple_salesforce import Salesforce\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration loaded\n",
      "   Active Statuses: ['Activated', 'Suspended (Late Payment)', 'Disconnect in Progress']\n",
      "   Record Type: Service Order Agreement\n",
      "   Excluding: Project_Group__c LIKE '%PA MARKET DECOM%'\n",
      "   OSS Active States: ['CL', 'OA']\n",
      "   Output: es_bbf_migration_analysis_v6_20260108_131334.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# ES (Source) Salesforce Credentials\n",
    "ES_USERNAME = \"sfdcapi@everstream.net\"\n",
    "ES_PASSWORD = \"pV4CAxns8DQtJsBq!\"\n",
    "ES_TOKEN = \"r1uoYiusK19RbrflARydi86TA\"\n",
    "ES_DOMAIN = \"login\"  # 'login' for production\n",
    "\n",
    "# OSS Database Credentials\n",
    "OSS_HOST = \"pg01.comlink.net\"\n",
    "OSS_PORT = \"5432\"\n",
    "OSS_DB = \"GLC\"\n",
    "OSS_USER = \"oss_server\"\n",
    "OSS_PASSWORD = \"3wU3uB28X?!r2?@ebrUg\"\n",
    "\n",
    "# Active Order Status Filter\n",
    "ACTIVE_STATUSES = [\"Activated\", \"Suspended (Late Payment)\", \"Disconnect in Progress\"]\n",
    "\n",
    "# Record Type Filter\n",
    "VALID_RECORD_TYPE = \"Service Order Agreement\"\n",
    "\n",
    "# PA Market Decom Exclusion\n",
    "PA_DECOM_FILTER = \"PA MARKET DECOM\"\n",
    "\n",
    "# OSS Order States that qualify as \"actively billing\"\n",
    "ACTIVE_OSS_STATES = [\"CL\", \"OA\"]  # Closed and Accepted\n",
    "\n",
    "# OSS Order States Reference\n",
    "OSS_ORDER_STATES = {\n",
    "    \"CL\": \"Closed (Active/Billing)\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"OS\": \"Submitted\",\n",
    "    \"OC\": \"Created\",\n",
    "    \"PN\": \"Pending\",\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"OR\": \"Rejected\",\n",
    "    \"OV\": \"Validated (Disabled)\",\n",
    "}\n",
    "\n",
    "# OSS Work Order Types Reference\n",
    "WORKORDER_TYPES = {\n",
    "    \"IT\": \"Professional Services\",\n",
    "    \"MR\": \"Maintenance/Repair\",\n",
    "    \"OS\": \"Other Service\",\n",
    "    \"VS\": \"Voice Service\",\n",
    "}\n",
    "\n",
    "# OSS Work Order States Reference\n",
    "WORKORDER_STATES = {\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"CL\": \"Closed\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"PN\": \"Pending\",\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_FILE = f\"es_bbf_migration_analysis_v6_{TIMESTAMP}.xlsx\"\n",
    "\n",
    "print(\"ðŸ“‹ Configuration loaded\")\n",
    "print(f\"   Active Statuses: {ACTIVE_STATUSES}\")\n",
    "print(f\"   Record Type: {VALID_RECORD_TYPE}\")\n",
    "print(f\"   Excluding: Project_Group__c LIKE '%{PA_DECOM_FILTER}%'\")\n",
    "print(f\"   OSS Active States: {ACTIVE_OSS_STATES}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONNECTING TO ES SALESFORCE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to ES...\n",
      "âœ… Connected to ES: everstream.my.salesforce.com\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO ES SALESFORCE ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONNECTING TO ES SALESFORCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to ES...\")\n",
    "es_sf = Salesforce(\n",
    "    username=ES_USERNAME,\n",
    "    password=ES_PASSWORD,\n",
    "    security_token=ES_TOKEN,\n",
    "    domain=ES_DOMAIN,\n",
    ")\n",
    "print(f\"âœ… Connected to ES: {es_sf.sf_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONNECTING TO OSS DATABASE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to OSS...\n",
      "âœ… Connected to OSS: pg01.comlink.net/GLC\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO OSS DATABASE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONNECTING TO OSS DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to OSS...\")\n",
    "oss_conn = psycopg2.connect(\n",
    "    dbname=OSS_DB,\n",
    "    user=OSS_USER,\n",
    "    password=OSS_PASSWORD,\n",
    "    host=OSS_HOST,\n",
    "    port=OSS_PORT,\n",
    ")\n",
    "print(f\"âœ… Connected to OSS: {OSS_HOST}/{OSS_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: QUERYING ACTIVE ORDERS\n",
      "================================================================================\n",
      "Querying all orders with active statuses...\n",
      "\n",
      "âœ… Total orders with active status: 17,980\n",
      "\n",
      "ðŸ“Š Record Type Breakdown:\n",
      "Service_Order_Record_Type__c\n",
      "Service Order Agreement    14059\n",
      "Work Order                  3921\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: QUERY ALL ACTIVE ORDERS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: QUERYING ACTIVE ORDERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status_filter = \"','\".join(ACTIVE_STATUSES)\n",
    "\n",
    "orders_query = f\"\"\"\n",
    "SELECT \n",
    "    Id, \n",
    "    Name,\n",
    "    Service_ID__c,\n",
    "    Status,\n",
    "    AccountId,\n",
    "    Account.Name,\n",
    "    Billing_Invoice__c,\n",
    "    Address_A__c,\n",
    "    Address_Z__c,\n",
    "    Node__c,\n",
    "    OpportunityId,\n",
    "    Service_Start_Date__c,\n",
    "    Billing_Start_Date__c,\n",
    "    Service_End_Date__c,\n",
    "    Service_Provided__c,\n",
    "    SOF_MRC__c,\n",
    "    OSS_Order__c,\n",
    "    OSS_Service_ID__c,\n",
    "    Vendor_Circuit_ID__c,\n",
    "    Primary_Product_Family__c,\n",
    "    Primary_Product_Name__c,\n",
    "    Project_Group__c,\n",
    "    Service_Order_Record_Type__c,\n",
    "    CreatedDate,\n",
    "    LastModifiedDate\n",
    "FROM Order\n",
    "WHERE Status IN ('{status_filter}')\n",
    "ORDER BY Service_ID__c\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying all orders with active statuses...\")\n",
    "result = es_sf.query_all(orders_query)\n",
    "orders_raw = result[\"records\"]\n",
    "\n",
    "# Flatten results\n",
    "all_orders = []\n",
    "for order in orders_raw:\n",
    "    all_orders.append(\n",
    "        {\n",
    "            \"Id\": order[\"Id\"],\n",
    "            \"Name\": order.get(\"Name\"),\n",
    "            \"Service_ID__c\": order.get(\"Service_ID__c\"),\n",
    "            \"Status\": order[\"Status\"],\n",
    "            \"AccountId\": order.get(\"AccountId\"),\n",
    "            \"Account_Name\": order[\"Account\"][\"Name\"] if order.get(\"Account\") else None,\n",
    "            \"Billing_Invoice__c\": order.get(\"Billing_Invoice__c\"),\n",
    "            \"Address_A__c\": order.get(\"Address_A__c\"),\n",
    "            \"Address_Z__c\": order.get(\"Address_Z__c\"),\n",
    "            \"Node__c\": order.get(\"Node__c\"),\n",
    "            \"OpportunityId\": order.get(\"OpportunityId\"),\n",
    "            \"Service_Start_Date__c\": order.get(\"Service_Start_Date__c\"),\n",
    "            \"Billing_Start_Date__c\": order.get(\"Billing_Start_Date__c\"),\n",
    "            \"Service_End_Date__c\": order.get(\"Service_End_Date__c\"),\n",
    "            \"Service_Provided__c\": order.get(\"Service_Provided__c\"),\n",
    "            \"SOF_MRC__c\": order.get(\"SOF_MRC__c\"),\n",
    "            \"OSS_Order__c\": order.get(\"OSS_Order__c\"),\n",
    "            \"OSS_Service_ID__c\": order.get(\"OSS_Service_ID__c\"),\n",
    "            \"Vendor_Circuit_ID__c\": order.get(\"Vendor_Circuit_ID__c\"),\n",
    "            \"Primary_Product_Family__c\": order.get(\"Primary_Product_Family__c\"),\n",
    "            \"Primary_Product_Name__c\": order.get(\"Primary_Product_Name__c\"),\n",
    "            \"Project_Group__c\": order.get(\"Project_Group__c\"),\n",
    "            \"Service_Order_Record_Type__c\": order.get(\"Service_Order_Record_Type__c\"),\n",
    "            \"CreatedDate\": order.get(\"CreatedDate\"),\n",
    "            \"LastModifiedDate\": order.get(\"LastModifiedDate\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_orders_df = pd.DataFrame(all_orders)\n",
    "print(f\"\\nâœ… Total orders with active status: {len(all_orders_df):,}\")\n",
    "\n",
    "# Show record type breakdown\n",
    "print(f\"\\nðŸ“Š Record Type Breakdown:\")\n",
    "print(all_orders_df[\"Service_Order_Record_Type__c\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FILTER - PA MARKET DECOM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PA MARKET DECOM Filter:\n",
      "   Before: 17,980\n",
      "   âŒ Excluded (PA MARKET DECOM): 887\n",
      "   âœ… Remaining: 17,093\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: FILTER - PA MARKET DECOM ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: FILTER - PA MARKET DECOM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate PA MARKET DECOM orders\n",
    "pa_decom_mask = (\n",
    "    all_orders_df[\"Project_Group__c\"]\n",
    "    .fillna(\"\")\n",
    "    .str.contains(PA_DECOM_FILTER, case=False)\n",
    ")\n",
    "excluded_pa_decom_df = all_orders_df[pa_decom_mask].copy()\n",
    "orders_after_pa_filter = all_orders_df[~pa_decom_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š PA MARKET DECOM Filter:\")\n",
    "print(f\"   Before: {len(all_orders_df):,}\")\n",
    "print(f\"   âŒ Excluded (PA MARKET DECOM): {len(excluded_pa_decom_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_pa_filter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FILTER - RECORD TYPE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Record Type Filter:\n",
      "   Before: 17,093\n",
      "   âŒ Excluded (Not 'Service Order Agreement'): 3,484\n",
      "   âœ… Remaining: 13,609\n",
      "\n",
      "   Excluded Record Types:\n",
      "Service_Order_Record_Type__c\n",
      "Work Order    3484\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: FILTER - RECORD TYPE (Service Order Agreement only) ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FILTER - RECORD TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Keep only Service Order Agreement\n",
    "record_type_mask = (\n",
    "    orders_after_pa_filter[\"Service_Order_Record_Type__c\"] == VALID_RECORD_TYPE\n",
    ")\n",
    "excluded_work_orders_df = orders_after_pa_filter[~record_type_mask].copy()\n",
    "orders_after_rt_filter = orders_after_pa_filter[record_type_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Record Type Filter:\")\n",
    "print(f\"   Before: {len(orders_after_pa_filter):,}\")\n",
    "print(f\"   âŒ Excluded (Not '{VALID_RECORD_TYPE}'): {len(excluded_work_orders_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_rt_filter):,}\")\n",
    "\n",
    "# Show breakdown of what was excluded\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    print(f\"\\n   Excluded Record Types:\")\n",
    "    print(\n",
    "        excluded_work_orders_df[\"Service_Order_Record_Type__c\"].value_counts(\n",
    "            dropna=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\n",
      "================================================================================\n",
      "\n",
      "   Work Orders with OSS_Order__c: 156\n",
      "   Work Orders without OSS_Order__c: 3,328\n",
      "   Unique workorder IDs to query: 156\n",
      "   Chunk 1: Retrieved 84 workorders\n",
      "\n",
      "âœ… Enriched 84 work orders with OSS data\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3B: ENRICH WORK ORDERS WITH OSS DATA ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    # Get work orders with OSS_Order__c (maps to workorder_id)\n",
    "    has_wo_oss = excluded_work_orders_df[\"OSS_Order__c\"].notna() & (\n",
    "        excluded_work_orders_df[\"OSS_Order__c\"] != \"\"\n",
    "    )\n",
    "    work_orders_with_oss = excluded_work_orders_df[has_wo_oss].copy()\n",
    "\n",
    "    print(f\"\\n   Work Orders with OSS_Order__c: {len(work_orders_with_oss):,}\")\n",
    "    print(f\"   Work Orders without OSS_Order__c: {(~has_wo_oss).sum():,}\")\n",
    "\n",
    "    if len(work_orders_with_oss) > 0:\n",
    "        workorder_ids = (\n",
    "            work_orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "        )\n",
    "        print(f\"   Unique workorder IDs to query: {len(workorder_ids):,}\")\n",
    "\n",
    "        # Query workorders.workorders\n",
    "        chunk_size = 5000\n",
    "        oss_workorders = []\n",
    "\n",
    "        for i in range(0, len(workorder_ids), chunk_size):\n",
    "            chunk = workorder_ids[i : i + chunk_size]\n",
    "            ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "            wo_query = f\"\"\"\n",
    "            SELECT \n",
    "                workorder_id,\n",
    "                order_nm,\n",
    "                order_id,\n",
    "                workorder_type_cd,\n",
    "                workorder_state_cd,\n",
    "                description,\n",
    "                start_date,\n",
    "                end_date,\n",
    "                disabled\n",
    "            FROM workorders.workorders\n",
    "            WHERE workorder_id IN ({ids_str})\n",
    "              AND disabled = 'infinity'\n",
    "            \"\"\"\n",
    "\n",
    "            with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "                cur.execute(wo_query)\n",
    "                rows = cur.fetchall()\n",
    "                oss_workorders.extend([dict(row) for row in rows])\n",
    "\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} workorders\")\n",
    "\n",
    "        if len(oss_workorders) > 0:\n",
    "            oss_wo_df = pd.DataFrame(oss_workorders)\n",
    "\n",
    "            # Convert date columns - strip timezone for Excel compatibility\n",
    "            for date_col in [\"start_date\", \"end_date\"]:\n",
    "                if date_col in oss_wo_df.columns:\n",
    "                    oss_wo_df[date_col] = pd.to_datetime(\n",
    "                        oss_wo_df[date_col], utc=True\n",
    "                    ).dt.tz_localize(None)\n",
    "\n",
    "            # Add descriptions\n",
    "            oss_wo_df[\"workorder_type_desc\"] = oss_wo_df[\"workorder_type_cd\"].map(\n",
    "                WORKORDER_TYPES\n",
    "            )\n",
    "            oss_wo_df[\"workorder_state_desc\"] = oss_wo_df[\"workorder_state_cd\"].map(\n",
    "                WORKORDER_STATES\n",
    "            )\n",
    "\n",
    "            # Merge with excluded work orders\n",
    "            excluded_work_orders_df[\"OSS_Order_ID\"] = pd.to_numeric(\n",
    "                excluded_work_orders_df[\"OSS_Order__c\"], errors=\"coerce\"\n",
    "            )\n",
    "            excluded_work_orders_df = excluded_work_orders_df.merge(\n",
    "                oss_wo_df, left_on=\"OSS_Order_ID\", right_on=\"workorder_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"\\nâœ… Enriched {oss_wo_df['workorder_id'].notna().sum():,} work orders with OSS data\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ No active workorders found in OSS\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No work orders have OSS_Order__c populated\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No work orders to enrich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š OSS_Order__c Population:\n",
      "   With OSS_Order__c: 11,525\n",
      "   Without OSS_Order__c: 2,084\n",
      "\n",
      "   Unique OSS Order IDs to query: 11,518\n",
      "   Chunk 1: Retrieved 5000 orders\n",
      "   Chunk 2: Retrieved 4999 orders\n",
      "   Chunk 3: Retrieved 1518 orders\n",
      "\n",
      "âœ… Total OSS orders retrieved: 11,517\n",
      "   OSS Order IDs not found in OSS: 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: CHECK OSS ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify orders with OSS_Order__c\n",
    "has_oss_id = orders_after_rt_filter[\"OSS_Order__c\"].notna() & (\n",
    "    orders_after_rt_filter[\"OSS_Order__c\"] != \"\"\n",
    ")\n",
    "orders_with_oss = orders_after_rt_filter[has_oss_id].copy()\n",
    "orders_without_oss = orders_after_rt_filter[~has_oss_id].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š OSS_Order__c Population:\")\n",
    "print(f\"   With OSS_Order__c: {len(orders_with_oss):,}\")\n",
    "print(f\"   Without OSS_Order__c: {len(orders_without_oss):,}\")\n",
    "\n",
    "# Query OSS for orders with OSS_Order__c\n",
    "if len(orders_with_oss) > 0:\n",
    "    oss_order_ids = (\n",
    "        orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "    )\n",
    "    print(f\"\\n   Unique OSS Order IDs to query: {len(oss_order_ids):,}\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 5000\n",
    "    oss_orders = []\n",
    "\n",
    "    for i in range(0, len(oss_order_ids), chunk_size):\n",
    "        chunk = oss_order_ids[i : i + chunk_size]\n",
    "        ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "        oss_query = f\"\"\"\n",
    "        SELECT \n",
    "            order_id,\n",
    "            order_state_cd,\n",
    "            order_type_cd,\n",
    "            bill_start_date,\n",
    "            bill_end_date,\n",
    "            circuit_active_date,\n",
    "            account_id,\n",
    "            service_id\n",
    "        FROM om.orders\n",
    "        WHERE order_id IN ({ids_str})\n",
    "        \"\"\"\n",
    "\n",
    "        with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(oss_query)\n",
    "            rows = cur.fetchall()\n",
    "            oss_orders.extend([dict(row) for row in rows])\n",
    "\n",
    "        print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} orders\")\n",
    "\n",
    "    oss_orders_df = pd.DataFrame(oss_orders)\n",
    "    print(f\"\\nâœ… Total OSS orders retrieved: {len(oss_orders_df):,}\")\n",
    "\n",
    "    # Check for OSS IDs not found\n",
    "    found_ids = (\n",
    "        set(oss_orders_df[\"order_id\"].tolist()) if len(oss_orders_df) > 0 else set()\n",
    "    )\n",
    "    not_found_ids = set(oss_order_ids) - found_ids\n",
    "    print(f\"   OSS Order IDs not found in OSS: {len(not_found_ids):,}\")\n",
    "else:\n",
    "    oss_orders_df = pd.DataFrame()\n",
    "    not_found_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: DETERMINING ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "Today's date for comparison: 2026-01-08\n",
      "Active OSS States: ['CL', 'OA']\n",
      "\n",
      "ðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\n",
      "   ACTIVELY BILLING: 11,490 (99.7%)\n",
      "   CL - Bill End Passed: 23 (0.2%)\n",
      "   Not Active State (CA): 6 (0.1%)\n",
      "   CL - Bill Start Future: 5 (0.0%)\n",
      "   No OSS Match: 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: DETERMINE ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: DETERMINING ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "today = pd.Timestamp.now().normalize()\n",
    "print(f\"\\nToday's date for comparison: {today.date()}\")\n",
    "print(f\"Active OSS States: {ACTIVE_OSS_STATES}\")\n",
    "\n",
    "# Merge SF orders with OSS data\n",
    "if len(orders_with_oss) > 0:\n",
    "    orders_with_oss[\"OSS_Order_ID\"] = orders_with_oss[\"OSS_Order__c\"].astype(int)\n",
    "\n",
    "if len(oss_orders_df) > 0:\n",
    "    # Convert date columns\n",
    "    oss_orders_df[\"bill_start_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_start_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "    oss_orders_df[\"bill_end_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_end_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged_df = orders_with_oss.merge(\n",
    "        oss_orders_df, left_on=\"OSS_Order_ID\", right_on=\"order_id\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    merged_df = orders_with_oss.copy()\n",
    "    merged_df[\"order_id\"] = None\n",
    "    merged_df[\"order_state_cd\"] = None\n",
    "    merged_df[\"bill_start_date\"] = None\n",
    "    merged_df[\"bill_end_date\"] = None\n",
    "\n",
    "\n",
    "# Determine actively billing status\n",
    "def get_billing_status(row):\n",
    "    if pd.isna(row.get(\"order_id\")):\n",
    "        return \"No OSS Match\"\n",
    "\n",
    "    state = row[\"order_state_cd\"].strip() if row.get(\"order_state_cd\") else None\n",
    "    bill_start = row.get(\"bill_start_date\")\n",
    "    bill_end = row.get(\"bill_end_date\")\n",
    "\n",
    "    # Check if state is in allowed active states (CL or OA)\n",
    "    if state not in ACTIVE_OSS_STATES:\n",
    "        return f\"Not Active State ({state})\"\n",
    "\n",
    "    if pd.isna(bill_start):\n",
    "        return f\"{state} - No Bill Start Date\"\n",
    "\n",
    "    if bill_start > today:\n",
    "        return f\"{state} - Bill Start Future\"\n",
    "\n",
    "    if pd.notna(bill_end) and bill_end <= today:\n",
    "        return f\"{state} - Bill End Passed\"\n",
    "\n",
    "    return \"ACTIVELY BILLING\"\n",
    "\n",
    "\n",
    "merged_df[\"OSS_Billing_Status\"] = merged_df.apply(get_billing_status, axis=1)\n",
    "\n",
    "# Add state description\n",
    "merged_df[\"OSS_State_Desc\"] = merged_df[\"order_state_cd\"].map(OSS_ORDER_STATES)\n",
    "\n",
    "# Also handle orders without OSS link\n",
    "orders_without_oss[\"OSS_Billing_Status\"] = \"No OSS_Order__c in SF\"\n",
    "orders_without_oss[\"order_state_cd\"] = None\n",
    "orders_without_oss[\"OSS_State_Desc\"] = None\n",
    "orders_without_oss[\"bill_start_date\"] = None\n",
    "orders_without_oss[\"bill_end_date\"] = None\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\")\n",
    "if len(merged_df) > 0:\n",
    "    billing_status_counts = merged_df[\"OSS_Billing_Status\"].value_counts()\n",
    "    for status, count in billing_status_counts.items():\n",
    "        pct = 100 * count / len(merged_df)\n",
    "        print(f\"   {status}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FILTER - ACTIVELY BILLING ONLY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Actively Billing Filter:\n",
      "   Before: 13,609\n",
      "   âŒ Excluded (Not Actively Billing): 2,119\n",
      "   âœ… Confirmed Actively Billing: 11,490\n",
      "\n",
      "   Breakdown of excluded:\n",
      "      No OSS_Order__c in SF: 2,084\n",
      "      CL - Bill End Passed: 23\n",
      "      Not Active State (CA): 6\n",
      "      CL - Bill Start Future: 5\n",
      "      No OSS Match: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_2388\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n",
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_2388\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: FILTER - ACTIVELY BILLING ONLY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: FILTER - ACTIVELY BILLING ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get common columns\n",
    "common_cols = [col for col in orders_after_rt_filter.columns]\n",
    "oss_cols = [\n",
    "    \"order_state_cd\",\n",
    "    \"OSS_State_Desc\",\n",
    "    \"bill_start_date\",\n",
    "    \"bill_end_date\",\n",
    "    \"OSS_Billing_Status\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "for col in oss_cols:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = None\n",
    "    if col not in orders_without_oss.columns:\n",
    "        orders_without_oss[col] = None\n",
    "\n",
    "all_cols = common_cols + oss_cols\n",
    "\n",
    "# Combine orders with and without OSS\n",
    "all_checked_orders = pd.concat(\n",
    "    [\n",
    "        merged_df[[c for c in all_cols if c in merged_df.columns]],\n",
    "        orders_without_oss[[c for c in all_cols if c in orders_without_oss.columns]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Split into actively billing vs not\n",
    "actively_billing_mask = all_checked_orders[\"OSS_Billing_Status\"] == \"ACTIVELY BILLING\"\n",
    "orders_actively_billing = all_checked_orders[actively_billing_mask].copy()\n",
    "excluded_not_billing_df = all_checked_orders[~actively_billing_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Actively Billing Filter:\")\n",
    "print(f\"   Before: {len(all_checked_orders):,}\")\n",
    "print(f\"   âŒ Excluded (Not Actively Billing): {len(excluded_not_billing_df):,}\")\n",
    "print(f\"   âœ… Confirmed Actively Billing: {len(orders_actively_billing):,}\")\n",
    "\n",
    "# Show breakdown of excluded\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    print(f\"\\n   Breakdown of excluded:\")\n",
    "    for status, count in (\n",
    "        excluded_not_billing_df[\"OSS_Billing_Status\"].value_counts().items()\n",
    "    ):\n",
    "        print(f\"      {status}: {count:,}\")\n",
    "\n",
    "# Set this as our active orders for the rest of the pipeline\n",
    "active_orders_df = orders_actively_billing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: QUERYING NEW BBF BANS\n",
      "================================================================================\n",
      "Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\n",
      "\n",
      "âœ… Found 2,505 new BBF BANs\n",
      "   Legacy BAN to BBF BAN mappings: 2,505\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: GET NEW BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: QUERYING NEW BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bbf_ban_query = \"\"\"\n",
    "SELECT \n",
    "    Id,\n",
    "    Name,\n",
    "    Account__c,\n",
    "    Account__r.Name,\n",
    "    Legacy_ES_Id__c,\n",
    "    BBF_Ban__c,\n",
    "    Billing_Address_1__c,\n",
    "    Billing_City__c,\n",
    "    Billing_State__c,\n",
    "    Billing_ZIP__c,\n",
    "    Payment_Terms__c,\n",
    "    Active_Billing__c\n",
    "FROM Billing_Invoice__c\n",
    "WHERE BBF_Ban__c = true\n",
    "  AND Legacy_ES_Id__c != null\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\")\n",
    "result = es_sf.query_all(bbf_ban_query)\n",
    "bbf_bans_df = pd.DataFrame(result[\"records\"])\n",
    "\n",
    "if len(bbf_bans_df) > 0:\n",
    "    # Flatten Account name\n",
    "    if \"Account__r\" in bbf_bans_df.columns:\n",
    "        bbf_bans_df[\"Account_Name\"] = bbf_bans_df[\"Account__r\"].apply(\n",
    "            lambda x: x[\"Name\"] if x else None\n",
    "        )\n",
    "\n",
    "    # Clean up\n",
    "    bbf_bans_df = bbf_bans_df.drop(\n",
    "        columns=[\"attributes\", \"Account__r\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Found {len(bbf_bans_df):,} new BBF BANs\")\n",
    "\n",
    "    # Create lookup from legacy BAN ID to new BBF BAN\n",
    "    legacy_to_bbf_ban = {}\n",
    "    for _, ban in bbf_bans_df.iterrows():\n",
    "        legacy_id = ban.get(\"Legacy_ES_Id__c\")\n",
    "        if legacy_id:\n",
    "            legacy_to_bbf_ban[legacy_id] = {\n",
    "                \"Id\": ban[\"Id\"],\n",
    "                \"Name\": ban[\"Name\"],\n",
    "                \"Account__c\": ban.get(\"Account__c\"),\n",
    "                \"Account_Name\": ban.get(\"Account_Name\"),\n",
    "            }\n",
    "    print(f\"   Legacy BAN to BBF BAN mappings: {len(legacy_to_bbf_ban):,}\")\n",
    "else:\n",
    "    legacy_to_bbf_ban = {}\n",
    "    print(\"âš ï¸ No BBF BANs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: MAPPING ORDERS TO BBF BANS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š BAN Mapping Results:\n",
      "   âœ… Ready to migrate (has BBF BAN): 11,476\n",
      "   âš ï¸ Missing BBF BAN mapping: 12\n",
      "   âŒ Missing ANY BAN: 2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: MAP ORDERS TO BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: MAPPING ORDERS TO BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track which orders can be mapped\n",
    "orders_with_bbf_ban = []\n",
    "orders_missing_bbf_ban = []\n",
    "orders_missing_any_ban = []\n",
    "\n",
    "for _, order in active_orders_df.iterrows():\n",
    "    legacy_ban_id = order.get(\"Billing_Invoice__c\")\n",
    "    order_dict = order.to_dict()\n",
    "\n",
    "    if not legacy_ban_id:\n",
    "        # Order has no BAN at all\n",
    "        orders_missing_any_ban.append(order_dict)\n",
    "    elif legacy_ban_id in legacy_to_bbf_ban:\n",
    "        # Order can be mapped to new BBF BAN\n",
    "        bbf_ban = legacy_to_bbf_ban[legacy_ban_id]\n",
    "        order_dict[\"New_BBF_BAN_Id\"] = bbf_ban[\"Id\"]\n",
    "        order_dict[\"New_BBF_BAN_Name\"] = bbf_ban[\"Name\"]\n",
    "        order_dict[\"New_BBF_BAN_Account__c\"] = bbf_ban[\"Account__c\"]\n",
    "        order_dict[\"New_BBF_BAN_Account_Name\"] = bbf_ban[\"Account_Name\"]\n",
    "        orders_with_bbf_ban.append(order_dict)\n",
    "    else:\n",
    "        # Order has legacy BAN but no BBF BAN mapping\n",
    "        orders_missing_bbf_ban.append(order_dict)\n",
    "\n",
    "orders_ready_df = pd.DataFrame(orders_with_bbf_ban)\n",
    "orders_no_bbf_ban_df = pd.DataFrame(orders_missing_bbf_ban)\n",
    "orders_no_ban_df = pd.DataFrame(orders_missing_any_ban)\n",
    "\n",
    "print(f\"\\nðŸ“Š BAN Mapping Results:\")\n",
    "print(f\"   âœ… Ready to migrate (has BBF BAN): {len(orders_ready_df):,}\")\n",
    "print(f\"   âš ï¸ Missing BBF BAN mapping: {len(orders_no_bbf_ban_df):,}\")\n",
    "print(f\"   âŒ Missing ANY BAN: {len(orders_no_ban_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Found 2,225 unique Accounts from new BBF BANs\n",
      "\n",
      "âœ… Accounts to migrate: 2,225\n"
     ]
    }
   ],
   "source": [
    "# === STEP 9: IDENTIFY ACCOUNTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    unique_account_ids = (\n",
    "        orders_ready_df[\"New_BBF_BAN_Account__c\"].dropna().unique().tolist()\n",
    "    )\n",
    "    print(f\"\\nFound {len(unique_account_ids):,} unique Accounts from new BBF BANs\")\n",
    "\n",
    "    if unique_account_ids:\n",
    "        chunk_size = 150\n",
    "        all_accounts = []\n",
    "\n",
    "        for i in range(0, len(unique_account_ids), chunk_size):\n",
    "            chunk = unique_account_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            account_query = f\"\"\"\n",
    "            SELECT Id, Name, Type, Industry, \n",
    "                   BillingStreet, BillingCity, BillingState, BillingPostalCode, BillingCountry,\n",
    "                   Phone, Website, BBF_New_Id__c\n",
    "            FROM Account\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(account_query)\n",
    "            all_accounts.extend(result[\"records\"])\n",
    "\n",
    "        accounts_df = pd.DataFrame(all_accounts)\n",
    "        if len(accounts_df) > 0:\n",
    "            accounts_df = accounts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Accounts to migrate: {len(accounts_df):,}\")\n",
    "    else:\n",
    "        accounts_df = pd.DataFrame()\n",
    "else:\n",
    "    accounts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Account query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: IDENTIFYING CONTACTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Contacts to migrate: 15,583\n"
     ]
    }
   ],
   "source": [
    "# === STEP 10: IDENTIFY CONTACTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 10: IDENTIFYING CONTACTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(accounts_df) > 0:\n",
    "    account_ids = accounts_df[\"Id\"].tolist()\n",
    "    chunk_size = 150\n",
    "    all_contacts = []\n",
    "\n",
    "    for i in range(0, len(account_ids), chunk_size):\n",
    "        chunk = account_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        contact_query = f\"\"\"\n",
    "        SELECT Id, AccountId, FirstName, LastName, Email, Phone, Title,\n",
    "               MailingStreet, MailingCity, MailingState, MailingPostalCode,\n",
    "               BBF_New_Id__c\n",
    "        FROM Contact\n",
    "        WHERE AccountId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(contact_query)\n",
    "        all_contacts.extend(result[\"records\"])\n",
    "\n",
    "    contacts_df = pd.DataFrame(all_contacts)\n",
    "    if len(contacts_df) > 0:\n",
    "        contacts_df = contacts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "    print(f\"\\nâœ… Contacts to migrate: {len(contacts_df):,}\")\n",
    "else:\n",
    "    contacts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No accounts to migrate, skipping Contact query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Unique locations referenced by orders:\n",
      "   Address_A: 723\n",
      "   Address_Z: 9,818\n",
      "   Combined unique: 10,176\n",
      "\n",
      "âœ… Locations to migrate: 10,176\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11: IDENTIFY LOCATIONS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    address_a_ids = orders_ready_df[\"Address_A__c\"].dropna().unique().tolist()\n",
    "    address_z_ids = orders_ready_df[\"Address_Z__c\"].dropna().unique().tolist()\n",
    "    all_address_ids = list(set(address_a_ids + address_z_ids))\n",
    "\n",
    "    print(f\"\\nUnique locations referenced by orders:\")\n",
    "    print(f\"   Address_A: {len(address_a_ids):,}\")\n",
    "    print(f\"   Address_Z: {len(address_z_ids):,}\")\n",
    "    print(f\"   Combined unique: {len(all_address_ids):,}\")\n",
    "\n",
    "    if all_address_ids:\n",
    "        chunk_size = 150\n",
    "        all_locations = []\n",
    "\n",
    "        for i in range(0, len(all_address_ids), chunk_size):\n",
    "            chunk = all_address_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            location_query = f\"\"\"\n",
    "            SELECT Id, Name, Address__c, City__c, State__c, County__c, Zip__c,\n",
    "                   Complete_Address__c, CLLI__c, Building_Status__c, On_Net__c,\n",
    "                   BBF_New_Id__c\n",
    "            FROM Address__c\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(location_query)\n",
    "            all_locations.extend(result[\"records\"])\n",
    "\n",
    "        locations_df = pd.DataFrame(all_locations)\n",
    "        if len(locations_df) > 0:\n",
    "            locations_df = locations_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Locations to migrate: {len(locations_df):,}\")\n",
    "    else:\n",
    "        locations_df = pd.DataFrame()\n",
    "else:\n",
    "    locations_df = pd.DataFrame()\n",
    "    all_address_ids = []\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Location query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11B: IDENTIFYING NODES TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Querying Nodes that look up to 11,476 migrating Orders...\n",
      "(via Service_Order_Agreement__c field)\n",
      "\n",
      "âœ… Nodes linked to migrating Orders: 2,021\n",
      "\n",
      "ðŸ“Š Node Statistics:\n",
      "   Unique Rings referenced: 1052\n",
      "   Nodes with Ring_Type__c: 2021\n",
      "   Nodes with Address__c (text): 2021\n",
      "   Nodes with East_Neighbor__c: 1172\n",
      "   Nodes with West_Neighbor__c: 1009\n",
      "\n",
      "ðŸ“Š Node_Order_List__c Distribution:\n",
      "Node_Order_List__c\n",
      "1     966\n",
      "2     405\n",
      "3     248\n",
      "4     132\n",
      "5      66\n",
      "6      47\n",
      "7      33\n",
      "8      26\n",
      "9      20\n",
      "10     18\n",
      "11     13\n",
      "12     12\n",
      "13      8\n",
      "15      7\n",
      "14      7\n",
      "17      5\n",
      "18      3\n",
      "19      2\n",
      "16      2\n",
      "20      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11B: IDENTIFY NODES TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11B: IDENTIFYING NODES TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    # Get all Order IDs that are ready to migrate\n",
    "    migrating_order_ids = orders_ready_df[\"Id\"].tolist()\n",
    "\n",
    "    print(\n",
    "        f\"\\nQuerying Nodes that look up to {len(migrating_order_ids):,} migrating Orders...\"\n",
    "    )\n",
    "    print(\"(via Service_Order_Agreement__c field)\")\n",
    "\n",
    "    # Query Nodes in chunks (ES Node__c.Service_Order_Agreement__c â†’ Order)\n",
    "    chunk_size = 150\n",
    "    all_nodes = []\n",
    "\n",
    "    for i in range(0, len(migrating_order_ids), chunk_size):\n",
    "        chunk = migrating_order_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        node_query = f\"\"\"\n",
    "        SELECT Id, Name, Node_ID__c, Address__c, Site_Name__c,\n",
    "               Ring__c, Ring_Type__c,\n",
    "               Service_Order_Agreement__c, Service_Order_Agreement_Billing__c,\n",
    "               Service_ID__c, Node_Order_List__c,\n",
    "               East_Neighbor__c, West_Neighbor__c,\n",
    "               Distance_to_East__c, Distance_to_West__c,\n",
    "               Maintenance_IP_Address__c,\n",
    "               BBF_New_Id__c, CreatedDate, LastModifiedDate\n",
    "        FROM Node__c\n",
    "        WHERE Service_Order_Agreement__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(node_query)\n",
    "        all_nodes.extend(result[\"records\"])\n",
    "\n",
    "    nodes_df = pd.DataFrame(all_nodes)\n",
    "    if len(nodes_df) > 0:\n",
    "        nodes_df = nodes_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "\n",
    "    print(f\"\\nâœ… Nodes linked to migrating Orders: {len(nodes_df):,}\")\n",
    "\n",
    "    if len(nodes_df) > 0:\n",
    "        # Basic stats\n",
    "        print(f\"\\nðŸ“Š Node Statistics:\")\n",
    "        print(f\"   Unique Rings referenced: {nodes_df['Ring__c'].nunique()}\")\n",
    "        print(f\"   Nodes with Ring_Type__c: {nodes_df['Ring_Type__c'].notna().sum()}\")\n",
    "        print(\n",
    "            f\"   Nodes with Address__c (text): {nodes_df['Address__c'].notna().sum()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Nodes with East_Neighbor__c: {nodes_df['East_Neighbor__c'].notna().sum()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Nodes with West_Neighbor__c: {nodes_df['West_Neighbor__c'].notna().sum()}\"\n",
    "        )\n",
    "\n",
    "        # Node_Order_List__c breakdown (potential A/Z indicator?)\n",
    "        if \"Node_Order_List__c\" in nodes_df.columns:\n",
    "            print(f\"\\nðŸ“Š Node_Order_List__c Distribution:\")\n",
    "            print(nodes_df[\"Node_Order_List__c\"].value_counts(dropna=False))\n",
    "else:\n",
    "    nodes_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Node query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11C: QUERYING RINGS FOR MIGRATING NODES\n",
      "================================================================================\n",
      "\n",
      "Querying 1,052 Rings referenced by migrating Nodes...\n",
      "\n",
      "âœ… Rings to analyze: 1,052\n",
      "\n",
      "ðŸ“Š Ring Statistics:\n",
      "   Ring_Type__c Distribution:\n",
      "Ring_Type__c\n",
      "Stand Alone                   496\n",
      "DWDM - Cassette               290\n",
      "DWDM - Field Mux              212\n",
      "DWDM                           18\n",
      "Stand Alone (Single Fiber)     13\n",
      "Regen                          11\n",
      "Stand Alone (Dual Fiber)        6\n",
      "CWDM                            5\n",
      "Single Fiber (Switched)         1\n",
      "\n",
      "   Status__c Distribution:\n",
      "Status__c\n",
      "Activated      1007\n",
      "Draft            22\n",
      "In Progress      16\n",
      "None              7\n",
      "\n",
      "   Rings with East_POP__c: 1045\n",
      "   Rings with West_POP__c: 585\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11C: QUERY RINGS ASSOCIATED WITH NODES ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11C: QUERYING RINGS FOR MIGRATING NODES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(nodes_df) > 0:\n",
    "    # Get unique Ring IDs from nodes\n",
    "    ring_ids = nodes_df[\"Ring__c\"].dropna().unique().tolist()\n",
    "\n",
    "    print(f\"\\nQuerying {len(ring_ids):,} Rings referenced by migrating Nodes...\")\n",
    "\n",
    "    if ring_ids:\n",
    "        chunk_size = 150\n",
    "        all_rings = []\n",
    "\n",
    "        for i in range(0, len(ring_ids), chunk_size):\n",
    "            chunk = ring_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            ring_query = f\"\"\"\n",
    "            SELECT Id, Name, Ring_Id__c, Ring_Type__c, Status__c,\n",
    "                   East_POP__c, West_POP__c,\n",
    "                   Total_Ring_Distance_ft__c, Number_of_Nodes__c,\n",
    "                   CreatedDate, LastModifiedDate\n",
    "            FROM Ring__c\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(ring_query)\n",
    "            all_rings.extend(result[\"records\"])\n",
    "\n",
    "        rings_df = pd.DataFrame(all_rings)\n",
    "        if len(rings_df) > 0:\n",
    "            rings_df = rings_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "\n",
    "        print(f\"\\nâœ… Rings to analyze: {len(rings_df):,}\")\n",
    "\n",
    "        if len(rings_df) > 0:\n",
    "            print(f\"\\nðŸ“Š Ring Statistics:\")\n",
    "            print(f\"   Ring_Type__c Distribution:\")\n",
    "            print(rings_df[\"Ring_Type__c\"].value_counts(dropna=False).to_string())\n",
    "            print(f\"\\n   Status__c Distribution:\")\n",
    "            print(rings_df[\"Status__c\"].value_counts(dropna=False).to_string())\n",
    "            print(\n",
    "                f\"\\n   Rings with East_POP__c: {rings_df['East_POP__c'].notna().sum()}\"\n",
    "            )\n",
    "            print(f\"   Rings with West_POP__c: {rings_df['West_POP__c'].notna().sum()}\")\n",
    "    else:\n",
    "        rings_df = pd.DataFrame()\n",
    "        print(\"âš ï¸ No Rings found (Nodes don't have Ring__c populated)\")\n",
    "else:\n",
    "    rings_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No nodes to process, skipping Ring query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11D: ANALYZING NODE A/Z PATTERNS\n",
      "================================================================================\n",
      "\n",
      "Analyzing patterns to determine A vs Z node assignment...\n",
      "\n",
      "ðŸ“Š Ring Position Analysis:\n",
      "   Nodes that are Ring's East_POP: 0\n",
      "   Nodes that are Ring's West_POP: 0\n",
      "   Nodes that are NEITHER: 2,021\n",
      "\n",
      "ðŸ“Š Ring Position Distribution:\n",
      "Ring_Position\n",
      "NEITHER    2021\n",
      "\n",
      "------------------------------------------------------------\n",
      "CROSS-REFERENCE: Node Address vs Order Address A/Z\n",
      "------------------------------------------------------------\n",
      "\n",
      "Orders with Node__c populated: 1,691\n",
      "Orders without Node__c: 9,785\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11D: ANALYZE NODE A/Z PATTERNS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11D: ANALYZING NODE A/Z PATTERNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "node_analysis = []\n",
    "node_analysis_df = pd.DataFrame()\n",
    "\n",
    "if len(nodes_df) > 0 and len(rings_df) > 0:\n",
    "    print(\"\\nAnalyzing patterns to determine A vs Z node assignment...\")\n",
    "\n",
    "    # Create Ring lookup dict\n",
    "    ring_lookup = {}\n",
    "    for _, ring in rings_df.iterrows():\n",
    "        ring_lookup[ring[\"Id\"]] = {\n",
    "            \"Ring_Id\": ring.get(\"Ring_Id__c\"),\n",
    "            \"Ring_Name\": ring.get(\"Name\"),\n",
    "            \"East_POP\": ring.get(\"East_POP__c\"),\n",
    "            \"West_POP\": ring.get(\"West_POP__c\"),\n",
    "            \"Ring_Type\": ring.get(\"Ring_Type__c\"),\n",
    "        }\n",
    "\n",
    "    # For each node, check if it's East_POP or West_POP on its Ring\n",
    "    east_pop_count = 0\n",
    "    west_pop_count = 0\n",
    "    neither_count = 0\n",
    "\n",
    "    for _, node in nodes_df.iterrows():\n",
    "        ring_id = node.get(\"Ring__c\")\n",
    "        node_id = node.get(\"Id\")\n",
    "\n",
    "        ring_info = ring_lookup.get(ring_id, {})\n",
    "\n",
    "        is_east_pop = node_id == ring_info.get(\"East_POP\")\n",
    "        is_west_pop = node_id == ring_info.get(\"West_POP\")\n",
    "\n",
    "        if is_east_pop:\n",
    "            east_pop_count += 1\n",
    "            position = \"EAST_POP\"\n",
    "        elif is_west_pop:\n",
    "            west_pop_count += 1\n",
    "            position = \"WEST_POP\"\n",
    "        else:\n",
    "            neither_count += 1\n",
    "            position = \"NEITHER\"\n",
    "\n",
    "        node_analysis.append(\n",
    "            {\n",
    "                \"Node_Id\": node_id,\n",
    "                \"Node_Name\": node.get(\"Name\"),\n",
    "                \"Node_ID__c\": node.get(\"Node_ID__c\"),\n",
    "                \"Address_Text\": node.get(\"Address__c\"),\n",
    "                \"Site_Name\": node.get(\"Site_Name__c\"),\n",
    "                \"Ring_Id\": ring_id,\n",
    "                \"Ring_Name\": ring_info.get(\"Ring_Name\"),\n",
    "                \"Ring_Type\": ring_info.get(\"Ring_Type\"),\n",
    "                \"Is_East_POP\": is_east_pop,\n",
    "                \"Is_West_POP\": is_west_pop,\n",
    "                \"Ring_Position\": position,\n",
    "                \"East_Neighbor\": node.get(\"East_Neighbor__c\"),\n",
    "                \"West_Neighbor\": node.get(\"West_Neighbor__c\"),\n",
    "                \"Node_Order_List\": node.get(\"Node_Order_List__c\"),\n",
    "                \"Service_Order_Agreement\": node.get(\"Service_Order_Agreement__c\"),\n",
    "                \"Service_ID\": node.get(\"Service_ID__c\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    node_analysis_df = pd.DataFrame(node_analysis)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Ring Position Analysis:\")\n",
    "    print(f\"   Nodes that are Ring's East_POP: {east_pop_count:,}\")\n",
    "    print(f\"   Nodes that are Ring's West_POP: {west_pop_count:,}\")\n",
    "    print(f\"   Nodes that are NEITHER: {neither_count:,}\")\n",
    "\n",
    "    print(f\"\\nðŸ“Š Ring Position Distribution:\")\n",
    "    print(node_analysis_df[\"Ring_Position\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "    # Cross-reference with Order Address A/Z\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"CROSS-REFERENCE: Node Address vs Order Address A/Z\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # For orders with nodes, check if Node.Address__c matches Address_A or Address_Z\n",
    "    orders_with_nodes = orders_ready_df[orders_ready_df[\"Node__c\"].notna()].copy()\n",
    "    print(f\"\\nOrders with Node__c populated: {len(orders_with_nodes):,}\")\n",
    "    print(f\"Orders without Node__c: {len(orders_ready_df) - len(orders_with_nodes):,}\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient data for Node A/Z analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11E: NODE TO ADDRESS TEXT MATCHING\n",
      "================================================================================\n",
      "\n",
      "Attempting to match Node.Address__c (TEXT) to Address__c records...\n",
      "\n",
      "ðŸ“Š Address Text Matching Results:\n",
      "   Matched to Address__c record: 0\n",
      "   Not matched: 2,021\n",
      "\n",
      "ðŸ“Š Match Type Distribution:\n",
      "Match_Type\n",
      "NO_MATCH    2021\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11E: NODE TO ADDRESS MATCHING ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11E: NODE TO ADDRESS TEXT MATCHING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "address_match_analysis = []\n",
    "address_match_df = pd.DataFrame()\n",
    "order_position_df = pd.DataFrame()\n",
    "\n",
    "if len(nodes_df) > 0 and len(locations_df) > 0:\n",
    "    print(\"\\nAttempting to match Node.Address__c (TEXT) to Address__c records...\")\n",
    "\n",
    "    # Create lookup by various Address fields\n",
    "    address_name_lookup = {}\n",
    "    address_complete_lookup = {}\n",
    "\n",
    "    for _, addr in locations_df.iterrows():\n",
    "        addr_id = addr.get(\"Id\")\n",
    "        if addr.get(\"Name\"):\n",
    "            address_name_lookup[str(addr[\"Name\"]).strip().upper()] = addr_id\n",
    "        if addr.get(\"Complete_Address__c\"):\n",
    "            address_complete_lookup[\n",
    "                str(addr[\"Complete_Address__c\"]).strip().upper()\n",
    "            ] = addr_id\n",
    "\n",
    "    # For each node, try to match Address__c text to an Address record\n",
    "    matched_count = 0\n",
    "    unmatched_count = 0\n",
    "\n",
    "    for _, node in nodes_df.iterrows():\n",
    "        node_addr_text = node.get(\"Address__c\")\n",
    "        matched_addr_id = None\n",
    "        match_type = None\n",
    "\n",
    "        if node_addr_text:\n",
    "            node_addr_upper = str(node_addr_text).strip().upper()\n",
    "\n",
    "            # Try exact match on Name\n",
    "            if node_addr_upper in address_name_lookup:\n",
    "                matched_addr_id = address_name_lookup[node_addr_upper]\n",
    "                match_type = \"NAME_EXACT\"\n",
    "            # Try exact match on Complete_Address__c\n",
    "            elif node_addr_upper in address_complete_lookup:\n",
    "                matched_addr_id = address_complete_lookup[node_addr_upper]\n",
    "                match_type = \"COMPLETE_ADDR_EXACT\"\n",
    "            else:\n",
    "                match_type = \"NO_MATCH\"\n",
    "        else:\n",
    "            match_type = \"NO_ADDRESS_TEXT\"\n",
    "\n",
    "        if matched_addr_id:\n",
    "            matched_count += 1\n",
    "        else:\n",
    "            unmatched_count += 1\n",
    "\n",
    "        address_match_analysis.append(\n",
    "            {\n",
    "                \"Node_Id\": node.get(\"Id\"),\n",
    "                \"Node_Name\": node.get(\"Name\"),\n",
    "                \"Node_Address_Text\": node_addr_text,\n",
    "                \"Matched_Address_Id\": matched_addr_id,\n",
    "                \"Match_Type\": match_type,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    address_match_df = pd.DataFrame(address_match_analysis)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Address Text Matching Results:\")\n",
    "    print(f\"   Matched to Address__c record: {matched_count:,}\")\n",
    "    print(f\"   Not matched: {unmatched_count:,}\")\n",
    "    print(f\"\\nðŸ“Š Match Type Distribution:\")\n",
    "    print(address_match_df[\"Match_Type\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "    # Now cross-reference: For matched nodes, is the Address__c the A or Z on their Order?\n",
    "    if matched_count > 0 and len(node_analysis_df) > 0:\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"NODE ADDRESS vs ORDER ADDRESS A/Z ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Merge node analysis with address match\n",
    "        combined = node_analysis_df.merge(\n",
    "            address_match_df[[\"Node_Id\", \"Matched_Address_Id\", \"Match_Type\"]],\n",
    "            on=\"Node_Id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # For each node with a matched address, check if it's Address_A or Address_Z on its Order\n",
    "        a_match_count = 0\n",
    "        z_match_count = 0\n",
    "        neither_match_count = 0\n",
    "\n",
    "        order_addr_lookup = {}\n",
    "        for _, order in orders_ready_df.iterrows():\n",
    "            order_addr_lookup[order[\"Id\"]] = {\n",
    "                \"Address_A\": order.get(\"Address_A__c\"),\n",
    "                \"Address_Z\": order.get(\"Address_Z__c\"),\n",
    "            }\n",
    "\n",
    "        order_position_results = []\n",
    "        for _, row in combined.iterrows():\n",
    "            soa = row.get(\"Service_Order_Agreement\")\n",
    "            matched_addr = row.get(\"Matched_Address_Id\")\n",
    "\n",
    "            order_addrs = order_addr_lookup.get(soa, {})\n",
    "            is_a = matched_addr == order_addrs.get(\"Address_A\")\n",
    "            is_z = matched_addr == order_addrs.get(\"Address_Z\")\n",
    "\n",
    "            if is_a:\n",
    "                a_match_count += 1\n",
    "                order_position = \"ADDRESS_A\"\n",
    "            elif is_z:\n",
    "                z_match_count += 1\n",
    "                order_position = \"ADDRESS_Z\"\n",
    "            else:\n",
    "                neither_match_count += 1\n",
    "                order_position = \"NEITHER\"\n",
    "\n",
    "            order_position_results.append(\n",
    "                {\n",
    "                    \"Node_Id\": row[\"Node_Id\"],\n",
    "                    \"Node_Name\": row[\"Node_Name\"],\n",
    "                    \"Ring_Position\": row.get(\"Ring_Position\"),\n",
    "                    \"Matched_Address_Id\": matched_addr,\n",
    "                    \"Order_Address_Position\": order_position,\n",
    "                    \"Service_Order_Agreement\": soa,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        order_position_df = pd.DataFrame(order_position_results)\n",
    "\n",
    "        print(f\"\\nðŸ“Š Node Address â†’ Order A/Z Position:\")\n",
    "        print(f\"   Matches Order's Address_A: {a_match_count:,}\")\n",
    "        print(f\"   Matches Order's Address_Z: {z_match_count:,}\")\n",
    "        print(f\"   Matches NEITHER: {neither_match_count:,}\")\n",
    "\n",
    "        # Cross-tab: Ring Position vs Order Position\n",
    "        print(f\"\\nðŸ“Š CROSS-TAB: Ring Position vs Order A/Z Position\")\n",
    "        print(\"-\" * 40)\n",
    "        if len(order_position_df) > 0:\n",
    "            crosstab = pd.crosstab(\n",
    "                order_position_df[\"Ring_Position\"],\n",
    "                order_position_df[\"Order_Address_Position\"],\n",
    "                margins=True,\n",
    "            )\n",
    "            print(crosstab.to_string())\n",
    "else:\n",
    "    print(\"âš ï¸ Insufficient data for address matching analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11F: NODE ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "SUMMARY FOR PROCESS OWNER DISCUSSION\n",
      "============================================================\n",
      "\n",
      "1. MIGRATION SCOPE:\n",
      "   - Orders ready to migrate: 11,476\n",
      "   - Nodes linked to those orders: 2,021\n",
      "   - Rings those nodes belong to: 1,052\n",
      "\n",
      "2. RING POSITION PATTERN:\n",
      "   - NEITHER: 2,021 (100.0%)\n",
      "\n",
      "4. KEY QUESTIONS FOR PROCESS OWNERS:\n",
      "   a) Does Ring.East_POP always correspond to Address_A (A-end)?\n",
      "   b) Does Ring.West_POP always correspond to Address_Z (Z-end)?\n",
      "   c) For nodes that are NEITHER East nor West POP, how to determine A vs Z?\n",
      "   d) Is there any other field or naming convention that indicates direction?\n",
      "\n",
      "5. DATA QUALITY NOTES:\n",
      "   - Orders WITH Node__c populated: 1,691\n",
      "   - Orders WITHOUT Node__c: 9,785\n",
      "   - Nodes without Ring__c: 0\n",
      "\n",
      "âœ… Node analysis complete - data ready for Excel export\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11F: NODE ANALYSIS SUMMARY FOR PROCESS OWNER DISCUSSION ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11F: NODE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "node_summary = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY FOR PROCESS OWNER DISCUSSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. MIGRATION SCOPE:\")\n",
    "print(f\"   - Orders ready to migrate: {len(orders_ready_df):,}\")\n",
    "print(f\"   - Nodes linked to those orders: {len(nodes_df):,}\")\n",
    "print(f\"   - Rings those nodes belong to: {len(rings_df):,}\")\n",
    "\n",
    "if len(node_analysis_df) > 0:\n",
    "    print(\"\\n2. RING POSITION PATTERN:\")\n",
    "    ring_pos_counts = node_analysis_df[\"Ring_Position\"].value_counts()\n",
    "    for pos, count in ring_pos_counts.items():\n",
    "        pct = (count / len(node_analysis_df)) * 100\n",
    "        print(f\"   - {pos}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if len(order_position_df) > 0:\n",
    "    print(\"\\n3. ORDER ADDRESS POSITION PATTERN:\")\n",
    "    order_pos_counts = order_position_df[\"Order_Address_Position\"].value_counts()\n",
    "    for pos, count in order_pos_counts.items():\n",
    "        pct = (count / len(order_position_df)) * 100\n",
    "        print(f\"   - {pos}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n4. KEY QUESTIONS FOR PROCESS OWNERS:\")\n",
    "print(\"   a) Does Ring.East_POP always correspond to Address_A (A-end)?\")\n",
    "print(\"   b) Does Ring.West_POP always correspond to Address_Z (Z-end)?\")\n",
    "print(\"   c) For nodes that are NEITHER East nor West POP, how to determine A vs Z?\")\n",
    "print(\"   d) Is there any other field or naming convention that indicates direction?\")\n",
    "\n",
    "print(\"\\n5. DATA QUALITY NOTES:\")\n",
    "orders_with_node = len(orders_ready_df[orders_ready_df[\"Node__c\"].notna()])\n",
    "orders_without_node = len(orders_ready_df) - orders_with_node\n",
    "print(f\"   - Orders WITH Node__c populated: {orders_with_node:,}\")\n",
    "print(f\"   - Orders WITHOUT Node__c: {orders_without_node:,}\")\n",
    "\n",
    "if len(nodes_df) > 0:\n",
    "    nodes_no_ring = len(nodes_df[nodes_df[\"Ring__c\"].isna()])\n",
    "    print(f\"   - Nodes without Ring__c: {nodes_no_ring:,}\")\n",
    "\n",
    "# Build summary dataframe for Excel\n",
    "node_summary = [\n",
    "    {\n",
    "        \"Metric\": \"Orders Ready to Migrate\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"Actively billing, has BBF BAN\",\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Nodes Linked to Orders\",\n",
    "        \"Count\": len(nodes_df),\n",
    "        \"Notes\": \"Via Service_Order_Agreement__c\",\n",
    "    },\n",
    "    {\"Metric\": \"Unique Rings\", \"Count\": len(rings_df), \"Notes\": \"From Node.Ring__c\"},\n",
    "    {\n",
    "        \"Metric\": \"Orders with Node__c\",\n",
    "        \"Count\": orders_with_node,\n",
    "        \"Notes\": \"Have direct Node reference\",\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Orders without Node__c\",\n",
    "        \"Count\": orders_without_node,\n",
    "        \"Notes\": \"Need to derive Node\",\n",
    "    },\n",
    "]\n",
    "\n",
    "if len(node_analysis_df) > 0:\n",
    "    for pos in [\"EAST_POP\", \"WEST_POP\", \"NEITHER\"]:\n",
    "        count = len(node_analysis_df[node_analysis_df[\"Ring_Position\"] == pos])\n",
    "        node_summary.append(\n",
    "            {\n",
    "                \"Metric\": f\"Nodes - Ring Position: {pos}\",\n",
    "                \"Count\": count,\n",
    "                \"Notes\": \"Ring.East_POP / Ring.West_POP match\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "node_summary_df = pd.DataFrame(node_summary)\n",
    "print(\"\\nâœ… Node analysis complete - data ready for Excel export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Off_Net records to migrate: 2,157\n"
     ]
    }
   ],
   "source": [
    "# === STEP 12: IDENTIFY OFF_NET RECORDS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_address_ids) > 0:\n",
    "    chunk_size = 100\n",
    "    all_offnet = []\n",
    "\n",
    "    for i in range(0, len(all_address_ids), chunk_size):\n",
    "        chunk = all_address_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        offnet_query = f\"\"\"\n",
    "        SELECT Id, Name, \n",
    "               Location_1__c, Location_1_Address__c,\n",
    "               Location_2__c, Location_2_Address__c,\n",
    "               Off_Net_Vendor__c, Vendor_Name__c,\n",
    "               Vendor_circuit_Id__c, Internal_Circuit_Id__c,\n",
    "               Cost_MRC__c, Cost_NRC__c, Invoice_MRC__c,\n",
    "               LEC_Order_Status__c, Off_Net_Type__c,\n",
    "               Bandwidth__c, Circuit_Type__c, Term__c,\n",
    "               Term_Agreement_Start_Date__c, Term_Agreement_End_Date__c,\n",
    "               Vendor_Bill_Start_Date__c, Vendor_Bill_Stop_Date__c,\n",
    "               SOF1__c\n",
    "        FROM Off_Net__c\n",
    "        WHERE Location_1__c IN ('{ids_str}')\n",
    "           OR Location_2__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(offnet_query)\n",
    "        all_offnet.extend(result[\"records\"])\n",
    "\n",
    "    offnet_df = pd.DataFrame(all_offnet)\n",
    "    if len(offnet_df) > 0:\n",
    "        offnet_df = offnet_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        offnet_df = offnet_df.drop_duplicates(subset=[\"Id\"])\n",
    "    print(f\"\\nâœ… Off_Net records to migrate: {len(offnet_df):,}\")\n",
    "else:\n",
    "    offnet_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No locations to migrate, skipping Off_Net query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13: DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\n",
      "   âœ… Has Billing_Start_Date__c (primary):  11,475 (100.0%)\n",
      "   âš ï¸ Needs OSS bill_start_date (fallback): 1 (0.0%)\n",
      "      â””â”€ OSS covers: 1 of 1\n",
      "\n",
      "ðŸ“Š Data Quality Issues:\n",
      "   [HIGH] Orders missing Address_A__c: 2 (0.0%)\n",
      "         â†’ Cannot set A_Location__c on BBF Service__c\n",
      "   [LOW] Orders missing Node__c: 9785 (85.3%)\n",
      "         â†’ Can fix post-migration - A_Node__c/Z_Node__c optional\n",
      "   [LOW] Orders missing Billing_Start_Date__c: 1 (0.0%)\n",
      "         â†’ OSS bill_start_date covers 1 of 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13: DATA QUALITY ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_quality_issues = []\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    total_ready = len(orders_ready_df)\n",
    "\n",
    "    # Check for missing Address_A__c\n",
    "    missing_addr_a = orders_ready_df[\"Address_A__c\"].isna().sum()\n",
    "    if missing_addr_a > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Address_A__c\",\n",
    "                \"Count\": missing_addr_a,\n",
    "                \"Percentage\": f\"{missing_addr_a/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"HIGH\",\n",
    "                \"Impact\": \"Cannot set A_Location__c on BBF Service__c\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check for missing Node__c\n",
    "    missing_node = orders_ready_df[\"Node__c\"].isna().sum()\n",
    "    if missing_node > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Node__c\",\n",
    "                \"Count\": missing_node,\n",
    "                \"Percentage\": f\"{missing_node/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can fix post-migration - A_Node__c/Z_Node__c optional\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check Active Date Coverage: Billing_Start_Date__c (primary) â†’ OSS bill_start_date (fallback)\n",
    "    # Note: Service_Start_Date__c is historical (when service originally started), not used for Active_Date__c\n",
    "    sf_bill_start_null = orders_ready_df[\"Billing_Start_Date__c\"].isna()\n",
    "    oss_start_exists = orders_ready_df[\"bill_start_date\"].notna()\n",
    "\n",
    "    # Count coverage\n",
    "    has_bill_start = (~sf_bill_start_null).sum()\n",
    "    needs_oss = sf_bill_start_null.sum()\n",
    "    oss_covers = (sf_bill_start_null & oss_start_exists).sum()\n",
    "    no_date = (sf_bill_start_null & ~oss_start_exists).sum()\n",
    "\n",
    "    print(f\"\\nðŸ“Š Active Date Coverage (for Service__c.Active_Date__c):\")\n",
    "    print(\n",
    "        f\"   âœ… Has Billing_Start_Date__c (primary):  {has_bill_start:,} ({100*has_bill_start/total_ready:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   âš ï¸ Needs OSS bill_start_date (fallback): {needs_oss:,} ({100*needs_oss/total_ready:.1f}%)\"\n",
    "    )\n",
    "    if needs_oss > 0:\n",
    "        print(f\"      â””â”€ OSS covers: {oss_covers:,} of {needs_oss:,}\")\n",
    "    if no_date > 0:\n",
    "        print(f\"   ðŸ”´ No date available: {no_date:,}\")\n",
    "\n",
    "    if sf_bill_start_null.sum() > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Billing_Start_Date__c\",\n",
    "                \"Count\": sf_bill_start_null.sum(),\n",
    "                \"Percentage\": f\"{sf_bill_start_null.sum()/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\" if oss_covers == needs_oss else \"MEDIUM\",\n",
    "                \"Impact\": f\"OSS bill_start_date covers {oss_covers:,} of {needs_oss:,}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "data_quality_df = pd.DataFrame(data_quality_issues)\n",
    "\n",
    "if len(data_quality_issues) > 0:\n",
    "    print(\"\\nðŸ“Š Data Quality Issues:\")\n",
    "    for issue in data_quality_issues:\n",
    "        print(\n",
    "            f\"   [{issue['Severity']}] {issue['Issue']}: {issue['Count']} ({issue['Percentage']})\"\n",
    "        )\n",
    "        print(f\"         â†’ {issue['Impact']}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No significant data quality issues found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13B: QUERYING ORDER ITEMS\n",
      "================================================================================\n",
      "\n",
      "Querying OrderItems for 11,476 orders...\n",
      "   Chunk 10: 2,565 items so far...\n",
      "   Chunk 20: 5,539 items so far...\n",
      "   Chunk 30: 7,943 items so far...\n",
      "   Chunk 40: 10,002 items so far...\n",
      "   Chunk 50: 13,157 items so far...\n",
      "   Chunk 60: 16,266 items so far...\n",
      "   Chunk 70: 18,627 items so far...\n",
      "\n",
      "âœ… Total OrderItems retrieved: 20,608\n",
      "\n",
      "ðŸ“Š OrderItem Statistics:\n",
      "   Orders with items: 11,476\n",
      "   Min items per order: 1\n",
      "   Max items per order: 18\n",
      "   Avg items per order: 1.8\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13B: QUERY ORDER ITEMS FOR MIGRATION SCOPE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13B: QUERYING ORDER ITEMS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    # Get all Order IDs from orders ready to migrate\n",
    "    order_ids = orders_ready_df[\"Id\"].unique().tolist()\n",
    "    print(f\"\\nQuerying OrderItems for {len(order_ids):,} orders...\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 150\n",
    "    all_order_items = []\n",
    "\n",
    "    for i in range(0, len(order_ids), chunk_size):\n",
    "        chunk = order_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        orderitem_query = f\"\"\"\n",
    "        SELECT \n",
    "            Id,\n",
    "            OrderId,\n",
    "            OrderItemNumber,\n",
    "            Product2Id,\n",
    "            Product_Name__c,\n",
    "            Product_Family__c,\n",
    "            Quantity,\n",
    "            UnitPrice,\n",
    "            TotalPrice,\n",
    "            ListPrice,\n",
    "            Total_MRC_Amortized__c,\n",
    "            NRC_IRU_FEE__c,\n",
    "            NRC_Non_Amortized__c,\n",
    "            Vendor_Fees_Monthly__c,\n",
    "            Vendor_NRC__c,\n",
    "            ServiceDate,\n",
    "            EndDate,\n",
    "            Contract_End_Month__c,\n",
    "            Description,\n",
    "            Bandwidth_NEW__c,\n",
    "            Bandwidth_Numerical__c,\n",
    "            Term__c,\n",
    "            Product_Service_Term__c,\n",
    "            Cancelled__c,\n",
    "            Last_Mile_Carrier__c,\n",
    "            Vendor_Last_Mile_CID__c,\n",
    "            SBQQ__ChargeType__c,\n",
    "            SBQQ__BillingFrequency__c,\n",
    "            SBQQ__Status__c,\n",
    "            OFF_NET_IDs__c,\n",
    "            CreatedDate,\n",
    "            LastModifiedDate\n",
    "        FROM OrderItem\n",
    "        WHERE OrderId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "\n",
    "        result = es_sf.query_all(orderitem_query)\n",
    "        all_order_items.extend(result[\"records\"])\n",
    "\n",
    "        if (i // chunk_size + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"   Chunk {i//chunk_size + 1}: {len(all_order_items):,} items so far...\"\n",
    "            )\n",
    "\n",
    "    order_items_df = pd.DataFrame(all_order_items)\n",
    "    if len(order_items_df) > 0:\n",
    "        order_items_df = order_items_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "\n",
    "    print(f\"\\nâœ… Total OrderItems retrieved: {len(order_items_df):,}\")\n",
    "\n",
    "    # Show basic stats\n",
    "    if len(order_items_df) > 0:\n",
    "        items_per_order = order_items_df.groupby(\"OrderId\").size()\n",
    "        print(f\"\\nðŸ“Š OrderItem Statistics:\")\n",
    "        print(f\"   Orders with items: {len(items_per_order):,}\")\n",
    "        print(f\"   Min items per order: {items_per_order.min()}\")\n",
    "        print(f\"   Max items per order: {items_per_order.max()}\")\n",
    "        print(f\"   Avg items per order: {items_per_order.mean():.1f}\")\n",
    "else:\n",
    "    order_items_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping OrderItem query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13C: ORDER ITEM ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Product Family Breakdown:\n",
      "   Point-to-Point (PTPS): 4,987 (24.2%)\n",
      "   Dedicated Internet Access (DIAS): 3,461 (16.8%)\n",
      "   IP: 3,323 (16.1%)\n",
      "   Hosted Voice (VOIC): 1,648 (8.0%)\n",
      "   Dark Fiber (DFBR): 1,467 (7.1%)\n",
      "   Point-to-MultiPoint (PMPS): 1,340 (6.5%)\n",
      "   Promotions: 875 (4.2%)\n",
      "   Managed Service (MSP): 771 (3.7%)\n",
      "   Handoff Type: 575 (2.8%)\n",
      "   Diversity: 497 (2.4%)\n",
      "   Additional Port: 277 (1.3%)\n",
      "   Tagged / Untagged: 234 (1.1%)\n",
      "   Logical Attributes: 230 (1.1%)\n",
      "   Routing: 125 (0.6%)\n",
      "   Managed Wave (MWAV): 106 (0.5%)\n",
      "   ... and 34 more families\n",
      "\n",
      "ðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\n",
      "   (null): 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Cancelled Status:\n",
      "   Cancelled=False: 20,608 (100.0%)\n",
      "\n",
      "ðŸ“Š Financial Fields:\n",
      "   Total_MRC_Amortized__c: 20,608 populated, sum=$9,648,124.07\n",
      "   NRC_IRU_FEE__c: 8,378 populated, sum=$14,307,429.82\n",
      "   Vendor_Fees_Monthly__c: 17 populated, sum=$8,780.00\n",
      "\n",
      "ðŸ“Š OrderItem Data Quality:\n",
      "\n",
      "âœ… OrderItem analysis complete\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13C: ORDER ITEM ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13C: ORDER ITEM ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "orderitem_summary = []\n",
    "orderitem_data_quality = []\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    total_items = len(order_items_df)\n",
    "\n",
    "    # === PRODUCT FAMILY BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Product Family Breakdown:\")\n",
    "    family_counts = order_items_df[\"Product_Family__c\"].value_counts(dropna=False)\n",
    "    for family, count in family_counts.head(15).items():\n",
    "        pct = 100 * count / total_items\n",
    "        family_name = family if family else \"(null)\"\n",
    "        print(f\"   {family_name}: {count:,} ({pct:.1f}%)\")\n",
    "    if len(family_counts) > 15:\n",
    "        print(f\"   ... and {len(family_counts) - 15} more families\")\n",
    "\n",
    "    # === CHARGE TYPE BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Charge Type Breakdown (SBQQ__ChargeType__c):\")\n",
    "    charge_counts = order_items_df[\"SBQQ__ChargeType__c\"].value_counts(dropna=False)\n",
    "    for charge, count in charge_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        charge_name = charge if charge else \"(null)\"\n",
    "        print(f\"   {charge_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === CANCELLED BREAKDOWN ===\n",
    "    print(\"\\nðŸ“Š Cancelled Status:\")\n",
    "    cancelled_counts = order_items_df[\"Cancelled__c\"].value_counts(dropna=False)\n",
    "    for status, count in cancelled_counts.items():\n",
    "        pct = 100 * count / total_items\n",
    "        status_name = str(status) if status is not None else \"(null)\"\n",
    "        print(f\"   Cancelled={status_name}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    # === MRC/NRC ANALYSIS ===\n",
    "    print(\"\\nðŸ“Š Financial Fields:\")\n",
    "\n",
    "    # Total MRC\n",
    "    mrc_populated = order_items_df[\"Total_MRC_Amortized__c\"].notna().sum()\n",
    "    mrc_sum = order_items_df[\"Total_MRC_Amortized__c\"].sum()\n",
    "    print(\n",
    "        f\"   Total_MRC_Amortized__c: {mrc_populated:,} populated, sum=${mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # NRC\n",
    "    nrc_populated = order_items_df[\"NRC_IRU_FEE__c\"].notna().sum()\n",
    "    nrc_sum = order_items_df[\"NRC_IRU_FEE__c\"].sum()\n",
    "    print(f\"   NRC_IRU_FEE__c: {nrc_populated:,} populated, sum=${nrc_sum:,.2f}\")\n",
    "\n",
    "    # Vendor costs\n",
    "    vendor_mrc_populated = order_items_df[\"Vendor_Fees_Monthly__c\"].notna().sum()\n",
    "    vendor_mrc_sum = order_items_df[\"Vendor_Fees_Monthly__c\"].sum()\n",
    "    print(\n",
    "        f\"   Vendor_Fees_Monthly__c: {vendor_mrc_populated:,} populated, sum=${vendor_mrc_sum:,.2f}\"\n",
    "    )\n",
    "\n",
    "    # === DATA QUALITY CHECKS ===\n",
    "    print(\"\\nðŸ“Š OrderItem Data Quality:\")\n",
    "\n",
    "    # Missing Product Name\n",
    "    missing_product = order_items_df[\"Product_Name__c\"].isna().sum()\n",
    "    if missing_product > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Name__c\",\n",
    "                \"Count\": missing_product,\n",
    "                \"Percentage\": f\"{100*missing_product/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need product mapping for Service_Charge__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Name__c: {missing_product:,} ({100*missing_product/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Product Family\n",
    "    missing_family = order_items_df[\"Product_Family__c\"].isna().sum()\n",
    "    if missing_family > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing Product_Family__c\",\n",
    "                \"Count\": missing_family,\n",
    "                \"Percentage\": f\"{100*missing_family/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Product_Name__c as fallback\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing Product_Family__c: {missing_family:,} ({100*missing_family/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing Unit Price\n",
    "    missing_price = order_items_df[\"UnitPrice\"].isna().sum()\n",
    "    if missing_price > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing UnitPrice\",\n",
    "                \"Count\": missing_price,\n",
    "                \"Percentage\": f\"{100*missing_price/total_items:.1f}%\",\n",
    "                \"Severity\": \"MEDIUM\",\n",
    "                \"Impact\": \"Need price for Service_Charge__c.Amount__c\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing UnitPrice: {missing_price:,} ({100*missing_price/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Cancelled items\n",
    "    cancelled_items = order_items_df[\"Cancelled__c\"].fillna(False).sum()\n",
    "    if cancelled_items > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems marked as Cancelled\",\n",
    "                \"Count\": int(cancelled_items),\n",
    "                \"Percentage\": f\"{100*cancelled_items/total_items:.1f}%\",\n",
    "                \"Severity\": \"INFO\",\n",
    "                \"Impact\": \"Consider excluding from migration\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   â„¹ï¸ Cancelled items: {int(cancelled_items):,} ({100*cancelled_items/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # Missing ServiceDate\n",
    "    missing_svc_date = order_items_df[\"ServiceDate\"].isna().sum()\n",
    "    if missing_svc_date > 0:\n",
    "        orderitem_data_quality.append(\n",
    "            {\n",
    "                \"Issue\": \"OrderItems missing ServiceDate\",\n",
    "                \"Count\": missing_svc_date,\n",
    "                \"Percentage\": f\"{100*missing_svc_date/total_items:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can use Order-level date or OSS bill_start_date\",\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"   âš ï¸ Missing ServiceDate: {missing_svc_date:,} ({100*missing_svc_date/total_items:.1f}%)\"\n",
    "        )\n",
    "\n",
    "    # === BUILD SUMMARY ===\n",
    "    orderitem_summary = [\n",
    "        {\"Metric\": \"Total OrderItems\", \"Value\": total_items},\n",
    "        {\"Metric\": \"Unique Orders\", \"Value\": order_items_df[\"OrderId\"].nunique()},\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Families\",\n",
    "            \"Value\": order_items_df[\"Product_Family__c\"].nunique(),\n",
    "        },\n",
    "        {\n",
    "            \"Metric\": \"Unique Product Names\",\n",
    "            \"Value\": order_items_df[\"Product_Name__c\"].nunique(),\n",
    "        },\n",
    "        {\"Metric\": \"Total MRC (sum)\", \"Value\": f\"${mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total NRC (sum)\", \"Value\": f\"${nrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Total Vendor MRC (sum)\", \"Value\": f\"${vendor_mrc_sum:,.2f}\"},\n",
    "        {\"Metric\": \"Cancelled Items\", \"Value\": int(cancelled_items)},\n",
    "        {\n",
    "            \"Metric\": \"Active Items (not cancelled)\",\n",
    "            \"Value\": total_items - int(cancelled_items),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    print(\"\\nâœ… OrderItem analysis complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No OrderItems to analyze\")\n",
    "\n",
    "orderitem_summary_df = pd.DataFrame(orderitem_summary)\n",
    "orderitem_dq_df = pd.DataFrame(orderitem_data_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13D: PRODUCT FAMILY SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Product Family Summary created: 49 families\n",
      "\n",
      "Top 10 by Item Count:\n",
      "                      Product_Family  Item_Count  Order_Count   Total_MRC\n",
      "33             Point-to-Point (PTPS)        4987         4970  3963654.84\n",
      "11  Dedicated Internet Access (DIAS)        3461         3431  2929647.50\n",
      "21                                IP        3323         3165     4473.25\n",
      "20               Hosted Voice (VOIC)        1648          417   189779.34\n",
      "6                  Dark Fiber (DFBR)        1467          771   601983.15\n",
      "32        Point-to-MultiPoint (PMPS)        1340         1340   970607.99\n",
      "37                        Promotions         875          867    -9700.00\n",
      "26             Managed Service (MSP)         771          270   126825.24\n",
      "18                      Handoff Type         575          573        0.00\n",
      "13                         Diversity         497          247       20.00\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13D: PRODUCT FAMILY SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13D: PRODUCT FAMILY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(order_items_df) > 0:\n",
    "    # Create product family summary with financials\n",
    "    product_summary = (\n",
    "        order_items_df.groupby(\"Product_Family__c\", dropna=False)\n",
    "        .agg(\n",
    "            {\n",
    "                \"Id\": \"count\",\n",
    "                \"OrderId\": \"nunique\",\n",
    "                \"Total_MRC_Amortized__c\": \"sum\",\n",
    "                \"NRC_IRU_FEE__c\": \"sum\",\n",
    "                \"Vendor_Fees_Monthly__c\": \"sum\",\n",
    "                \"UnitPrice\": \"mean\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    product_summary.columns = [\n",
    "        \"Product_Family\",\n",
    "        \"Item_Count\",\n",
    "        \"Order_Count\",\n",
    "        \"Total_MRC\",\n",
    "        \"Total_NRC\",\n",
    "        \"Total_Vendor_MRC\",\n",
    "        \"Avg_UnitPrice\",\n",
    "    ]\n",
    "\n",
    "    # Sort by item count\n",
    "    product_summary = product_summary.sort_values(\"Item_Count\", ascending=False)\n",
    "\n",
    "    # Fill nulls for display\n",
    "    product_summary[\"Product_Family\"] = product_summary[\"Product_Family\"].fillna(\n",
    "        \"(null)\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Product Family Summary created: {len(product_summary)} families\")\n",
    "    print(\"\\nTop 10 by Item Count:\")\n",
    "    print(\n",
    "        product_summary.head(10)[\n",
    "            [\"Product_Family\", \"Item_Count\", \"Order_Count\", \"Total_MRC\"]\n",
    "        ].to_string()\n",
    "    )\n",
    "else:\n",
    "    product_summary = pd.DataFrame()\n",
    "\n",
    "product_summary_df = product_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 14: CREATING BAN MAPPING TABLE\n",
      "================================================================================\n",
      "\n",
      "âœ… BAN mappings with orders: 2,441\n"
     ]
    }
   ],
   "source": [
    "# === STEP 14: CREATE BAN MAPPING TABLE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 14: CREATING BAN MAPPING TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    ban_mapping_data = []\n",
    "    legacy_ban_counts = orders_ready_df.groupby(\"Billing_Invoice__c\").size().to_dict()\n",
    "\n",
    "    for legacy_id, bbf_ban in legacy_to_bbf_ban.items():\n",
    "        order_count = legacy_ban_counts.get(legacy_id, 0)\n",
    "        if order_count > 0:\n",
    "            ban_mapping_data.append(\n",
    "                {\n",
    "                    \"Legacy_BAN_Id\": legacy_id,\n",
    "                    \"New_BBF_BAN_Id\": bbf_ban[\"Id\"],\n",
    "                    \"New_BBF_BAN_Name\": bbf_ban[\"Name\"],\n",
    "                    \"Account__c\": bbf_ban[\"Account__c\"],\n",
    "                    \"Account_Name\": bbf_ban[\"Account_Name\"],\n",
    "                    \"Order_Count\": order_count,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    ban_mapping_df = pd.DataFrame(ban_mapping_data)\n",
    "    ban_mapping_df = ban_mapping_df.sort_values(\"Order_Count\", ascending=False)\n",
    "    print(f\"\\nâœ… BAN mappings with orders: {len(ban_mapping_df):,}\")\n",
    "else:\n",
    "    ban_mapping_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 15: MIGRATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "FILTER PIPELINE:\n",
      "  Total Active Status Orders:     17,980\n",
      "  â”œâ”€ Excluded (PA MARKET DECOM):  887\n",
      "  â”œâ”€ Excluded (Work Orders):      3,484\n",
      "  â”œâ”€ Excluded (Not Billing OSS):  2,119\n",
      "  â””â”€ Confirmed Actively Billing:  11,490\n",
      "\n",
      "BAN MAPPING:\n",
      "  â”œâ”€ Ready to migrate:            11,476\n",
      "  â”œâ”€ Missing BBF BAN:             12\n",
      "  â””â”€ Missing ANY BAN:             2\n",
      "\n",
      "RECORDS TO MIGRATE:\n",
      "  â”œâ”€ Orders â†’ Service__c:         11,476\n",
      "  â”œâ”€ OrderItems â†’ Service_Charge: 20,608\n",
      "  â”œâ”€ BANs:                        2,441\n",
      "  â”œâ”€ Accounts:                    2,225\n",
      "  â”œâ”€ Contacts:                    15,583\n",
      "  â”œâ”€ Locations:                   10,176\n",
      "  â””â”€ Off_Net:                     2,157\n",
      "\n",
      "NODE/RING ANALYSIS:\n",
      "  â”œâ”€ Nodes (via SOA lookup):      2,021\n",
      "  â””â”€ Rings:                       1,052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === STEP 15: GENERATE SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 15: MIGRATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get OrderItem count safely\n",
    "orderitem_count = (\n",
    "    len(order_items_df) if \"order_items_df\" in dir() and len(order_items_df) > 0 else 0\n",
    ")\n",
    "product_family_count = (\n",
    "    order_items_df[\"Product_Family__c\"].nunique() if orderitem_count > 0 else 0\n",
    ")\n",
    "\n",
    "summary_data = [\n",
    "    {\"Category\": \"FILTER PIPELINE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"1. Total Active Status Orders\",\n",
    "        \"Count\": len(all_orders_df),\n",
    "        \"Notes\": \"All orders with qualifying status\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"2. Excluded (PA MARKET DECOM)\",\n",
    "        \"Count\": len(excluded_pa_decom_df),\n",
    "        \"Notes\": \"Project_Group__c contains 'PA MARKET DECOM'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"3. Excluded (Work Orders)\",\n",
    "        \"Count\": len(excluded_work_orders_df),\n",
    "        \"Notes\": f\"Service_Order_Record_Type__c != '{VALID_RECORD_TYPE}'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"4. Excluded (Not Actively Billing)\",\n",
    "        \"Count\": len(excluded_not_billing_df),\n",
    "        \"Notes\": f\"OSS: not in {ACTIVE_OSS_STATES} or billing dates invalid\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"5. Confirmed Actively Billing\",\n",
    "        \"Count\": len(active_orders_df),\n",
    "        \"Notes\": \"Passed all filters\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"BAN MAPPING\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Ready (has BBF BAN)\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"Can be migrated now\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing BBF BAN\",\n",
    "        \"Count\": len(orders_no_bbf_ban_df),\n",
    "        \"Notes\": \"Need new BBF BAN created\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing ANY BAN\",\n",
    "        \"Count\": len(orders_no_ban_df),\n",
    "        \"Notes\": \"CRITICAL - no BAN reference\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"RECORDS TO MIGRATE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Orders â†’ Service__c\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"ES Order to BBF Service__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"OrderItems â†’ Service_Charge__c\",\n",
    "        \"Count\": orderitem_count,\n",
    "        \"Notes\": \"ES OrderItem to BBF Service_Charge__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"BANs (with orders)\",\n",
    "        \"Count\": len(ban_mapping_df),\n",
    "        \"Notes\": \"Billing_Invoice__c to BAN__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Accounts\",\n",
    "        \"Count\": len(accounts_df),\n",
    "        \"Notes\": \"Unique accounts from BBF BANs\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Contacts\",\n",
    "        \"Count\": len(contacts_df),\n",
    "        \"Notes\": \"Contacts for migration accounts\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Locations\",\n",
    "        \"Count\": len(locations_df),\n",
    "        \"Notes\": \"Address_A + Address_Z from orders\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Off_Net\",\n",
    "        \"Count\": len(offnet_df),\n",
    "        \"Notes\": \"Off_Net for migration locations\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"NODE/RING ANALYSIS\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Nodes (linked to Orders)\",\n",
    "        \"Count\": len(nodes_df),\n",
    "        \"Notes\": \"Via Service_Order_Agreement__c lookup\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Rings\",\n",
    "        \"Count\": len(rings_df),\n",
    "        \"Notes\": \"From Node.Ring__c references\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "ðŸ“Š SUMMARY\n",
    "{'â”€'*50}\n",
    "FILTER PIPELINE:\n",
    "  Total Active Status Orders:     {len(all_orders_df):,}\n",
    "  â”œâ”€ Excluded (PA MARKET DECOM):  {len(excluded_pa_decom_df):,}\n",
    "  â”œâ”€ Excluded (Work Orders):      {len(excluded_work_orders_df):,}\n",
    "  â”œâ”€ Excluded (Not Billing OSS):  {len(excluded_not_billing_df):,}\n",
    "  â””â”€ Confirmed Actively Billing:  {len(active_orders_df):,}\n",
    "\n",
    "BAN MAPPING:\n",
    "  â”œâ”€ Ready to migrate:            {len(orders_ready_df):,}\n",
    "  â”œâ”€ Missing BBF BAN:             {len(orders_no_bbf_ban_df):,}\n",
    "  â””â”€ Missing ANY BAN:             {len(orders_no_ban_df):,}\n",
    "\n",
    "RECORDS TO MIGRATE:\n",
    "  â”œâ”€ Orders â†’ Service__c:         {len(orders_ready_df):,}\n",
    "  â”œâ”€ OrderItems â†’ Service_Charge: {orderitem_count:,}\n",
    "  â”œâ”€ BANs:                        {len(ban_mapping_df):,}\n",
    "  â”œâ”€ Accounts:                    {len(accounts_df):,}\n",
    "  â”œâ”€ Contacts:                    {len(contacts_df):,}\n",
    "  â”œâ”€ Locations:                   {len(locations_df):,}\n",
    "  â””â”€ Off_Net:                     {len(offnet_df):,}\n",
    "\n",
    "NODE/RING ANALYSIS:\n",
    "  â”œâ”€ Nodes (via SOA lookup):      {len(nodes_df):,}\n",
    "  â””â”€ Rings:                       {len(rings_df):,}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 16: EXPORTING TO EXCEL\n",
      "================================================================================\n",
      "   âœ… Legend\n",
      "   âœ… Summary\n",
      "   âœ… Active_Orders (11,476 records)\n",
      "   âœ… OrderItems (20,608 records)\n",
      "   âœ… OrderItem_Summary (9 metrics)\n",
      "   âœ… Product_Family_Summary (49 families)\n",
      "   âœ… BAN_Mapping (2,441 records)\n",
      "   âœ… Accounts (2,225 records)\n",
      "   âœ… Contacts (15,583 records)\n",
      "   âœ… Locations (10,176 records)\n",
      "   âœ… Off_Net (2,157 records)\n",
      "   âœ… Data_Quality (3 issues)\n",
      "   âœ… Orders_Missing_BBF_BAN (12 records)\n",
      "   âœ… Excluded_PA_DECOM (887 records)\n",
      "   âœ… Excluded_Work_Orders (3,484 records)\n",
      "   âœ… Excluded_Not_Billing (2,119 records)\n",
      "\n",
      "   Creating Field Reference sheets...\n",
      "   âœ… Fields_Order\n",
      "   âœ… Fields_OrderItem\n",
      "   âœ… Fields_BAN\n",
      "   âœ… Fields_Account\n",
      "   âœ… Fields_Contact\n",
      "   âœ… Fields_Location\n",
      "\n",
      "   Creating Node/Ring Analysis sheets...\n",
      "   âœ… Nodes (2,021 records)\n",
      "   âœ… Rings (1,052 records)\n",
      "   âœ… Node_Analysis (2,021 records)\n",
      "   âœ… Node_Address_Match (2,021 records)\n",
      "   âœ… Node_Summary (8 metrics)\n",
      "\n",
      "âœ… Excel file saved: es_bbf_migration_analysis_v6_20260108_131334.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === STEP 16: EXPORT TO EXCEL ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 16: EXPORTING TO EXCEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "wb = Workbook()\n",
    "\n",
    "# Styles\n",
    "header_font = Font(bold=True, size=12, color=\"FFFFFF\")\n",
    "header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "section_font = Font(bold=True, size=11)\n",
    "section_fill = PatternFill(start_color=\"D9E2F3\", end_color=\"D9E2F3\", fill_type=\"solid\")\n",
    "bold_font = Font(bold=True)\n",
    "thin_border = Border(\n",
    "    left=Side(style=\"thin\"),\n",
    "    right=Side(style=\"thin\"),\n",
    "    top=Side(style=\"thin\"),\n",
    "    bottom=Side(style=\"thin\"),\n",
    ")\n",
    "\n",
    "\n",
    "def write_df_to_sheet(ws, df, start_row=1):\n",
    "    \"\"\"Write dataframe to worksheet with formatting\"\"\"\n",
    "    for r_idx, row in enumerate(\n",
    "        dataframe_to_rows(df, index=False, header=True), start=start_row\n",
    "    ):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == start_row:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            cell.border = thin_border\n",
    "\n",
    "\n",
    "# Get counts for legend\n",
    "orderitem_count_val = (\n",
    "    len(order_items_df) if \"order_items_df\" in dir() and len(order_items_df) > 0 else 0\n",
    ")\n",
    "product_family_count_val = (\n",
    "    order_items_df[\"Product_Family__c\"].nunique() if orderitem_count_val > 0 else 0\n",
    ")\n",
    "\n",
    "# --- SHEET 1: LEGEND (comprehensive documentation) ---\n",
    "ws_legend = wb.active\n",
    "ws_legend.title = \"Legend\"\n",
    "\n",
    "row = 1\n",
    "# Title\n",
    "ws_legend[f\"A{row}\"] = \"ES â†’ BBF Migration Analysis (v5)\"\n",
    "ws_legend[f\"A{row}\"].font = Font(bold=True, size=14)\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "row += 3\n",
    "\n",
    "# Document Overview\n",
    "ws_legend[f\"A{row}\"] = \"DOCUMENT OVERVIEW\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Purpose:\"\n",
    "ws_legend[f\"B{row}\"] = (\n",
    "    \"Comprehensive analysis of ES Salesforce data required for migration to BBF Salesforce\"\n",
    ")\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Driving Principle:\"\n",
    "ws_legend[f\"B{row}\"] = (\n",
    "    \"Everything is driven from Active ES Orders that are ACTUALLY BILLING in OSS\"\n",
    ")\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Source:\"\n",
    "ws_legend[f\"B{row}\"] = \"es_bbf_migration_data_analysis_v5 notebook\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"OSS Validation:\"\n",
    "ws_legend[f\"B{row}\"] = (\n",
    "    f\"order_state_cd IN {ACTIVE_OSS_STATES}, bill_start_date <= today, bill_end_date IS NULL or > today\"\n",
    ")\n",
    "row += 2\n",
    "\n",
    "# Active Order Criteria\n",
    "ws_legend[f\"A{row}\"] = \"ACTIVE ORDER CRITERIA\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Included Statuses:\"\n",
    "ws_legend[f\"B{row}\"] = \", \".join(ACTIVE_STATUSES)\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Record Type:\"\n",
    "ws_legend[f\"B{row}\"] = f\"Service_Order_Record_Type__c = '{VALID_RECORD_TYPE}'\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Excluded:\"\n",
    "ws_legend[f\"B{row}\"] = \"Orders where Project_Group__c contains 'PA MARKET DECOM'\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"BAN Requirement:\"\n",
    "ws_legend[f\"B{row}\"] = (\n",
    "    \"Must have Billing_Invoice__c linked to a BBF_Ban__c = true record\"\n",
    ")\n",
    "row += 2\n",
    "\n",
    "# Migration Scope Breakdown\n",
    "ws_legend[f\"A{row}\"] = \"MIGRATION SCOPE BREAKDOWN\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Total Active Status Orders:\"\n",
    "ws_legend[f\"B{row}\"] = len(all_orders_df)\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Excluded (PA MARKET DECOM):\"\n",
    "ws_legend[f\"B{row}\"] = len(excluded_pa_decom_df)\n",
    "ws_legend[f\"C{row}\"] = \"Orders in decommission project - not migrated\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Excluded (Work Orders):\"\n",
    "ws_legend[f\"B{row}\"] = len(excluded_work_orders_df)\n",
    "ws_legend[f\"C{row}\"] = \"Service_Order_Record_Type__c != Service Order Agreement\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Excluded (Not Actively Billing):\"\n",
    "ws_legend[f\"B{row}\"] = len(excluded_not_billing_df)\n",
    "ws_legend[f\"C{row}\"] = f\"OSS: not in {ACTIVE_OSS_STATES} or billing dates invalid\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Confirmed Actively Billing:\"\n",
    "ws_legend[f\"B{row}\"] = len(active_orders_df)\n",
    "ws_legend[f\"C{row}\"] = \"Passed all filters\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Ready (has BBF BAN):\"\n",
    "ws_legend[f\"B{row}\"] = len(orders_ready_df)\n",
    "ws_legend[f\"C{row}\"] = \"Can be migrated now\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Missing BBF BAN:\"\n",
    "ws_legend[f\"B{row}\"] = len(orders_no_bbf_ban_df)\n",
    "ws_legend[f\"C{row}\"] = \"Need new BBF BAN created first\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Missing ANY BAN:\"\n",
    "ws_legend[f\"B{row}\"] = len(orders_no_ban_df)\n",
    "ws_legend[f\"C{row}\"] = \"CRITICAL - cannot migrate, no BAN association\"\n",
    "row += 2\n",
    "\n",
    "# Records to Migrate\n",
    "ws_legend[f\"A{row}\"] = \"RECORDS TO MIGRATE\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "# Table header\n",
    "ws_legend[f\"A{row}\"] = \"ES Object\"\n",
    "ws_legend[f\"B{row}\"] = \"BBF Object\"\n",
    "ws_legend[f\"C{row}\"] = \"Record Count\"\n",
    "ws_legend[f\"D{row}\"] = \"Notes\"\n",
    "ws_legend[f\"A{row}\"].font = bold_font\n",
    "ws_legend[f\"B{row}\"].font = bold_font\n",
    "ws_legend[f\"C{row}\"].font = bold_font\n",
    "ws_legend[f\"D{row}\"].font = bold_font\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Order\"\n",
    "ws_legend[f\"B{row}\"] = \"Service__c\"\n",
    "ws_legend[f\"C{row}\"] = len(orders_ready_df)\n",
    "ws_legend[f\"D{row}\"] = \"Standard to custom - data transformation required\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"OrderItem\"\n",
    "ws_legend[f\"B{row}\"] = \"Service_Charge__c\"\n",
    "ws_legend[f\"C{row}\"] = orderitem_count_val\n",
    "ws_legend[f\"D{row}\"] = \"Standard to custom - data transformation required\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Billing_Invoice__c\"\n",
    "ws_legend[f\"B{row}\"] = \"BAN__c\"\n",
    "ws_legend[f\"C{row}\"] = len(ban_mapping_df)\n",
    "ws_legend[f\"D{row}\"] = \"Custom object to custom object\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Account\"\n",
    "ws_legend[f\"B{row}\"] = \"Account\"\n",
    "ws_legend[f\"C{row}\"] = len(accounts_df)\n",
    "ws_legend[f\"D{row}\"] = \"Direct migration with field mapping\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Contact\"\n",
    "ws_legend[f\"B{row}\"] = \"Contact\"\n",
    "ws_legend[f\"C{row}\"] = len(contacts_df)\n",
    "ws_legend[f\"D{row}\"] = \"Direct migration, requires Account parent\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Address__c\"\n",
    "ws_legend[f\"B{row}\"] = \"Location__c\"\n",
    "ws_legend[f\"C{row}\"] = len(locations_df)\n",
    "ws_legend[f\"D{row}\"] = \"Custom object to custom object\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Off_Net__c\"\n",
    "ws_legend[f\"B{row}\"] = \"Off_Net__c\"\n",
    "ws_legend[f\"C{row}\"] = len(offnet_df)\n",
    "ws_legend[f\"D{row}\"] = \"Same object name, field mapping needed\"\n",
    "row += 2\n",
    "\n",
    "# Key Tracking Fields\n",
    "ws_legend[f\"A{row}\"] = \"KEY TRACKING FIELDS\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"BBF_New_Id__c\"\n",
    "ws_legend[f\"B{row}\"] = \"On ES records\"\n",
    "ws_legend[f\"C{row}\"] = \"Stores the BBF Salesforce Id after successful migration\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"ES_Legacy_Id__c\"\n",
    "ws_legend[f\"B{row}\"] = \"On BBF records\"\n",
    "ws_legend[f\"C{row}\"] = \"Stores the original ES Salesforce Id for traceability\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"New_BBF_BAN_Id\"\n",
    "ws_legend[f\"B{row}\"] = \"In Active_Orders sheet\"\n",
    "ws_legend[f\"C{row}\"] = \"The BBF BAN__c Id that will be parent for migrated service\"\n",
    "row += 2\n",
    "\n",
    "# Active Date Logic\n",
    "ws_legend[f\"A{row}\"] = \"ACTIVE DATE LOGIC (for Service__c.Active_Date__c)\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Primary:\"\n",
    "ws_legend[f\"B{row}\"] = \"ES Order.Billing_Start_Date__c\"\n",
    "ws_legend[f\"C{row}\"] = \"When billing started on current active order\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Fallback:\"\n",
    "ws_legend[f\"B{row}\"] = \"OSS om.orders.bill_start_date\"\n",
    "ws_legend[f\"C{row}\"] = \"If SF Billing_Start_Date__c is null\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Note:\"\n",
    "ws_legend[f\"B{row}\"] = (\n",
    "    \"Service_Start_Date__c is historical (when service originally started) - NOT used for Active_Date__c\"\n",
    ")\n",
    "row += 2\n",
    "\n",
    "# Sheet Descriptions\n",
    "ws_legend[f\"A{row}\"] = \"SHEET DESCRIPTIONS\"\n",
    "ws_legend[f\"A{row}\"].font = section_font\n",
    "ws_legend[f\"A{row}\"].fill = section_fill\n",
    "row += 1\n",
    "# Table header\n",
    "ws_legend[f\"A{row}\"] = \"Sheet Name\"\n",
    "ws_legend[f\"B{row}\"] = \"Record Count\"\n",
    "ws_legend[f\"C{row}\"] = \"Purpose\"\n",
    "ws_legend[f\"A{row}\"].font = bold_font\n",
    "ws_legend[f\"B{row}\"].font = bold_font\n",
    "ws_legend[f\"C{row}\"].font = bold_font\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Summary\"\n",
    "ws_legend[f\"B{row}\"] = \"metrics\"\n",
    "ws_legend[f\"C{row}\"] = \"High-level overview of migration scope with counts by category\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Active_Orders\"\n",
    "ws_legend[f\"B{row}\"] = len(orders_ready_df)\n",
    "ws_legend[f\"C{row}\"] = (\n",
    "    \"ES Orders ready for migration (have BBF BAN mapping) â†’ Service__c\"\n",
    ")\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"OrderItems\"\n",
    "ws_legend[f\"B{row}\"] = orderitem_count_val\n",
    "ws_legend[f\"C{row}\"] = \"ES OrderItems for migration orders â†’ Service_Charge__c\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"BAN_Mapping\"\n",
    "ws_legend[f\"B{row}\"] = len(ban_mapping_df)\n",
    "ws_legend[f\"C{row}\"] = \"Mapping from ES Billing_Invoice__c to BBF BAN__c records\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Accounts\"\n",
    "ws_legend[f\"B{row}\"] = len(accounts_df)\n",
    "ws_legend[f\"C{row}\"] = \"ES Accounts to migrate (unique from BANs with orders)\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Contacts\"\n",
    "ws_legend[f\"B{row}\"] = len(contacts_df)\n",
    "ws_legend[f\"C{row}\"] = \"ES Contacts associated with migration accounts\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Locations\"\n",
    "ws_legend[f\"B{row}\"] = len(locations_df)\n",
    "ws_legend[f\"C{row}\"] = \"ES Address__c records (A and Z endpoints) â†’ Location__c\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Off_Net\"\n",
    "ws_legend[f\"B{row}\"] = len(offnet_df)\n",
    "ws_legend[f\"C{row}\"] = \"Off_Net__c carrier/vendor records for migration locations\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Data_Quality\"\n",
    "ws_legend[f\"B{row}\"] = \"issues\"\n",
    "ws_legend[f\"C{row}\"] = \"Data quality issues affecting migration\"\n",
    "row += 1\n",
    "ws_legend[f\"A{row}\"] = \"Fields_*\"\n",
    "ws_legend[f\"B{row}\"] = \"reference\"\n",
    "ws_legend[f\"C{row}\"] = \"Field lists for each object being migrated\"\n",
    "\n",
    "# Column widths for Legend\n",
    "ws_legend.column_dimensions[\"A\"].width = 32\n",
    "ws_legend.column_dimensions[\"B\"].width = 35\n",
    "ws_legend.column_dimensions[\"C\"].width = 60\n",
    "ws_legend.column_dimensions[\"D\"].width = 50\n",
    "\n",
    "print(\"   âœ… Legend\")\n",
    "\n",
    "# --- SHEET 2: Summary ---\n",
    "ws_summary = wb.create_sheet(\"Summary\")\n",
    "ws_summary.append([\"ES â†’ BBF Migration Analysis (v5)\"])\n",
    "ws_summary[\"A1\"].font = Font(bold=True, size=16)\n",
    "ws_summary.append([f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"])\n",
    "ws_summary.append(\n",
    "    [\n",
    "        f\"OSS Active States: {ACTIVE_OSS_STATES} | Record Type filter | OSS Billing validation | OrderItem Analysis\"\n",
    "    ]\n",
    ")\n",
    "ws_summary.append([])\n",
    "write_df_to_sheet(ws_summary, summary_df, start_row=5)\n",
    "\n",
    "ws_summary.column_dimensions[\"A\"].width = 20\n",
    "ws_summary.column_dimensions[\"B\"].width = 40\n",
    "ws_summary.column_dimensions[\"C\"].width = 12\n",
    "ws_summary.column_dimensions[\"D\"].width = 55\n",
    "print(\"   âœ… Summary\")\n",
    "\n",
    "# --- SHEET 3: Active Orders (Ready) ---\n",
    "if len(orders_ready_df) > 0:\n",
    "    ws = wb.create_sheet(\"Active_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"New_BBF_BAN_Id\",\n",
    "        \"New_BBF_BAN_Name\",\n",
    "        \"Address_A__c\",\n",
    "        \"Address_Z__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "        \"Service_Start_Date__c\",\n",
    "        \"Billing_Start_Date__c\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_ready_df.columns]\n",
    "    write_df_to_sheet(ws, orders_ready_df[export_cols])\n",
    "    print(f\"   âœ… Active_Orders ({len(orders_ready_df):,} records)\")\n",
    "\n",
    "# --- SHEET 4: Order Items ---\n",
    "if \"order_items_df\" in dir() and len(order_items_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItems\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"OrderId\",\n",
    "        \"OrderItemNumber\",\n",
    "        \"Product_Name__c\",\n",
    "        \"Product_Family__c\",\n",
    "        \"Quantity\",\n",
    "        \"UnitPrice\",\n",
    "        \"TotalPrice\",\n",
    "        \"Total_MRC_Amortized__c\",\n",
    "        \"NRC_IRU_FEE__c\",\n",
    "        \"Vendor_Fees_Monthly__c\",\n",
    "        \"ServiceDate\",\n",
    "        \"EndDate\",\n",
    "        \"Bandwidth_NEW__c\",\n",
    "        \"Term__c\",\n",
    "        \"Cancelled__c\",\n",
    "        \"SBQQ__ChargeType__c\",\n",
    "        \"Last_Mile_Carrier__c\",\n",
    "        \"OFF_NET_IDs__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in order_items_df.columns]\n",
    "    write_df_to_sheet(ws, order_items_df[export_cols])\n",
    "    print(f\"   âœ… OrderItems ({len(order_items_df):,} records)\")\n",
    "\n",
    "# --- SHEET 5: OrderItem Summary ---\n",
    "if \"orderitem_summary_df\" in dir() and len(orderitem_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItem_Summary\")\n",
    "    write_df_to_sheet(ws, orderitem_summary_df)\n",
    "    print(f\"   âœ… OrderItem_Summary ({len(orderitem_summary_df):,} metrics)\")\n",
    "\n",
    "# --- SHEET 6: Product Family Summary ---\n",
    "if \"product_summary_df\" in dir() and len(product_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"Product_Family_Summary\")\n",
    "    write_df_to_sheet(ws, product_summary_df)\n",
    "    print(f\"   âœ… Product_Family_Summary ({len(product_summary_df):,} families)\")\n",
    "\n",
    "# --- SHEET 7: BAN Mapping ---\n",
    "if len(ban_mapping_df) > 0:\n",
    "    ws = wb.create_sheet(\"BAN_Mapping\")\n",
    "    write_df_to_sheet(ws, ban_mapping_df)\n",
    "    print(f\"   âœ… BAN_Mapping ({len(ban_mapping_df):,} records)\")\n",
    "\n",
    "# --- SHEET 8: Accounts ---\n",
    "if len(accounts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Accounts\")\n",
    "    write_df_to_sheet(ws, accounts_df)\n",
    "    print(f\"   âœ… Accounts ({len(accounts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 9: Contacts ---\n",
    "if len(contacts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Contacts\")\n",
    "    write_df_to_sheet(ws, contacts_df)\n",
    "    print(f\"   âœ… Contacts ({len(contacts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 10: Locations ---\n",
    "if len(locations_df) > 0:\n",
    "    ws = wb.create_sheet(\"Locations\")\n",
    "    write_df_to_sheet(ws, locations_df)\n",
    "    print(f\"   âœ… Locations ({len(locations_df):,} records)\")\n",
    "\n",
    "# --- SHEET 11: Off_Net ---\n",
    "if len(offnet_df) > 0:\n",
    "    ws = wb.create_sheet(\"Off_Net\")\n",
    "    write_df_to_sheet(ws, offnet_df)\n",
    "    print(f\"   âœ… Off_Net ({len(offnet_df):,} records)\")\n",
    "\n",
    "# --- SHEET 12: Data Quality (combined Order + OrderItem) ---\n",
    "combined_dq = data_quality_issues.copy()\n",
    "if \"orderitem_data_quality\" in dir() and len(orderitem_data_quality) > 0:\n",
    "    combined_dq.extend(orderitem_data_quality)\n",
    "\n",
    "if len(combined_dq) > 0:\n",
    "    ws = wb.create_sheet(\"Data_Quality\")\n",
    "    dq_df = pd.DataFrame(combined_dq)\n",
    "    write_df_to_sheet(ws, dq_df)\n",
    "    print(f\"   âœ… Data_Quality ({len(combined_dq):,} issues)\")\n",
    "\n",
    "# --- SHEET 13: Orders Missing BBF BAN ---\n",
    "if len(orders_no_bbf_ban_df) > 0:\n",
    "    ws = wb.create_sheet(\"Orders_Missing_BBF_BAN\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_no_bbf_ban_df.columns]\n",
    "    write_df_to_sheet(ws, orders_no_bbf_ban_df[export_cols])\n",
    "    print(f\"   âœ… Orders_Missing_BBF_BAN ({len(orders_no_bbf_ban_df):,} records)\")\n",
    "\n",
    "# --- SHEET 14: Excluded - PA MARKET DECOM ---\n",
    "if len(excluded_pa_decom_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_PA_DECOM\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Project_Group__c\",\n",
    "        \"Account_Name\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_pa_decom_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_pa_decom_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_PA_DECOM ({len(excluded_pa_decom_df):,} records)\")\n",
    "\n",
    "# --- SHEET 15: Excluded - Work Orders (enriched with OSS data) ---\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Work_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Service_Order_Record_Type__c\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_nm\",\n",
    "        \"order_id\",\n",
    "        \"workorder_type_cd\",\n",
    "        \"workorder_type_desc\",\n",
    "        \"workorder_state_cd\",\n",
    "        \"workorder_state_desc\",\n",
    "        \"description\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_work_orders_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_work_orders_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Work_Orders ({len(excluded_work_orders_df):,} records)\")\n",
    "\n",
    "# --- SHEET 16: Excluded - Not Actively Billing ---\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Not_Billing\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"OSS_State_Desc\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "        \"OSS_Billing_Status\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_not_billing_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_not_billing_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Not_Billing ({len(excluded_not_billing_df):,} records)\")\n",
    "\n",
    "# --- SHEET 17: OrderItem Data Quality ---\n",
    "if \"orderitem_dq_df\" in dir() and len(orderitem_dq_df) > 0:\n",
    "    ws = wb.create_sheet(\"OrderItem_Data_Quality\")\n",
    "    write_df_to_sheet(ws, orderitem_dq_df)\n",
    "    print(f\"   âœ… OrderItem_Data_Quality ({len(orderitem_dq_df):,} issues)\")\n",
    "\n",
    "# === FIELD REFERENCE SHEETS ===\n",
    "print(\"\\n   Creating Field Reference sheets...\")\n",
    "\n",
    "# --- Fields: Order â†’ Service__c ---\n",
    "ws = wb.create_sheet(\"Fields_Order\")\n",
    "order_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Service_ID__c\", \"BBF Field\": \"Name\", \"Notes\": \"Service identifier\"},\n",
    "    {\n",
    "        \"ES Field\": \"AccountId\",\n",
    "        \"BBF Field\": \"Account__c\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Invoice__c\",\n",
    "        \"BBF Field\": \"Billing_Account_Number__c\",\n",
    "        \"Notes\": \"Lookup via BAN migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_A__c\",\n",
    "        \"BBF Field\": \"A_Location__c\",\n",
    "        \"Notes\": \"Lookup via Location migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_Z__c\",\n",
    "        \"BBF Field\": \"Z_Location__c\",\n",
    "        \"Notes\": \"Lookup via Location migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Node__c\",\n",
    "        \"BBF Field\": \"A_Node__c / Z_Node__c\",\n",
    "        \"Notes\": \"ES has single node - mapping TBD\",\n",
    "    },\n",
    "    {\"ES Field\": \"Status\", \"BBF Field\": \"Status__c\", \"Notes\": \"Value mapping required\"},\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Start_Date__c\",\n",
    "        \"BBF Field\": \"Active_Date__c\",\n",
    "        \"Notes\": \"PRIMARY date source\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_End_Date__c\",\n",
    "        \"BBF Field\": \"Disconnect_Date__c\",\n",
    "        \"Notes\": \"Direct copy if present\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Service_Provided__c\",\n",
    "        \"BBF Field\": \"Bandwidth__c\",\n",
    "        \"Notes\": \"Service bandwidth in Mbps\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"SOF_MRC__c\",\n",
    "        \"BBF Field\": \"mrc__c\",\n",
    "        \"Notes\": \"Monthly recurring charge\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Primary_Product_Family__c\",\n",
    "        \"BBF Field\": \"Product_Type__c\",\n",
    "        \"Notes\": \"Value mapping required\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_Circuit_ID__c\",\n",
    "        \"BBF Field\": \"Vendor_Circuit_ID__c\",\n",
    "        \"Notes\": \"Direct copy\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"OSS_Order__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"OSS order ID for billing validation\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"OSS_Service_ID__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"OSS service identifier\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"CreatedDate\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"Original creation date\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(order_fields))\n",
    "ws.column_dimensions[\"A\"].width = 25\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 45\n",
    "print(\"   âœ… Fields_Order\")\n",
    "\n",
    "# --- Fields: OrderItem â†’ Service_Charge__c ---\n",
    "ws = wb.create_sheet(\"Fields_OrderItem\")\n",
    "orderitem_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"OrderId\",\n",
    "        \"BBF Field\": \"Service__c\",\n",
    "        \"Notes\": \"Lookup via Orderâ†’Service migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Product_Name__c\",\n",
    "        \"BBF Field\": \"Product_Simple__c\",\n",
    "        \"Notes\": \"Value mapping required\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Product_Family__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"Used for product mapping\",\n",
    "    },\n",
    "    {\"ES Field\": \"UnitPrice\", \"BBF Field\": \"Unit_Rate__c\", \"Notes\": \"Unit price\"},\n",
    "    {\n",
    "        \"ES Field\": \"Total_MRC_Amortized__c\",\n",
    "        \"BBF Field\": \"Amount__c\",\n",
    "        \"Notes\": \"MRC amount\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"NRC_IRU_FEE__c\",\n",
    "        \"BBF Field\": \"NRC__c\",\n",
    "        \"Notes\": \"Non-recurring charge\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_Fees_Monthly__c\",\n",
    "        \"BBF Field\": \"MRC_COGS__c\",\n",
    "        \"Notes\": \"Vendor monthly cost\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Vendor_NRC__c\",\n",
    "        \"BBF Field\": \"NRC_COGS__c\",\n",
    "        \"Notes\": \"Vendor NRC cost\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"ServiceDate\",\n",
    "        \"BBF Field\": \"Start_Date__c\",\n",
    "        \"Notes\": \"Service start date\",\n",
    "    },\n",
    "    {\"ES Field\": \"EndDate\", \"BBF Field\": \"End_Date__c\", \"Notes\": \"Service end date\"},\n",
    "    {\"ES Field\": \"Quantity\", \"BBF Field\": \"Units__c\", \"Notes\": \"Quantity\"},\n",
    "    {\n",
    "        \"ES Field\": \"Description\",\n",
    "        \"BBF Field\": \"Description__c\",\n",
    "        \"Notes\": \"Line item description\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"SBQQ__ChargeType__c\",\n",
    "        \"BBF Field\": \"Charge_Class__c\",\n",
    "        \"Notes\": \"One-Timeâ†’NONRECUR, Recurringâ†’RECUR\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Bandwidth_NEW__c\",\n",
    "        \"BBF Field\": \"(reference)\",\n",
    "        \"Notes\": \"Bandwidth info\",\n",
    "    },\n",
    "    {\"ES Field\": \"Term__c\", \"BBF Field\": \"(reference)\", \"Notes\": \"Contract term\"},\n",
    "    {\n",
    "        \"ES Field\": \"Cancelled__c\",\n",
    "        \"BBF Field\": \"Charge_Active__c\",\n",
    "        \"Notes\": \"Inverse - Active if not cancelled\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Last_Mile_Carrier__c\",\n",
    "        \"BBF Field\": \"Aloc_COGS_Provider__c\",\n",
    "        \"Notes\": \"Vendor lookup\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"OFF_NET_IDs__c\",\n",
    "        \"BBF Field\": \"Off_Net__c\",\n",
    "        \"Notes\": \"Off-net reference\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(orderitem_fields))\n",
    "ws.column_dimensions[\"A\"].width = 25\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 50\n",
    "print(\"   âœ… Fields_OrderItem\")\n",
    "\n",
    "# --- Fields: Billing_Invoice__c â†’ BAN__c ---\n",
    "ws = wb.create_sheet(\"Fields_BAN\")\n",
    "ban_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"BAN identifier\"},\n",
    "    {\n",
    "        \"ES Field\": \"Account__c\",\n",
    "        \"BBF Field\": \"Account__c\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Account_Number__c\",\n",
    "        \"BBF Field\": \"Account_Number__c\",\n",
    "        \"Notes\": \"Account number\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Address_1__c\",\n",
    "        \"BBF Field\": \"Billing_Address_1__c\",\n",
    "        \"Notes\": \"Billing address line 1\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_Address_2__c\",\n",
    "        \"BBF Field\": \"Billing_Address_2__c\",\n",
    "        \"Notes\": \"Billing address line 2\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_City__c\",\n",
    "        \"BBF Field\": \"Billing_City__c\",\n",
    "        \"Notes\": \"Billing city\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_State__c\",\n",
    "        \"BBF Field\": \"Billing_State__c\",\n",
    "        \"Notes\": \"Billing state\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_ZIP__c\",\n",
    "        \"BBF Field\": \"Billing_Zip__c\",\n",
    "        \"Notes\": \"Billing ZIP code\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Billing_E_mail__c\",\n",
    "        \"BBF Field\": \"Billing_Email__c\",\n",
    "        \"Notes\": \"Billing email\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Payment_Terms__c\",\n",
    "        \"BBF Field\": \"Payment_Terms__c\",\n",
    "        \"Notes\": \"Payment terms\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Invoice_Delivery_Preference__c\",\n",
    "        \"BBF Field\": \"Invoice_Delivery_Preference__c\",\n",
    "        \"Notes\": \"Delivery preference\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Disable_Late_Fees__c\",\n",
    "        \"BBF Field\": \"Disable_Late_Fees__c\",\n",
    "        \"Notes\": \"Late fee flag\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Suppress_Invoice_Generation__c\",\n",
    "        \"BBF Field\": \"Suppress_Invoice_Generation__c\",\n",
    "        \"Notes\": \"Suppress invoices\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Description__c\",\n",
    "        \"BBF Field\": \"Description__c\",\n",
    "        \"Notes\": \"Description\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"AP_Contact__c\",\n",
    "        \"BBF Field\": \"AP_Contact__c\",\n",
    "        \"Notes\": \"AP contact lookup\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(ban_fields))\n",
    "ws.column_dimensions[\"A\"].width = 30\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 35\n",
    "print(\"   âœ… Fields_BAN\")\n",
    "\n",
    "# --- Fields: Account â†’ Account ---\n",
    "ws = wb.create_sheet(\"Fields_Account\")\n",
    "account_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"Account name\"},\n",
    "    {\"ES Field\": \"Type\", \"BBF Field\": \"Type\", \"Notes\": \"Account type\"},\n",
    "    {\"ES Field\": \"Industry\", \"BBF Field\": \"Industry\", \"Notes\": \"Industry\"},\n",
    "    {\n",
    "        \"ES Field\": \"BillingStreet\",\n",
    "        \"BBF Field\": \"BillingStreet\",\n",
    "        \"Notes\": \"Billing address\",\n",
    "    },\n",
    "    {\"ES Field\": \"BillingCity\", \"BBF Field\": \"BillingCity\", \"Notes\": \"Billing city\"},\n",
    "    {\"ES Field\": \"BillingState\", \"BBF Field\": \"BillingState\", \"Notes\": \"Billing state\"},\n",
    "    {\n",
    "        \"ES Field\": \"BillingPostalCode\",\n",
    "        \"BBF Field\": \"BillingPostalCode\",\n",
    "        \"Notes\": \"Billing postal code\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"BillingCountry\",\n",
    "        \"BBF Field\": \"BillingCountry\",\n",
    "        \"Notes\": \"Billing country\",\n",
    "    },\n",
    "    {\"ES Field\": \"Phone\", \"BBF Field\": \"Phone\", \"Notes\": \"Phone number\"},\n",
    "    {\"ES Field\": \"Website\", \"BBF Field\": \"Website\", \"Notes\": \"Website URL\"},\n",
    "    {\n",
    "        \"ES Field\": \"AnnualRevenue\",\n",
    "        \"BBF Field\": \"AnnualRevenue\",\n",
    "        \"Notes\": \"Annual revenue\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"NumberOfEmployees\",\n",
    "        \"BBF Field\": \"NumberOfEmployees\",\n",
    "        \"Notes\": \"Employee count\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Description\",\n",
    "        \"BBF Field\": \"Description\",\n",
    "        \"Notes\": \"Account description\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"ShippingStreet\",\n",
    "        \"BBF Field\": \"ShippingStreet\",\n",
    "        \"Notes\": \"Shipping address\",\n",
    "    },\n",
    "    {\"ES Field\": \"ShippingCity\", \"BBF Field\": \"ShippingCity\", \"Notes\": \"Shipping city\"},\n",
    "    {\n",
    "        \"ES Field\": \"ShippingState\",\n",
    "        \"BBF Field\": \"ShippingState\",\n",
    "        \"Notes\": \"Shipping state\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"ShippingPostalCode\",\n",
    "        \"BBF Field\": \"ShippingPostalCode\",\n",
    "        \"Notes\": \"Shipping postal code\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"AccountNumber\",\n",
    "        \"BBF Field\": \"AccountNumber\",\n",
    "        \"Notes\": \"Account number\",\n",
    "    },\n",
    "    {\"ES Field\": \"Ownership\", \"BBF Field\": \"Ownership\", \"Notes\": \"Ownership type\"},\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(account_fields))\n",
    "ws.column_dimensions[\"A\"].width = 22\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 30\n",
    "print(\"   âœ… Fields_Account\")\n",
    "\n",
    "# --- Fields: Contact â†’ Contact ---\n",
    "ws = wb.create_sheet(\"Fields_Contact\")\n",
    "contact_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"AccountId\",\n",
    "        \"BBF Field\": \"AccountId\",\n",
    "        \"Notes\": \"Lookup via Account migration\",\n",
    "    },\n",
    "    {\"ES Field\": \"FirstName\", \"BBF Field\": \"FirstName\", \"Notes\": \"First name\"},\n",
    "    {\"ES Field\": \"LastName\", \"BBF Field\": \"LastName\", \"Notes\": \"Last name\"},\n",
    "    {\"ES Field\": \"Email\", \"BBF Field\": \"Email\", \"Notes\": \"Email address\"},\n",
    "    {\"ES Field\": \"Phone\", \"BBF Field\": \"Phone\", \"Notes\": \"Phone number\"},\n",
    "    {\"ES Field\": \"Title\", \"BBF Field\": \"Title\", \"Notes\": \"Job title\"},\n",
    "    {\"ES Field\": \"MobilePhone\", \"BBF Field\": \"MobilePhone\", \"Notes\": \"Mobile phone\"},\n",
    "    {\"ES Field\": \"Department\", \"BBF Field\": \"Department\", \"Notes\": \"Department\"},\n",
    "    {\n",
    "        \"ES Field\": \"MailingStreet\",\n",
    "        \"BBF Field\": \"MailingStreet\",\n",
    "        \"Notes\": \"Mailing address\",\n",
    "    },\n",
    "    {\"ES Field\": \"MailingCity\", \"BBF Field\": \"MailingCity\", \"Notes\": \"Mailing city\"},\n",
    "    {\"ES Field\": \"MailingState\", \"BBF Field\": \"MailingState\", \"Notes\": \"Mailing state\"},\n",
    "    {\n",
    "        \"ES Field\": \"MailingPostalCode\",\n",
    "        \"BBF Field\": \"MailingPostalCode\",\n",
    "        \"Notes\": \"Mailing postal code\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"MailingCountry\",\n",
    "        \"BBF Field\": \"MailingCountry\",\n",
    "        \"Notes\": \"Mailing country\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Description\",\n",
    "        \"BBF Field\": \"Description\",\n",
    "        \"Notes\": \"Contact description\",\n",
    "    },\n",
    "    {\"ES Field\": \"LeadSource\", \"BBF Field\": \"LeadSource\", \"Notes\": \"Lead source\"},\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(contact_fields))\n",
    "ws.column_dimensions[\"A\"].width = 22\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 30\n",
    "print(\"   âœ… Fields_Contact\")\n",
    "\n",
    "# --- Fields: Address__c â†’ Location__c ---\n",
    "ws = wb.create_sheet(\"Fields_Location\")\n",
    "location_fields = [\n",
    "    {\n",
    "        \"ES Field\": \"Id\",\n",
    "        \"BBF Field\": \"ES_Legacy_Id__c\",\n",
    "        \"Notes\": \"Track original ES record\",\n",
    "    },\n",
    "    {\"ES Field\": \"Name\", \"BBF Field\": \"Name\", \"Notes\": \"Location name\"},\n",
    "    {\"ES Field\": \"Address__c\", \"BBF Field\": \"Street__c\", \"Notes\": \"Street address\"},\n",
    "    {\"ES Field\": \"City__c\", \"BBF Field\": \"City__c\", \"Notes\": \"City\"},\n",
    "    {\"ES Field\": \"State__c\", \"BBF Field\": \"State__c\", \"Notes\": \"State\"},\n",
    "    {\"ES Field\": \"County__c\", \"BBF Field\": \"County__c\", \"Notes\": \"County\"},\n",
    "    {\"ES Field\": \"Zip__c\", \"BBF Field\": \"Zip__c\", \"Notes\": \"ZIP code\"},\n",
    "    {\"ES Field\": \"Country__c\", \"BBF Field\": \"Country__c\", \"Notes\": \"Country\"},\n",
    "    {\"ES Field\": \"CLLI__c\", \"BBF Field\": \"CLLI__c\", \"Notes\": \"CLLI code\"},\n",
    "    {\n",
    "        \"ES Field\": \"Building_Status__c\",\n",
    "        \"BBF Field\": \"Building_Status__c\",\n",
    "        \"Notes\": \"Building status\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Building_Type__c\",\n",
    "        \"BBF Field\": \"Building_Type__c\",\n",
    "        \"Notes\": \"Building type\",\n",
    "    },\n",
    "    {\"ES Field\": \"On_Net__c\", \"BBF Field\": \"On_Net__c\", \"Notes\": \"On-net flag\"},\n",
    "    {\n",
    "        \"ES Field\": \"Geocode_Lat_Long__Latitude__s\",\n",
    "        \"BBF Field\": \"Latitude__c\",\n",
    "        \"Notes\": \"Latitude\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Geocode_Lat_Long__Longitude__s\",\n",
    "        \"BBF Field\": \"Longitude__c\",\n",
    "        \"Notes\": \"Longitude\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_Type__c\",\n",
    "        \"BBF Field\": \"Address_Type__c\",\n",
    "        \"Notes\": \"Address type\",\n",
    "    },\n",
    "    {\n",
    "        \"ES Field\": \"Address_Status__c\",\n",
    "        \"BBF Field\": \"Address_Status__c\",\n",
    "        \"Notes\": \"Address status\",\n",
    "    },\n",
    "]\n",
    "write_df_to_sheet(ws, pd.DataFrame(location_fields))\n",
    "ws.column_dimensions[\"A\"].width = 30\n",
    "ws.column_dimensions[\"B\"].width = 22\n",
    "ws.column_dimensions[\"C\"].width = 25\n",
    "print(\"   âœ… Fields_Location\")\n",
    "\n",
    "\n",
    "# === NODE/RING ANALYSIS SHEETS ===\n",
    "print(\"\\n   Creating Node/Ring Analysis sheets...\")\n",
    "\n",
    "# --- SHEET: Nodes ---\n",
    "if len(nodes_df) > 0:\n",
    "    ws = wb.create_sheet(\"Nodes\")\n",
    "    write_df_to_sheet(ws, nodes_df)\n",
    "    print(f\"   âœ… Nodes ({len(nodes_df):,} records)\")\n",
    "\n",
    "# --- SHEET: Rings ---\n",
    "if len(rings_df) > 0:\n",
    "    ws = wb.create_sheet(\"Rings\")\n",
    "    write_df_to_sheet(ws, rings_df)\n",
    "    print(f\"   âœ… Rings ({len(rings_df):,} records)\")\n",
    "\n",
    "# --- SHEET: Node_Analysis ---\n",
    "if len(node_analysis_df) > 0:\n",
    "    ws = wb.create_sheet(\"Node_Analysis\")\n",
    "    write_df_to_sheet(ws, node_analysis_df)\n",
    "    print(f\"   âœ… Node_Analysis ({len(node_analysis_df):,} records)\")\n",
    "\n",
    "# --- SHEET: Node_Address_Match ---\n",
    "if len(address_match_df) > 0:\n",
    "    ws = wb.create_sheet(\"Node_Address_Match\")\n",
    "    write_df_to_sheet(ws, address_match_df)\n",
    "    print(f\"   âœ… Node_Address_Match ({len(address_match_df):,} records)\")\n",
    "\n",
    "# --- SHEET: Node_Order_Position ---\n",
    "if len(order_position_df) > 0:\n",
    "    ws = wb.create_sheet(\"Node_Order_Position\")\n",
    "    write_df_to_sheet(ws, order_position_df)\n",
    "    print(f\"   âœ… Node_Order_Position ({len(order_position_df):,} records)\")\n",
    "\n",
    "# --- SHEET: Node_Summary ---\n",
    "if len(node_summary_df) > 0:\n",
    "    ws = wb.create_sheet(\"Node_Summary\")\n",
    "    write_df_to_sheet(ws, node_summary_df)\n",
    "    ws.column_dimensions[\"A\"].width = 35\n",
    "    ws.column_dimensions[\"B\"].width = 12\n",
    "    ws.column_dimensions[\"C\"].width = 40\n",
    "    print(f\"   âœ… Node_Summary ({len(node_summary_df):,} metrics)\")\n",
    "\n",
    "# Save\n",
    "wb.save(OUTPUT_FILE)\n",
    "print(f\"\\nâœ… Excel file saved: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANUP\n",
      "================================================================================\n",
      "âœ… OSS connection closed\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Output file: es_bbf_migration_analysis_v6_20260108_131334.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CLEANUP ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "oss_conn.close()\n",
    "print(\"âœ… OSS connection closed\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## v6 Changes Summary\n",
    "\n",
    "### NEW: Node/Ring Analysis (Steps 11B-11F)\n",
    "- **Step 11B**: Query Nodes linked to migrating Orders via Service_Order_Agreement__c\n",
    "- **Step 11C**: Query Rings referenced by those Nodes\n",
    "- **Step 11D**: Analyze Ring position (is Node the Ring's East_POP or West_POP?)\n",
    "- **Step 11E**: Match Node.Address__c (text) to Address__c records, cross-reference with Order A/Z\n",
    "- **Step 11F**: Summary with key questions for process owner discussion\n",
    "\n",
    "### NEW: Excel Sheets\n",
    "- **Nodes**: Raw Node__c records linked to migrating orders\n",
    "- **Rings**: Ring__c records for those nodes\n",
    "- **Node_Analysis**: Ring position analysis per node\n",
    "- **Node_Address_Match**: Node.Address__c â†’ Address__c matching results\n",
    "- **Node_Order_Position**: Cross-reference: Ring position vs Order A/Z\n",
    "- **Node_Summary**: Summary metrics for process owner discussion\n",
    "\n",
    "### Inherited from v5\n",
    "- Comprehensive Legend sheet with documentation\n",
    "- Field reference sheets for each object\n",
    "\n",
    "### Inherited from v4\n",
    "- OrderItem analysis for migration scope orders\n",
    "- Product/Family breakdown analysis\n",
    "- Active Date uses Billing_Start_Date__c (primary) â†’ OSS bill_start_date (fallback)\n",
    "\n",
    "### Inherited from v3\n",
    "- OSS Active States: CL (Closed) and OA (Accepted)\n",
    "- Work Orders enriched with OSS workorder data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
