{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES â†’ BBF Migration Data Analysis (v3)\n",
    "\n",
    "This notebook analyzes all data required for the ES to BBF Salesforce migration.\n",
    "\n",
    "## Version 3 Changes\n",
    "- Include OA (Accepted) orders in addition to CL (Closed) - both must pass billing date criteria\n",
    "- Data Quality updated: OSS `bill_start_date` can be used when SF `Service_Start_Date__c` is missing\n",
    "- Work Orders enriched with data from `workorders.workorders` table\n",
    "\n",
    "## Driving Principle\n",
    "**Everything is driven from Active ES Orders that are ACTUALLY BILLING in OSS** - we migrate only the data needed to support truly active services.\n",
    "\n",
    "## Filter Pipeline\n",
    "1. Status IN ('Activated', 'Suspended (Late Payment)', 'Disconnect in Progress')\n",
    "2. Project_Group__c NOT LIKE '%PA MARKET DECOM%'\n",
    "3. Service_Order_Record_Type__c = 'Service Order Agreement' (excludes Work Orders)\n",
    "4. OSS Actively Billing:\n",
    "   - order_state_cd IN ('CL', 'OA')\n",
    "   - bill_start_date <= today\n",
    "   - bill_end_date IS NULL or > today\n",
    "5. Has BBF BAN mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\vjero\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe\n",
      "Pandas: 2.2.3\n",
      "âœ… Imports successful\n"
     ]
    }
   ],
   "source": [
    "# === SETUP & IMPORTS ===\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from simple_salesforce import Salesforce\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration loaded\n",
      "   Active Statuses: ['Activated', 'Suspended (Late Payment)', 'Disconnect in Progress']\n",
      "   Record Type: Service Order Agreement\n",
      "   Excluding: Project_Group__c LIKE '%PA MARKET DECOM%'\n",
      "   OSS Active States: ['CL', 'OA']\n",
      "   Output: es_bbf_migration_analysis_v3_20260107_203703.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# ES (Source) Salesforce Credentials\n",
    "ES_USERNAME = \"sfdcapi@everstream.net\"\n",
    "ES_PASSWORD = \"pV4CAxns8DQtJsBq!\"\n",
    "ES_TOKEN = \"r1uoYiusK19RbrflARydi86TA\"\n",
    "ES_DOMAIN = \"login\"  # 'login' for production\n",
    "\n",
    "# OSS Database Credentials\n",
    "OSS_HOST = \"pg01.comlink.net\"\n",
    "OSS_PORT = \"5432\"\n",
    "OSS_DB = \"GLC\"\n",
    "OSS_USER = \"oss_server\"\n",
    "OSS_PASSWORD = \"3wU3uB28X?!r2?@ebrUg\"\n",
    "\n",
    "# Active Order Status Filter\n",
    "ACTIVE_STATUSES = [\"Activated\", \"Suspended (Late Payment)\", \"Disconnect in Progress\"]\n",
    "\n",
    "# Record Type Filter\n",
    "VALID_RECORD_TYPE = \"Service Order Agreement\"\n",
    "\n",
    "# PA Market Decom Exclusion\n",
    "PA_DECOM_FILTER = \"PA MARKET DECOM\"\n",
    "\n",
    "# OSS Order States that qualify as \"actively billing\"\n",
    "ACTIVE_OSS_STATES = [\"CL\", \"OA\"]  # Closed and Accepted\n",
    "\n",
    "# OSS Order States Reference\n",
    "OSS_ORDER_STATES = {\n",
    "    \"CL\": \"Closed (Active/Billing)\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"OS\": \"Submitted\",\n",
    "    \"OC\": \"Created\",\n",
    "    \"PN\": \"Pending\",\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"OR\": \"Rejected\",\n",
    "    \"OV\": \"Validated (Disabled)\",\n",
    "}\n",
    "\n",
    "# OSS Work Order Types Reference\n",
    "WORKORDER_TYPES = {\n",
    "    \"IT\": \"Professional Services\",\n",
    "    \"MR\": \"Maintenance/Repair\",\n",
    "    \"OS\": \"Other Service\",\n",
    "    \"VS\": \"Voice Service\",\n",
    "}\n",
    "\n",
    "# OSS Work Order States Reference\n",
    "WORKORDER_STATES = {\n",
    "    \"CA\": \"Cancelled\",\n",
    "    \"CL\": \"Closed\",\n",
    "    \"OA\": \"Accepted\",\n",
    "    \"PN\": \"Pending\",\n",
    "}\n",
    "\n",
    "# Output Configuration\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_FILE = f\"es_bbf_migration_analysis_v3_{TIMESTAMP}.xlsx\"\n",
    "\n",
    "print(\"ðŸ“‹ Configuration loaded\")\n",
    "print(f\"   Active Statuses: {ACTIVE_STATUSES}\")\n",
    "print(f\"   Record Type: {VALID_RECORD_TYPE}\")\n",
    "print(f\"   Excluding: Project_Group__c LIKE '%{PA_DECOM_FILTER}%'\")\n",
    "print(f\"   OSS Active States: {ACTIVE_OSS_STATES}\")\n",
    "print(f\"   Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CONNECTING TO ES SALESFORCE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to ES...\n",
      "âœ… Connected to ES: everstream.my.salesforce.com\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO ES SALESFORCE ===\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONNECTING TO ES SALESFORCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to ES...\")\n",
    "es_sf = Salesforce(\n",
    "    username=ES_USERNAME,\n",
    "    password=ES_PASSWORD,\n",
    "    security_token=ES_TOKEN,\n",
    "    domain=ES_DOMAIN,\n",
    ")\n",
    "print(f\"âœ… Connected to ES: {es_sf.sf_instance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONNECTING TO OSS DATABASE\n",
      "================================================================================\n",
      "\n",
      "ðŸ”Œ Connecting to OSS...\n",
      "âœ… Connected to OSS: pg01.comlink.net/GLC\n"
     ]
    }
   ],
   "source": [
    "# === CONNECT TO OSS DATABASE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONNECTING TO OSS DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”Œ Connecting to OSS...\")\n",
    "oss_conn = psycopg2.connect(\n",
    "    dbname=OSS_DB,\n",
    "    user=OSS_USER,\n",
    "    password=OSS_PASSWORD,\n",
    "    host=OSS_HOST,\n",
    "    port=OSS_PORT,\n",
    ")\n",
    "print(f\"âœ… Connected to OSS: {OSS_HOST}/{OSS_DB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: QUERYING ACTIVE ORDERS\n",
      "================================================================================\n",
      "Querying all orders with active statuses...\n",
      "\n",
      "âœ… Total orders with active status: 17,972\n",
      "\n",
      "ðŸ“Š Record Type Breakdown:\n",
      "Service_Order_Record_Type__c\n",
      "Service Order Agreement    14058\n",
      "Work Order                  3914\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: QUERY ALL ACTIVE ORDERS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: QUERYING ACTIVE ORDERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status_filter = \"','\".join(ACTIVE_STATUSES)\n",
    "\n",
    "orders_query = f\"\"\"\n",
    "SELECT \n",
    "    Id, \n",
    "    Name,\n",
    "    Service_ID__c,\n",
    "    Status,\n",
    "    AccountId,\n",
    "    Account.Name,\n",
    "    Billing_Invoice__c,\n",
    "    Address_A__c,\n",
    "    Address_Z__c,\n",
    "    Node__c,\n",
    "    OpportunityId,\n",
    "    Service_Start_Date__c,\n",
    "    Service_End_Date__c,\n",
    "    Service_Provided__c,\n",
    "    SOF_MRC__c,\n",
    "    OSS_Order__c,\n",
    "    OSS_Service_ID__c,\n",
    "    Vendor_Circuit_ID__c,\n",
    "    Primary_Product_Family__c,\n",
    "    Primary_Product_Name__c,\n",
    "    Project_Group__c,\n",
    "    Service_Order_Record_Type__c,\n",
    "    CreatedDate,\n",
    "    LastModifiedDate\n",
    "FROM Order\n",
    "WHERE Status IN ('{status_filter}')\n",
    "ORDER BY Service_ID__c\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying all orders with active statuses...\")\n",
    "result = es_sf.query_all(orders_query)\n",
    "orders_raw = result[\"records\"]\n",
    "\n",
    "# Flatten results\n",
    "all_orders = []\n",
    "for order in orders_raw:\n",
    "    all_orders.append(\n",
    "        {\n",
    "            \"Id\": order[\"Id\"],\n",
    "            \"Name\": order.get(\"Name\"),\n",
    "            \"Service_ID__c\": order.get(\"Service_ID__c\"),\n",
    "            \"Status\": order[\"Status\"],\n",
    "            \"AccountId\": order.get(\"AccountId\"),\n",
    "            \"Account_Name\": order[\"Account\"][\"Name\"] if order.get(\"Account\") else None,\n",
    "            \"Billing_Invoice__c\": order.get(\"Billing_Invoice__c\"),\n",
    "            \"Address_A__c\": order.get(\"Address_A__c\"),\n",
    "            \"Address_Z__c\": order.get(\"Address_Z__c\"),\n",
    "            \"Node__c\": order.get(\"Node__c\"),\n",
    "            \"OpportunityId\": order.get(\"OpportunityId\"),\n",
    "            \"Service_Start_Date__c\": order.get(\"Service_Start_Date__c\"),\n",
    "            \"Service_End_Date__c\": order.get(\"Service_End_Date__c\"),\n",
    "            \"Service_Provided__c\": order.get(\"Service_Provided__c\"),\n",
    "            \"SOF_MRC__c\": order.get(\"SOF_MRC__c\"),\n",
    "            \"OSS_Order__c\": order.get(\"OSS_Order__c\"),\n",
    "            \"OSS_Service_ID__c\": order.get(\"OSS_Service_ID__c\"),\n",
    "            \"Vendor_Circuit_ID__c\": order.get(\"Vendor_Circuit_ID__c\"),\n",
    "            \"Primary_Product_Family__c\": order.get(\"Primary_Product_Family__c\"),\n",
    "            \"Primary_Product_Name__c\": order.get(\"Primary_Product_Name__c\"),\n",
    "            \"Project_Group__c\": order.get(\"Project_Group__c\"),\n",
    "            \"Service_Order_Record_Type__c\": order.get(\"Service_Order_Record_Type__c\"),\n",
    "            \"CreatedDate\": order.get(\"CreatedDate\"),\n",
    "            \"LastModifiedDate\": order.get(\"LastModifiedDate\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_orders_df = pd.DataFrame(all_orders)\n",
    "print(f\"\\nâœ… Total orders with active status: {len(all_orders_df):,}\")\n",
    "\n",
    "# Show record type breakdown\n",
    "print(f\"\\nðŸ“Š Record Type Breakdown:\")\n",
    "print(all_orders_df[\"Service_Order_Record_Type__c\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FILTER - PA MARKET DECOM\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PA MARKET DECOM Filter:\n",
      "   Before: 17,972\n",
      "   âŒ Excluded (PA MARKET DECOM): 887\n",
      "   âœ… Remaining: 17,085\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: FILTER - PA MARKET DECOM ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: FILTER - PA MARKET DECOM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate PA MARKET DECOM orders\n",
    "pa_decom_mask = (\n",
    "    all_orders_df[\"Project_Group__c\"]\n",
    "    .fillna(\"\")\n",
    "    .str.contains(PA_DECOM_FILTER, case=False)\n",
    ")\n",
    "excluded_pa_decom_df = all_orders_df[pa_decom_mask].copy()\n",
    "orders_after_pa_filter = all_orders_df[~pa_decom_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š PA MARKET DECOM Filter:\")\n",
    "print(f\"   Before: {len(all_orders_df):,}\")\n",
    "print(f\"   âŒ Excluded (PA MARKET DECOM): {len(excluded_pa_decom_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_pa_filter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FILTER - RECORD TYPE\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Record Type Filter:\n",
      "   Before: 17,085\n",
      "   âŒ Excluded (Not 'Service Order Agreement'): 3,477\n",
      "   âœ… Remaining: 13,608\n",
      "\n",
      "   Excluded Record Types:\n",
      "Service_Order_Record_Type__c\n",
      "Work Order    3477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3: FILTER - RECORD TYPE (Service Order Agreement only) ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: FILTER - RECORD TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Keep only Service Order Agreement\n",
    "record_type_mask = (\n",
    "    orders_after_pa_filter[\"Service_Order_Record_Type__c\"] == VALID_RECORD_TYPE\n",
    ")\n",
    "excluded_work_orders_df = orders_after_pa_filter[~record_type_mask].copy()\n",
    "orders_after_rt_filter = orders_after_pa_filter[record_type_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Record Type Filter:\")\n",
    "print(f\"   Before: {len(orders_after_pa_filter):,}\")\n",
    "print(f\"   âŒ Excluded (Not '{VALID_RECORD_TYPE}'): {len(excluded_work_orders_df):,}\")\n",
    "print(f\"   âœ… Remaining: {len(orders_after_rt_filter):,}\")\n",
    "\n",
    "# Show breakdown of what was excluded\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    print(f\"\\n   Excluded Record Types:\")\n",
    "    print(\n",
    "        excluded_work_orders_df[\"Service_Order_Record_Type__c\"].value_counts(\n",
    "            dropna=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\n",
      "================================================================================\n",
      "\n",
      "   Work Orders with OSS_Order__c: 156\n",
      "   Work Orders without OSS_Order__c: 3,321\n",
      "   Unique workorder IDs to query: 156\n",
      "   Chunk 1: Retrieved 84 workorders\n",
      "\n",
      "âœ… Enriched 84 work orders with OSS data\n"
     ]
    }
   ],
   "source": [
    "# === STEP 3B: ENRICH WORK ORDERS WITH OSS DATA ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3B: ENRICHING WORK ORDERS WITH OSS DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    # Get work orders with OSS_Order__c (maps to workorder_id)\n",
    "    has_wo_oss = excluded_work_orders_df[\"OSS_Order__c\"].notna() & (\n",
    "        excluded_work_orders_df[\"OSS_Order__c\"] != \"\"\n",
    "    )\n",
    "    work_orders_with_oss = excluded_work_orders_df[has_wo_oss].copy()\n",
    "\n",
    "    print(f\"\\n   Work Orders with OSS_Order__c: {len(work_orders_with_oss):,}\")\n",
    "    print(f\"   Work Orders without OSS_Order__c: {(~has_wo_oss).sum():,}\")\n",
    "\n",
    "    if len(work_orders_with_oss) > 0:\n",
    "        workorder_ids = (\n",
    "            work_orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "        )\n",
    "        print(f\"   Unique workorder IDs to query: {len(workorder_ids):,}\")\n",
    "\n",
    "        # Query workorders.workorders\n",
    "        chunk_size = 5000\n",
    "        oss_workorders = []\n",
    "\n",
    "        for i in range(0, len(workorder_ids), chunk_size):\n",
    "            chunk = workorder_ids[i : i + chunk_size]\n",
    "            ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "            wo_query = f\"\"\"\n",
    "            SELECT \n",
    "                workorder_id,\n",
    "                order_nm,\n",
    "                order_id,\n",
    "                workorder_type_cd,\n",
    "                workorder_state_cd,\n",
    "                description,\n",
    "                start_date,\n",
    "                end_date,\n",
    "                disabled\n",
    "            FROM workorders.workorders\n",
    "            WHERE workorder_id IN ({ids_str})\n",
    "              AND disabled = 'infinity'\n",
    "            \"\"\"\n",
    "\n",
    "            with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "                cur.execute(wo_query)\n",
    "                rows = cur.fetchall()\n",
    "                oss_workorders.extend([dict(row) for row in rows])\n",
    "\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} workorders\")\n",
    "\n",
    "        if len(oss_workorders) > 0:\n",
    "            oss_wo_df = pd.DataFrame(oss_workorders)\n",
    "\n",
    "            # Add descriptions\n",
    "            oss_wo_df[\"workorder_type_desc\"] = oss_wo_df[\"workorder_type_cd\"].map(\n",
    "                WORKORDER_TYPES\n",
    "            )\n",
    "            oss_wo_df[\"workorder_state_desc\"] = oss_wo_df[\"workorder_state_cd\"].map(\n",
    "                WORKORDER_STATES\n",
    "            )\n",
    "\n",
    "            # Merge with excluded work orders\n",
    "            excluded_work_orders_df[\"OSS_Order_ID\"] = pd.to_numeric(\n",
    "                excluded_work_orders_df[\"OSS_Order__c\"], errors=\"coerce\"\n",
    "            )\n",
    "            excluded_work_orders_df = excluded_work_orders_df.merge(\n",
    "                oss_wo_df, left_on=\"OSS_Order_ID\", right_on=\"workorder_id\", how=\"left\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"\\nâœ… Enriched {oss_wo_df['workorder_id'].notna().sum():,} work orders with OSS data\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\nâš ï¸ No active workorders found in OSS\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No work orders have OSS_Order__c populated\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No work orders to enrich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š OSS_Order__c Population:\n",
      "   With OSS_Order__c: 11,525\n",
      "   Without OSS_Order__c: 2,083\n",
      "\n",
      "   Unique OSS Order IDs to query: 11,518\n",
      "   Chunk 1: Retrieved 5000 orders\n",
      "   Chunk 2: Retrieved 4999 orders\n",
      "   Chunk 3: Retrieved 1518 orders\n",
      "\n",
      "âœ… Total OSS orders retrieved: 11,517\n",
      "   OSS Order IDs not found in OSS: 1\n"
     ]
    }
   ],
   "source": [
    "# === STEP 4: CHECK OSS ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CHECKING OSS ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify orders with OSS_Order__c\n",
    "has_oss_id = orders_after_rt_filter[\"OSS_Order__c\"].notna() & (\n",
    "    orders_after_rt_filter[\"OSS_Order__c\"] != \"\"\n",
    ")\n",
    "orders_with_oss = orders_after_rt_filter[has_oss_id].copy()\n",
    "orders_without_oss = orders_after_rt_filter[~has_oss_id].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š OSS_Order__c Population:\")\n",
    "print(f\"   With OSS_Order__c: {len(orders_with_oss):,}\")\n",
    "print(f\"   Without OSS_Order__c: {len(orders_without_oss):,}\")\n",
    "\n",
    "# Query OSS for orders with OSS_Order__c\n",
    "if len(orders_with_oss) > 0:\n",
    "    oss_order_ids = (\n",
    "        orders_with_oss[\"OSS_Order__c\"].dropna().astype(int).unique().tolist()\n",
    "    )\n",
    "    print(f\"\\n   Unique OSS Order IDs to query: {len(oss_order_ids):,}\")\n",
    "\n",
    "    # Query in chunks\n",
    "    chunk_size = 5000\n",
    "    oss_orders = []\n",
    "\n",
    "    for i in range(0, len(oss_order_ids), chunk_size):\n",
    "        chunk = oss_order_ids[i : i + chunk_size]\n",
    "        ids_str = \",\".join(str(x) for x in chunk)\n",
    "\n",
    "        oss_query = f\"\"\"\n",
    "        SELECT \n",
    "            order_id,\n",
    "            order_state_cd,\n",
    "            order_type_cd,\n",
    "            bill_start_date,\n",
    "            bill_end_date,\n",
    "            circuit_active_date,\n",
    "            account_id,\n",
    "            service_id\n",
    "        FROM om.orders\n",
    "        WHERE order_id IN ({ids_str})\n",
    "        \"\"\"\n",
    "\n",
    "        with oss_conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(oss_query)\n",
    "            rows = cur.fetchall()\n",
    "            oss_orders.extend([dict(row) for row in rows])\n",
    "\n",
    "        print(f\"   Chunk {i//chunk_size + 1}: Retrieved {len(rows)} orders\")\n",
    "\n",
    "    oss_orders_df = pd.DataFrame(oss_orders)\n",
    "    print(f\"\\nâœ… Total OSS orders retrieved: {len(oss_orders_df):,}\")\n",
    "\n",
    "    # Check for OSS IDs not found\n",
    "    found_ids = (\n",
    "        set(oss_orders_df[\"order_id\"].tolist()) if len(oss_orders_df) > 0 else set()\n",
    "    )\n",
    "    not_found_ids = set(oss_order_ids) - found_ids\n",
    "    print(f\"   OSS Order IDs not found in OSS: {len(not_found_ids):,}\")\n",
    "else:\n",
    "    oss_orders_df = pd.DataFrame()\n",
    "    not_found_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: DETERMINING ACTIVELY BILLING STATUS\n",
      "================================================================================\n",
      "\n",
      "Today's date for comparison: 2026-01-07\n",
      "Active OSS States: ['CL', 'OA']\n",
      "\n",
      "ðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\n",
      "   ACTIVELY BILLING: 11,489 (99.7%)\n",
      "   CL - Bill End Passed: 23 (0.2%)\n",
      "   Not Active State (CA): 6 (0.1%)\n",
      "   CL - Bill Start Future: 6 (0.1%)\n",
      "   No OSS Match: 1 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# === STEP 5: DETERMINE ACTIVELY BILLING STATUS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: DETERMINING ACTIVELY BILLING STATUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "today = pd.Timestamp.now().normalize()\n",
    "print(f\"\\nToday's date for comparison: {today.date()}\")\n",
    "print(f\"Active OSS States: {ACTIVE_OSS_STATES}\")\n",
    "\n",
    "# Merge SF orders with OSS data\n",
    "if len(orders_with_oss) > 0:\n",
    "    orders_with_oss[\"OSS_Order_ID\"] = orders_with_oss[\"OSS_Order__c\"].astype(int)\n",
    "\n",
    "if len(oss_orders_df) > 0:\n",
    "    # Convert date columns\n",
    "    oss_orders_df[\"bill_start_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_start_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "    oss_orders_df[\"bill_end_date\"] = pd.to_datetime(\n",
    "        oss_orders_df[\"bill_end_date\"], utc=True\n",
    "    ).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged_df = orders_with_oss.merge(\n",
    "        oss_orders_df, left_on=\"OSS_Order_ID\", right_on=\"order_id\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    merged_df = orders_with_oss.copy()\n",
    "    merged_df[\"order_id\"] = None\n",
    "    merged_df[\"order_state_cd\"] = None\n",
    "    merged_df[\"bill_start_date\"] = None\n",
    "    merged_df[\"bill_end_date\"] = None\n",
    "\n",
    "\n",
    "# Determine actively billing status\n",
    "def get_billing_status(row):\n",
    "    if pd.isna(row.get(\"order_id\")):\n",
    "        return \"No OSS Match\"\n",
    "\n",
    "    state = row[\"order_state_cd\"].strip() if row.get(\"order_state_cd\") else None\n",
    "    bill_start = row.get(\"bill_start_date\")\n",
    "    bill_end = row.get(\"bill_end_date\")\n",
    "\n",
    "    # Check if state is in allowed active states (CL or OA)\n",
    "    if state not in ACTIVE_OSS_STATES:\n",
    "        return f\"Not Active State ({state})\"\n",
    "\n",
    "    if pd.isna(bill_start):\n",
    "        return f\"{state} - No Bill Start Date\"\n",
    "\n",
    "    if bill_start > today:\n",
    "        return f\"{state} - Bill Start Future\"\n",
    "\n",
    "    if pd.notna(bill_end) and bill_end <= today:\n",
    "        return f\"{state} - Bill End Passed\"\n",
    "\n",
    "    return \"ACTIVELY BILLING\"\n",
    "\n",
    "\n",
    "merged_df[\"OSS_Billing_Status\"] = merged_df.apply(get_billing_status, axis=1)\n",
    "\n",
    "# Add state description\n",
    "merged_df[\"OSS_State_Desc\"] = merged_df[\"order_state_cd\"].map(OSS_ORDER_STATES)\n",
    "\n",
    "# Also handle orders without OSS link\n",
    "orders_without_oss[\"OSS_Billing_Status\"] = \"No OSS_Order__c in SF\"\n",
    "orders_without_oss[\"order_state_cd\"] = None\n",
    "orders_without_oss[\"OSS_State_Desc\"] = None\n",
    "orders_without_oss[\"bill_start_date\"] = None\n",
    "orders_without_oss[\"bill_end_date\"] = None\n",
    "\n",
    "# Summary\n",
    "print(\"\\nðŸ“Š OSS Billing Status Distribution (orders with OSS_Order__c):\")\n",
    "if len(merged_df) > 0:\n",
    "    billing_status_counts = merged_df[\"OSS_Billing_Status\"].value_counts()\n",
    "    for status, count in billing_status_counts.items():\n",
    "        pct = 100 * count / len(merged_df)\n",
    "        print(f\"   {status}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: FILTER - ACTIVELY BILLING ONLY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Actively Billing Filter:\n",
      "   Before: 13,608\n",
      "   âŒ Excluded (Not Actively Billing): 2,119\n",
      "   âœ… Confirmed Actively Billing: 11,489\n",
      "\n",
      "   Breakdown of excluded:\n",
      "      No OSS_Order__c in SF: 2,083\n",
      "      CL - Bill End Passed: 23\n",
      "      Not Active State (CA): 6\n",
      "      CL - Bill Start Future: 6\n",
      "      No OSS Match: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_6256\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n",
      "C:\\Users\\vjero\\AppData\\Local\\Temp\\ipykernel_6256\\1019780369.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_checked_orders = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "# === STEP 6: FILTER - ACTIVELY BILLING ONLY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: FILTER - ACTIVELY BILLING ONLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get common columns\n",
    "common_cols = [col for col in orders_after_rt_filter.columns]\n",
    "oss_cols = [\n",
    "    \"order_state_cd\",\n",
    "    \"OSS_State_Desc\",\n",
    "    \"bill_start_date\",\n",
    "    \"bill_end_date\",\n",
    "    \"OSS_Billing_Status\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist\n",
    "for col in oss_cols:\n",
    "    if col not in merged_df.columns:\n",
    "        merged_df[col] = None\n",
    "    if col not in orders_without_oss.columns:\n",
    "        orders_without_oss[col] = None\n",
    "\n",
    "all_cols = common_cols + oss_cols\n",
    "\n",
    "# Combine orders with and without OSS\n",
    "all_checked_orders = pd.concat(\n",
    "    [\n",
    "        merged_df[[c for c in all_cols if c in merged_df.columns]],\n",
    "        orders_without_oss[[c for c in all_cols if c in orders_without_oss.columns]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Split into actively billing vs not\n",
    "actively_billing_mask = all_checked_orders[\"OSS_Billing_Status\"] == \"ACTIVELY BILLING\"\n",
    "orders_actively_billing = all_checked_orders[actively_billing_mask].copy()\n",
    "excluded_not_billing_df = all_checked_orders[~actively_billing_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Actively Billing Filter:\")\n",
    "print(f\"   Before: {len(all_checked_orders):,}\")\n",
    "print(f\"   âŒ Excluded (Not Actively Billing): {len(excluded_not_billing_df):,}\")\n",
    "print(f\"   âœ… Confirmed Actively Billing: {len(orders_actively_billing):,}\")\n",
    "\n",
    "# Show breakdown of excluded\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    print(f\"\\n   Breakdown of excluded:\")\n",
    "    for status, count in (\n",
    "        excluded_not_billing_df[\"OSS_Billing_Status\"].value_counts().items()\n",
    "    ):\n",
    "        print(f\"      {status}: {count:,}\")\n",
    "\n",
    "# Set this as our active orders for the rest of the pipeline\n",
    "active_orders_df = orders_actively_billing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: QUERYING NEW BBF BANS\n",
      "================================================================================\n",
      "Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\n",
      "\n",
      "âœ… Found 2,505 new BBF BANs\n",
      "   Legacy BAN to BBF BAN mappings: 2,505\n"
     ]
    }
   ],
   "source": [
    "# === STEP 7: GET NEW BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: QUERYING NEW BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bbf_ban_query = \"\"\"\n",
    "SELECT \n",
    "    Id,\n",
    "    Name,\n",
    "    Account__c,\n",
    "    Account__r.Name,\n",
    "    Legacy_ES_Id__c,\n",
    "    BBF_Ban__c,\n",
    "    Billing_Address_1__c,\n",
    "    Billing_City__c,\n",
    "    Billing_State__c,\n",
    "    Billing_ZIP__c,\n",
    "    Payment_Terms__c,\n",
    "    Active_Billing__c\n",
    "FROM Billing_Invoice__c\n",
    "WHERE BBF_Ban__c = true\n",
    "  AND Legacy_ES_Id__c != null\n",
    "\"\"\"\n",
    "\n",
    "print(\"Querying new BBF BANs (BBF_Ban__c = true, Legacy_ES_Id__c populated)...\")\n",
    "result = es_sf.query_all(bbf_ban_query)\n",
    "bbf_bans_df = pd.DataFrame(result[\"records\"])\n",
    "\n",
    "if len(bbf_bans_df) > 0:\n",
    "    # Flatten Account name\n",
    "    if \"Account__r\" in bbf_bans_df.columns:\n",
    "        bbf_bans_df[\"Account_Name\"] = bbf_bans_df[\"Account__r\"].apply(\n",
    "            lambda x: x[\"Name\"] if x else None\n",
    "        )\n",
    "\n",
    "    # Clean up\n",
    "    bbf_bans_df = bbf_bans_df.drop(\n",
    "        columns=[\"attributes\", \"Account__r\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Found {len(bbf_bans_df):,} new BBF BANs\")\n",
    "\n",
    "    # Create lookup from legacy BAN ID to new BBF BAN\n",
    "    legacy_to_bbf_ban = {}\n",
    "    for _, ban in bbf_bans_df.iterrows():\n",
    "        legacy_id = ban.get(\"Legacy_ES_Id__c\")\n",
    "        if legacy_id:\n",
    "            legacy_to_bbf_ban[legacy_id] = {\n",
    "                \"Id\": ban[\"Id\"],\n",
    "                \"Name\": ban[\"Name\"],\n",
    "                \"Account__c\": ban.get(\"Account__c\"),\n",
    "                \"Account_Name\": ban.get(\"Account_Name\"),\n",
    "            }\n",
    "    print(f\"   Legacy BAN to BBF BAN mappings: {len(legacy_to_bbf_ban):,}\")\n",
    "else:\n",
    "    legacy_to_bbf_ban = {}\n",
    "    print(\"âš ï¸ No BBF BANs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: MAPPING ORDERS TO BBF BANS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š BAN Mapping Results:\n",
      "   âœ… Ready to migrate (has BBF BAN): 11,475\n",
      "   âš ï¸ Missing BBF BAN mapping: 12\n",
      "   âŒ Missing ANY BAN: 2\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8: MAP ORDERS TO BBF BANS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: MAPPING ORDERS TO BBF BANS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track which orders can be mapped\n",
    "orders_with_bbf_ban = []\n",
    "orders_missing_bbf_ban = []\n",
    "orders_missing_any_ban = []\n",
    "\n",
    "for _, order in active_orders_df.iterrows():\n",
    "    legacy_ban_id = order.get(\"Billing_Invoice__c\")\n",
    "    order_dict = order.to_dict()\n",
    "\n",
    "    if not legacy_ban_id:\n",
    "        # Order has no BAN at all\n",
    "        orders_missing_any_ban.append(order_dict)\n",
    "    elif legacy_ban_id in legacy_to_bbf_ban:\n",
    "        # Order can be mapped to new BBF BAN\n",
    "        bbf_ban = legacy_to_bbf_ban[legacy_ban_id]\n",
    "        order_dict[\"New_BBF_BAN_Id\"] = bbf_ban[\"Id\"]\n",
    "        order_dict[\"New_BBF_BAN_Name\"] = bbf_ban[\"Name\"]\n",
    "        order_dict[\"New_BBF_BAN_Account__c\"] = bbf_ban[\"Account__c\"]\n",
    "        order_dict[\"New_BBF_BAN_Account_Name\"] = bbf_ban[\"Account_Name\"]\n",
    "        orders_with_bbf_ban.append(order_dict)\n",
    "    else:\n",
    "        # Order has legacy BAN but no BBF BAN mapping\n",
    "        orders_missing_bbf_ban.append(order_dict)\n",
    "\n",
    "orders_ready_df = pd.DataFrame(orders_with_bbf_ban)\n",
    "orders_no_bbf_ban_df = pd.DataFrame(orders_missing_bbf_ban)\n",
    "orders_no_ban_df = pd.DataFrame(orders_missing_any_ban)\n",
    "\n",
    "print(f\"\\nðŸ“Š BAN Mapping Results:\")\n",
    "print(f\"   âœ… Ready to migrate (has BBF BAN): {len(orders_ready_df):,}\")\n",
    "print(f\"   âš ï¸ Missing BBF BAN mapping: {len(orders_no_bbf_ban_df):,}\")\n",
    "print(f\"   âŒ Missing ANY BAN: {len(orders_no_ban_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Found 2,224 unique Accounts from new BBF BANs\n",
      "\n",
      "âœ… Accounts to migrate: 2,224\n"
     ]
    }
   ],
   "source": [
    "# === STEP 9: IDENTIFY ACCOUNTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: IDENTIFYING ACCOUNTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    unique_account_ids = (\n",
    "        orders_ready_df[\"New_BBF_BAN_Account__c\"].dropna().unique().tolist()\n",
    "    )\n",
    "    print(f\"\\nFound {len(unique_account_ids):,} unique Accounts from new BBF BANs\")\n",
    "\n",
    "    if unique_account_ids:\n",
    "        chunk_size = 150\n",
    "        all_accounts = []\n",
    "\n",
    "        for i in range(0, len(unique_account_ids), chunk_size):\n",
    "            chunk = unique_account_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            account_query = f\"\"\"\n",
    "            SELECT Id, Name, Type, Industry, \n",
    "                   BillingStreet, BillingCity, BillingState, BillingPostalCode, BillingCountry,\n",
    "                   Phone, Website, BBF_New_Id__c\n",
    "            FROM Account\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(account_query)\n",
    "            all_accounts.extend(result[\"records\"])\n",
    "\n",
    "        accounts_df = pd.DataFrame(all_accounts)\n",
    "        if len(accounts_df) > 0:\n",
    "            accounts_df = accounts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Accounts to migrate: {len(accounts_df):,}\")\n",
    "    else:\n",
    "        accounts_df = pd.DataFrame()\n",
    "else:\n",
    "    accounts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Account query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: IDENTIFYING CONTACTS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Contacts to migrate: 15,576\n"
     ]
    }
   ],
   "source": [
    "# === STEP 10: IDENTIFY CONTACTS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 10: IDENTIFYING CONTACTS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(accounts_df) > 0:\n",
    "    account_ids = accounts_df[\"Id\"].tolist()\n",
    "    chunk_size = 150\n",
    "    all_contacts = []\n",
    "\n",
    "    for i in range(0, len(account_ids), chunk_size):\n",
    "        chunk = account_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        contact_query = f\"\"\"\n",
    "        SELECT Id, AccountId, FirstName, LastName, Email, Phone, Title,\n",
    "               MailingStreet, MailingCity, MailingState, MailingPostalCode,\n",
    "               BBF_New_Id__c\n",
    "        FROM Contact\n",
    "        WHERE AccountId IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(contact_query)\n",
    "        all_contacts.extend(result[\"records\"])\n",
    "\n",
    "    contacts_df = pd.DataFrame(all_contacts)\n",
    "    if len(contacts_df) > 0:\n",
    "        contacts_df = contacts_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "    print(f\"\\nâœ… Contacts to migrate: {len(contacts_df):,}\")\n",
    "else:\n",
    "    contacts_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No accounts to migrate, skipping Contact query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "Unique locations referenced by orders:\n",
      "   Address_A: 723\n",
      "   Address_Z: 9,818\n",
      "   Combined unique: 10,175\n",
      "\n",
      "âœ… Locations to migrate: 10,175\n"
     ]
    }
   ],
   "source": [
    "# === STEP 11: IDENTIFY LOCATIONS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 11: IDENTIFYING LOCATIONS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    address_a_ids = orders_ready_df[\"Address_A__c\"].dropna().unique().tolist()\n",
    "    address_z_ids = orders_ready_df[\"Address_Z__c\"].dropna().unique().tolist()\n",
    "    all_address_ids = list(set(address_a_ids + address_z_ids))\n",
    "\n",
    "    print(f\"\\nUnique locations referenced by orders:\")\n",
    "    print(f\"   Address_A: {len(address_a_ids):,}\")\n",
    "    print(f\"   Address_Z: {len(address_z_ids):,}\")\n",
    "    print(f\"   Combined unique: {len(all_address_ids):,}\")\n",
    "\n",
    "    if all_address_ids:\n",
    "        chunk_size = 150\n",
    "        all_locations = []\n",
    "\n",
    "        for i in range(0, len(all_address_ids), chunk_size):\n",
    "            chunk = all_address_ids[i : i + chunk_size]\n",
    "            ids_str = \"','\".join(chunk)\n",
    "\n",
    "            location_query = f\"\"\"\n",
    "            SELECT Id, Name, Address__c, City__c, State__c, County__c, Zip__c,\n",
    "                   Complete_Address__c, CLLI__c, Building_Status__c, On_Net__c,\n",
    "                   BBF_New_Id__c\n",
    "            FROM Address__c\n",
    "            WHERE Id IN ('{ids_str}')\n",
    "            \"\"\"\n",
    "            result = es_sf.query_all(location_query)\n",
    "            all_locations.extend(result[\"records\"])\n",
    "\n",
    "        locations_df = pd.DataFrame(all_locations)\n",
    "        if len(locations_df) > 0:\n",
    "            locations_df = locations_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        print(f\"\\nâœ… Locations to migrate: {len(locations_df):,}\")\n",
    "    else:\n",
    "        locations_df = pd.DataFrame()\n",
    "else:\n",
    "    locations_df = pd.DataFrame()\n",
    "    all_address_ids = []\n",
    "    print(\"âš ï¸ No orders ready to migrate, skipping Location query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\n",
      "================================================================================\n",
      "\n",
      "âœ… Off_Net records to migrate: 2,157\n"
     ]
    }
   ],
   "source": [
    "# === STEP 12: IDENTIFY OFF_NET RECORDS TO MIGRATE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 12: IDENTIFYING OFF_NET RECORDS TO MIGRATE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(all_address_ids) > 0:\n",
    "    chunk_size = 100\n",
    "    all_offnet = []\n",
    "\n",
    "    for i in range(0, len(all_address_ids), chunk_size):\n",
    "        chunk = all_address_ids[i : i + chunk_size]\n",
    "        ids_str = \"','\".join(chunk)\n",
    "\n",
    "        offnet_query = f\"\"\"\n",
    "        SELECT Id, Name, \n",
    "               Location_1__c, Location_1_Address__c,\n",
    "               Location_2__c, Location_2_Address__c,\n",
    "               Off_Net_Vendor__c, Vendor_Name__c,\n",
    "               Vendor_circuit_Id__c, Internal_Circuit_Id__c,\n",
    "               Cost_MRC__c, Cost_NRC__c, Invoice_MRC__c,\n",
    "               LEC_Order_Status__c, Off_Net_Type__c,\n",
    "               Bandwidth__c, Circuit_Type__c, Term__c,\n",
    "               Term_Agreement_Start_Date__c, Term_Agreement_End_Date__c,\n",
    "               Vendor_Bill_Start_Date__c, Vendor_Bill_Stop_Date__c,\n",
    "               SOF1__c\n",
    "        FROM Off_Net__c\n",
    "        WHERE Location_1__c IN ('{ids_str}')\n",
    "           OR Location_2__c IN ('{ids_str}')\n",
    "        \"\"\"\n",
    "        result = es_sf.query_all(offnet_query)\n",
    "        all_offnet.extend(result[\"records\"])\n",
    "\n",
    "    offnet_df = pd.DataFrame(all_offnet)\n",
    "    if len(offnet_df) > 0:\n",
    "        offnet_df = offnet_df.drop(columns=[\"attributes\"], errors=\"ignore\")\n",
    "        offnet_df = offnet_df.drop_duplicates(subset=[\"Id\"])\n",
    "    print(f\"\\nâœ… Off_Net records to migrate: {len(offnet_df):,}\")\n",
    "else:\n",
    "    offnet_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ No locations to migrate, skipping Off_Net query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 13: DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Data Quality Issues:\n",
      "   [HIGH] Orders missing Address_A__c: 2 (0.0%)\n",
      "         â†’ Cannot set A_Location__c on BBF Service__c\n",
      "   [LOW] Orders missing Node__c: 9784 (85.3%)\n",
      "         â†’ Can fix post-migration - A_Node__c/Z_Node__c optional\n",
      "   [LOW] Orders missing Service_Start_Date__c in SF: 4425 (38.6%)\n",
      "         â†’ OSS has bill_start_date for 4,425 - USE FOR MIGRATION\n"
     ]
    }
   ],
   "source": [
    "# === STEP 13: DATA QUALITY ANALYSIS ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 13: DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "data_quality_issues = []\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    total_ready = len(orders_ready_df)\n",
    "\n",
    "    # Check for missing Address_A__c\n",
    "    missing_addr_a = orders_ready_df[\"Address_A__c\"].isna().sum()\n",
    "    if missing_addr_a > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Address_A__c\",\n",
    "                \"Count\": missing_addr_a,\n",
    "                \"Percentage\": f\"{missing_addr_a/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"HIGH\",\n",
    "                \"Impact\": \"Cannot set A_Location__c on BBF Service__c\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check for missing Node__c\n",
    "    missing_node = orders_ready_df[\"Node__c\"].isna().sum()\n",
    "    if missing_node > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Node__c\",\n",
    "                \"Count\": missing_node,\n",
    "                \"Percentage\": f\"{missing_node/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": \"Can fix post-migration - A_Node__c/Z_Node__c optional\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check for missing Service_Start_Date__c - but OSS has bill_start_date\n",
    "    sf_start_null = orders_ready_df[\"Service_Start_Date__c\"].isna()\n",
    "    oss_start_exists = orders_ready_df[\"bill_start_date\"].notna()\n",
    "    missing_sf_has_oss = (sf_start_null & oss_start_exists).sum()\n",
    "\n",
    "    if sf_start_null.sum() > 0:\n",
    "        data_quality_issues.append(\n",
    "            {\n",
    "                \"Issue\": \"Orders missing Service_Start_Date__c in SF\",\n",
    "                \"Count\": sf_start_null.sum(),\n",
    "                \"Percentage\": f\"{sf_start_null.sum()/total_ready*100:.1f}%\",\n",
    "                \"Severity\": \"LOW\",\n",
    "                \"Impact\": f\"OSS has bill_start_date for {missing_sf_has_oss:,} - USE FOR MIGRATION\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "data_quality_df = pd.DataFrame(data_quality_issues)\n",
    "\n",
    "if len(data_quality_issues) > 0:\n",
    "    print(\"\\nðŸ“Š Data Quality Issues:\")\n",
    "    for issue in data_quality_issues:\n",
    "        print(\n",
    "            f\"   [{issue['Severity']}] {issue['Issue']}: {issue['Count']} ({issue['Percentage']})\"\n",
    "        )\n",
    "        print(f\"         â†’ {issue['Impact']}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No significant data quality issues found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 14: CREATING BAN MAPPING TABLE\n",
      "================================================================================\n",
      "\n",
      "âœ… BAN mappings with orders: 2,440\n"
     ]
    }
   ],
   "source": [
    "# === STEP 14: CREATE BAN MAPPING TABLE ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 14: CREATING BAN MAPPING TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(orders_ready_df) > 0:\n",
    "    ban_mapping_data = []\n",
    "    legacy_ban_counts = orders_ready_df.groupby(\"Billing_Invoice__c\").size().to_dict()\n",
    "\n",
    "    for legacy_id, bbf_ban in legacy_to_bbf_ban.items():\n",
    "        order_count = legacy_ban_counts.get(legacy_id, 0)\n",
    "        if order_count > 0:\n",
    "            ban_mapping_data.append(\n",
    "                {\n",
    "                    \"Legacy_BAN_Id\": legacy_id,\n",
    "                    \"New_BBF_BAN_Id\": bbf_ban[\"Id\"],\n",
    "                    \"New_BBF_BAN_Name\": bbf_ban[\"Name\"],\n",
    "                    \"Account__c\": bbf_ban[\"Account__c\"],\n",
    "                    \"Account_Name\": bbf_ban[\"Account_Name\"],\n",
    "                    \"Order_Count\": order_count,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    ban_mapping_df = pd.DataFrame(ban_mapping_data)\n",
    "    ban_mapping_df = ban_mapping_df.sort_values(\"Order_Count\", ascending=False)\n",
    "    print(f\"\\nâœ… BAN mappings with orders: {len(ban_mapping_df):,}\")\n",
    "else:\n",
    "    ban_mapping_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 15: MIGRATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "FILTER PIPELINE:\n",
      "  Total Active Status Orders:     17,972\n",
      "  â”œâ”€ Excluded (PA MARKET DECOM):  887\n",
      "  â”œâ”€ Excluded (Work Orders):      3,477\n",
      "  â”œâ”€ Excluded (Not Billing OSS):  2,119\n",
      "  â””â”€ Confirmed Actively Billing:  11,489\n",
      "\n",
      "BAN MAPPING:\n",
      "  â”œâ”€ Ready to migrate:            11,475\n",
      "  â”œâ”€ Missing BBF BAN:             12\n",
      "  â””â”€ Missing ANY BAN:             2\n",
      "\n",
      "RECORDS TO MIGRATE:\n",
      "  â”œâ”€ BANs:                        2,440\n",
      "  â”œâ”€ Accounts:                    2,224\n",
      "  â”œâ”€ Contacts:                    15,576\n",
      "  â”œâ”€ Locations:                   10,175\n",
      "  â””â”€ Off_Net:                     2,157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === STEP 15: GENERATE SUMMARY ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 15: MIGRATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = [\n",
    "    {\"Category\": \"FILTER PIPELINE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"1. Total Active Status Orders\",\n",
    "        \"Count\": len(all_orders_df),\n",
    "        \"Notes\": \"All orders with qualifying status\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"2. Excluded (PA MARKET DECOM)\",\n",
    "        \"Count\": len(excluded_pa_decom_df),\n",
    "        \"Notes\": \"Project_Group__c contains 'PA MARKET DECOM'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"3. Excluded (Work Orders)\",\n",
    "        \"Count\": len(excluded_work_orders_df),\n",
    "        \"Notes\": f\"Service_Order_Record_Type__c != '{VALID_RECORD_TYPE}'\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"4. Excluded (Not Actively Billing)\",\n",
    "        \"Count\": len(excluded_not_billing_df),\n",
    "        \"Notes\": f\"OSS: not in {ACTIVE_OSS_STATES} or billing dates invalid\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"5. Confirmed Actively Billing\",\n",
    "        \"Count\": len(active_orders_df),\n",
    "        \"Notes\": \"Passed all filters\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"BAN MAPPING\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Ready (has BBF BAN)\",\n",
    "        \"Count\": len(orders_ready_df),\n",
    "        \"Notes\": \"Can be migrated now\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing BBF BAN\",\n",
    "        \"Count\": len(orders_no_bbf_ban_df),\n",
    "        \"Notes\": \"Need new BBF BAN created\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Missing ANY BAN\",\n",
    "        \"Count\": len(orders_no_ban_df),\n",
    "        \"Notes\": \"CRITICAL - no BAN reference\",\n",
    "    },\n",
    "    {\"Category\": \"\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\"Category\": \"RECORDS TO MIGRATE\", \"Metric\": \"\", \"Count\": \"\", \"Notes\": \"\"},\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"BANs (with orders)\",\n",
    "        \"Count\": len(ban_mapping_df),\n",
    "        \"Notes\": \"Billing_Invoice__c to BAN__c\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Accounts\",\n",
    "        \"Count\": len(accounts_df),\n",
    "        \"Notes\": \"Unique accounts from BBF BANs\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Contacts\",\n",
    "        \"Count\": len(contacts_df),\n",
    "        \"Notes\": \"Contacts for migration accounts\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Locations\",\n",
    "        \"Count\": len(locations_df),\n",
    "        \"Notes\": \"Address_A + Address_Z from orders\",\n",
    "    },\n",
    "    {\n",
    "        \"Category\": \"\",\n",
    "        \"Metric\": \"Off_Net\",\n",
    "        \"Count\": len(offnet_df),\n",
    "        \"Notes\": \"Off_Net for migration locations\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "ðŸ“Š SUMMARY\n",
    "{'â”€'*50}\n",
    "FILTER PIPELINE:\n",
    "  Total Active Status Orders:     {len(all_orders_df):,}\n",
    "  â”œâ”€ Excluded (PA MARKET DECOM):  {len(excluded_pa_decom_df):,}\n",
    "  â”œâ”€ Excluded (Work Orders):      {len(excluded_work_orders_df):,}\n",
    "  â”œâ”€ Excluded (Not Billing OSS):  {len(excluded_not_billing_df):,}\n",
    "  â””â”€ Confirmed Actively Billing:  {len(active_orders_df):,}\n",
    "\n",
    "BAN MAPPING:\n",
    "  â”œâ”€ Ready to migrate:            {len(orders_ready_df):,}\n",
    "  â”œâ”€ Missing BBF BAN:             {len(orders_no_bbf_ban_df):,}\n",
    "  â””â”€ Missing ANY BAN:             {len(orders_no_ban_df):,}\n",
    "\n",
    "RECORDS TO MIGRATE:\n",
    "  â”œâ”€ BANs:                        {len(ban_mapping_df):,}\n",
    "  â”œâ”€ Accounts:                    {len(accounts_df):,}\n",
    "  â”œâ”€ Contacts:                    {len(contacts_df):,}\n",
    "  â”œâ”€ Locations:                   {len(locations_df):,}\n",
    "  â””â”€ Off_Net:                     {len(offnet_df):,}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 16: EXPORTING TO EXCEL\n",
      "================================================================================\n",
      "   âœ… Summary\n",
      "   âœ… Active_Orders (11,475 records)\n",
      "   âœ… BAN_Mapping (2,440 records)\n",
      "   âœ… Accounts (2,224 records)\n",
      "   âœ… Contacts (15,576 records)\n",
      "   âœ… Locations (10,175 records)\n",
      "   âœ… Off_Net (2,157 records)\n",
      "   âœ… Data_Quality (3 issues)\n",
      "   âœ… Orders_Missing_BBF_BAN (12 records)\n",
      "   âœ… Excluded_PA_DECOM (887 records)\n",
      "   âœ… Excluded_Work_Orders (3,477 records)\n",
      "   âœ… Excluded_Not_Billing (2,119 records)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Excel does not support timezones in datetimes. The tzinfo in the datetime/time object must be set to None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 191\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   âœ… Excluded_Not_Billing (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(excluded_not_billing_df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[43mwb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Excel file saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\workbook\\workbook.py:386\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[1;32m--> 386\u001b[0m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\writer\\excel.py:294\u001b[0m, in \u001b[0;36msave_workbook\u001b[1;34m(workbook, filename)\u001b[0m\n\u001b[0;32m    292\u001b[0m workbook\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mmodified \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(tz\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mtimezone\u001b[38;5;241m.\u001b[39mutc)\u001b[38;5;241m.\u001b[39mreplace(tzinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n\u001b[1;32m--> 294\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\writer\\excel.py:275\u001b[0m, in \u001b[0;36mExcelWriter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\writer\\excel.py:77\u001b[0m, in \u001b[0;36mExcelWriter.write_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m     custom_override \u001b[38;5;241m=\u001b[39m CustomOverride()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifest\u001b[38;5;241m.\u001b[39mappend(custom_override)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_worksheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_chartsheets()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_images()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\writer\\excel.py:215\u001b[0m, in \u001b[0;36mExcelWriter._write_worksheets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, ws \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkbook\u001b[38;5;241m.\u001b[39mworksheets, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    214\u001b[0m     ws\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_worksheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ws\u001b[38;5;241m.\u001b[39m_drawing:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_drawing(ws\u001b[38;5;241m.\u001b[39m_drawing)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\writer\\excel.py:200\u001b[0m, in \u001b[0;36mExcelWriter.write_worksheet\u001b[1;34m(self, ws)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     writer \u001b[38;5;241m=\u001b[39m WorksheetWriter(ws)\n\u001b[1;32m--> 200\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m ws\u001b[38;5;241m.\u001b[39m_rels \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39m_rels\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mwrite(writer\u001b[38;5;241m.\u001b[39mout, ws\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\worksheet\\_writer.py:359\u001b[0m, in \u001b[0;36mWorksheetWriter.write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03mHigh level\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_top()\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_tail()\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\worksheet\\_writer.py:125\u001b[0m, in \u001b[0;36mWorksheetWriter.write_rows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xf\u001b[38;5;241m.\u001b[39melement(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msheetData\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row_idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrows():\n\u001b[1;32m--> 125\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxf\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\worksheet\\_writer.py:147\u001b[0m, in \u001b[0;36mWorksheetWriter.write_row\u001b[1;34m(self, xf, row, row_idx)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     cell\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mhas_style\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cell\u001b[38;5;241m.\u001b[39m_comment\n\u001b[0;32m    145\u001b[0m     ):\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mwrite_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_style\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\cell\\_writer.py:90\u001b[0m, in \u001b[0;36mlxml_write_cell\u001b[1;34m(xf, worksheet, cell, styled)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlxml_write_cell\u001b[39m(xf, worksheet, cell, styled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 90\u001b[0m     value, attributes \u001b[38;5;241m=\u001b[39m \u001b[43m_set_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m xf\u001b[38;5;241m.\u001b[39melement(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, attributes):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\cell\\_writer.py:30\u001b[0m, in \u001b[0;36m_set_attributes\u001b[1;34m(cell, styled)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtzinfo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mtzinfo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel does not support timezones in datetimes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tzinfo in the datetime/time object must be set to None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39miso_dates \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, timedelta):\n\u001b[0;32m     34\u001b[0m         value \u001b[38;5;241m=\u001b[39m to_ISO8601(value)\n",
      "\u001b[1;31mTypeError\u001b[0m: Excel does not support timezones in datetimes. The tzinfo in the datetime/time object must be set to None."
     ]
    }
   ],
   "source": [
    "# === STEP 16: EXPORT TO EXCEL ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 16: EXPORTING TO EXCEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "wb = Workbook()\n",
    "ws_summary = wb.active\n",
    "ws_summary.title = \"Summary\"\n",
    "\n",
    "# Styles\n",
    "header_font = Font(bold=True, size=12, color=\"FFFFFF\")\n",
    "header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "thin_border = Border(\n",
    "    left=Side(style=\"thin\"),\n",
    "    right=Side(style=\"thin\"),\n",
    "    top=Side(style=\"thin\"),\n",
    "    bottom=Side(style=\"thin\"),\n",
    ")\n",
    "\n",
    "\n",
    "def write_df_to_sheet(ws, df, start_row=1):\n",
    "    \"\"\"Write dataframe to worksheet with formatting\"\"\"\n",
    "    for r_idx, row in enumerate(\n",
    "        dataframe_to_rows(df, index=False, header=True), start=start_row\n",
    "    ):\n",
    "        for c_idx, value in enumerate(row, start=1):\n",
    "            cell = ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "            if r_idx == start_row:\n",
    "                cell.font = header_font\n",
    "                cell.fill = header_fill\n",
    "            cell.border = thin_border\n",
    "\n",
    "\n",
    "# --- SHEET 1: Summary ---\n",
    "ws_summary.append([\"ES â†’ BBF Migration Analysis (v3)\"])\n",
    "ws_summary[\"A1\"].font = Font(bold=True, size=16)\n",
    "ws_summary.append([f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"])\n",
    "ws_summary.append(\n",
    "    [\n",
    "        f\"OSS Active States: {ACTIVE_OSS_STATES} | Record Type filter | OSS Billing validation\"\n",
    "    ]\n",
    ")\n",
    "ws_summary.append([])\n",
    "write_df_to_sheet(ws_summary, summary_df, start_row=5)\n",
    "\n",
    "ws_summary.column_dimensions[\"A\"].width = 20\n",
    "ws_summary.column_dimensions[\"B\"].width = 40\n",
    "ws_summary.column_dimensions[\"C\"].width = 12\n",
    "ws_summary.column_dimensions[\"D\"].width = 55\n",
    "print(\"   âœ… Summary\")\n",
    "\n",
    "# --- SHEET 2: Active Orders (Ready) ---\n",
    "if len(orders_ready_df) > 0:\n",
    "    ws = wb.create_sheet(\"Active_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"New_BBF_BAN_Id\",\n",
    "        \"New_BBF_BAN_Name\",\n",
    "        \"Address_A__c\",\n",
    "        \"Address_Z__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "        \"Service_Start_Date__c\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_ready_df.columns]\n",
    "    write_df_to_sheet(ws, orders_ready_df[export_cols])\n",
    "    print(f\"   âœ… Active_Orders ({len(orders_ready_df):,} records)\")\n",
    "\n",
    "# --- SHEET 3: BAN Mapping ---\n",
    "if len(ban_mapping_df) > 0:\n",
    "    ws = wb.create_sheet(\"BAN_Mapping\")\n",
    "    write_df_to_sheet(ws, ban_mapping_df)\n",
    "    print(f\"   âœ… BAN_Mapping ({len(ban_mapping_df):,} records)\")\n",
    "\n",
    "# --- SHEET 4: Accounts ---\n",
    "if len(accounts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Accounts\")\n",
    "    write_df_to_sheet(ws, accounts_df)\n",
    "    print(f\"   âœ… Accounts ({len(accounts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 5: Contacts ---\n",
    "if len(contacts_df) > 0:\n",
    "    ws = wb.create_sheet(\"Contacts\")\n",
    "    write_df_to_sheet(ws, contacts_df)\n",
    "    print(f\"   âœ… Contacts ({len(contacts_df):,} records)\")\n",
    "\n",
    "# --- SHEET 6: Locations ---\n",
    "if len(locations_df) > 0:\n",
    "    ws = wb.create_sheet(\"Locations\")\n",
    "    write_df_to_sheet(ws, locations_df)\n",
    "    print(f\"   âœ… Locations ({len(locations_df):,} records)\")\n",
    "\n",
    "# --- SHEET 7: Off_Net ---\n",
    "if len(offnet_df) > 0:\n",
    "    ws = wb.create_sheet(\"Off_Net\")\n",
    "    write_df_to_sheet(ws, offnet_df)\n",
    "    print(f\"   âœ… Off_Net ({len(offnet_df):,} records)\")\n",
    "\n",
    "# --- SHEET 8: Data Quality ---\n",
    "if len(data_quality_df) > 0:\n",
    "    ws = wb.create_sheet(\"Data_Quality\")\n",
    "    write_df_to_sheet(ws, data_quality_df)\n",
    "    print(f\"   âœ… Data_Quality ({len(data_quality_df):,} issues)\")\n",
    "\n",
    "# --- SHEET 9: Orders Missing BBF BAN ---\n",
    "if len(orders_no_bbf_ban_df) > 0:\n",
    "    ws = wb.create_sheet(\"Orders_Missing_BBF_BAN\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"Billing_Invoice__c\",\n",
    "        \"SOF_MRC__c\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in orders_no_bbf_ban_df.columns]\n",
    "    write_df_to_sheet(ws, orders_no_bbf_ban_df[export_cols])\n",
    "    print(f\"   âœ… Orders_Missing_BBF_BAN ({len(orders_no_bbf_ban_df):,} records)\")\n",
    "\n",
    "# --- SHEET 10: Excluded - PA MARKET DECOM ---\n",
    "if len(excluded_pa_decom_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_PA_DECOM\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Project_Group__c\",\n",
    "        \"Account_Name\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_pa_decom_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_pa_decom_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_PA_DECOM ({len(excluded_pa_decom_df):,} records)\")\n",
    "\n",
    "# --- SHEET 11: Excluded - Work Orders (enriched with OSS data) ---\n",
    "if len(excluded_work_orders_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Work_Orders\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Service_Order_Record_Type__c\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_nm\",\n",
    "        \"order_id\",\n",
    "        \"workorder_type_cd\",\n",
    "        \"workorder_type_desc\",\n",
    "        \"workorder_state_cd\",\n",
    "        \"workorder_state_desc\",\n",
    "        \"description\",\n",
    "        \"start_date\",\n",
    "        \"end_date\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_work_orders_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_work_orders_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Work_Orders ({len(excluded_work_orders_df):,} records)\")\n",
    "\n",
    "# --- SHEET 12: Excluded - Not Actively Billing ---\n",
    "if len(excluded_not_billing_df) > 0:\n",
    "    ws = wb.create_sheet(\"Excluded_Not_Billing\")\n",
    "    export_cols = [\n",
    "        \"Id\",\n",
    "        \"Name\",\n",
    "        \"Service_ID__c\",\n",
    "        \"Status\",\n",
    "        \"Account_Name\",\n",
    "        \"OSS_Order__c\",\n",
    "        \"order_state_cd\",\n",
    "        \"OSS_State_Desc\",\n",
    "        \"bill_start_date\",\n",
    "        \"bill_end_date\",\n",
    "        \"OSS_Billing_Status\",\n",
    "    ]\n",
    "    export_cols = [c for c in export_cols if c in excluded_not_billing_df.columns]\n",
    "    write_df_to_sheet(ws, excluded_not_billing_df[export_cols])\n",
    "    print(f\"   âœ… Excluded_Not_Billing ({len(excluded_not_billing_df):,} records)\")\n",
    "\n",
    "# Save\n",
    "wb.save(OUTPUT_FILE)\n",
    "print(f\"\\nâœ… Excel file saved: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLEANUP ===\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "oss_conn.close()\n",
    "print(\"âœ… OSS connection closed\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## v3 Changes Summary\n",
    "\n",
    "### OSS Active States\n",
    "- Now includes both **CL (Closed)** and **OA (Accepted)** as valid active states\n",
    "- Both must still pass billing date criteria (bill_start_date <= today, bill_end_date null or > today)\n",
    "\n",
    "### Data Quality - Service Start Date\n",
    "- Updated to show that OSS `bill_start_date` can be used when SF `Service_Start_Date__c` is missing\n",
    "- Severity reduced to LOW since we have the data from OSS\n",
    "\n",
    "### Work Orders Enrichment\n",
    "- Excluded_Work_Orders sheet now includes OSS data from `workorders.workorders` table:\n",
    "  - `order_nm`, `order_id`, `workorder_type_cd`, `workorder_state_cd`, `description`, `start_date`, `end_date`\n",
    "- Only includes active workorders (`disabled = 'infinity'`)\n",
    "\n",
    "### Next Steps\n",
    "Same as v2 - proceed with migration execution order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
