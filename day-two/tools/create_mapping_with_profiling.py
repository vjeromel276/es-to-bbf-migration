#!/usr/bin/env python3
"""
Enhanced Field Mapping Creator - Uses actual data profiling insights.

This example demonstrates how to integrate ES data profiling results
with the AI-powered field mapping process.

Usage:
    # Step 1: Profile the data first
    python es_data_profiler.py --object Account --sample-size 10000 --output-dir ../data-profiles

    # Step 2: Create mapping with profiling insights
    python create_mapping_with_profiling.py --object Account --profile-dir ../data-profiles

How it enhances mapping:
1. Marks low-population fields (<10%) as "SKIP - Low Data"
2. Prioritizes high-population fields (>80%) as "PRIORITIZE"
3. Only includes USED picklist values in transformers (not all possible values)
4. Flags fields that need truncation
5. Shows actual value distributions in mapping notes
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, List, Optional, Set

try:
    import pandas as pd
except ImportError:
    print("Error: pandas not installed. Run: pip install pandas")
    sys.exit(1)


def load_profiling_data(profile_dir: Path, object_name: str) -> Optional[Dict]:
    """Load profiling data from CSV files."""
    print(f"\nüìä Loading profiling data for {object_name}...")

    # Find the most recent profiling files
    pattern_summary = f"{object_name}_field_population_summary_*.csv"
    pattern_picklist = f"{object_name}_picklist_distributions_*.csv"
    pattern_recommendations = f"{object_name}_field_recommendations_*.csv"

    summary_files = sorted(profile_dir.glob(pattern_summary), reverse=True)
    picklist_files = sorted(profile_dir.glob(pattern_picklist), reverse=True)
    recommendation_files = sorted(profile_dir.glob(pattern_recommendations), reverse=True)

    if not summary_files:
        print(f"‚ö†Ô∏è  No profiling data found for {object_name}")
        print(f"   Run: python es_data_profiler.py --object {object_name}")
        return None

    # Load most recent files
    summary_df = pd.read_csv(summary_files[0])
    picklist_df = pd.read_csv(picklist_files[0]) if picklist_files else None
    recommendations_df = pd.read_csv(recommendation_files[0]) if recommendation_files else None

    print(f"‚úÖ Loaded profiling data from {summary_files[0].name}")

    return {
        'summary': summary_df,
        'picklist': picklist_df,
        'recommendations': recommendations_df
    }


def get_field_priority(field_name: str, profiling_data: Dict) -> str:
    """Get field priority based on profiling data."""
    if profiling_data is None:
        return 'UNKNOWN'

    recommendations = profiling_data.get('recommendations')
    if recommendations is None:
        return 'UNKNOWN'

    match = recommendations[recommendations['Field Name'] == field_name]
    if match.empty:
        return 'UNKNOWN'

    return match.iloc[0]['Recommendation']


def get_population_rate(field_name: str, profiling_data: Dict) -> float:
    """Get field population rate from profiling data."""
    if profiling_data is None:
        return 0.0

    summary = profiling_data.get('summary')
    if summary is None:
        return 0.0

    match = summary[summary['Field Name'] == field_name]
    if match.empty:
        return 0.0

    return match.iloc[0]['Population Rate %']


def get_used_picklist_values(field_name: str, profiling_data: Dict) -> List[str]:
    """Get only the picklist values that are actually USED in production data."""
    if profiling_data is None:
        return []

    picklist = profiling_data.get('picklist')
    if picklist is None:
        return []

    # Filter for this field and USED status
    used_values = picklist[
        (picklist['Field Name'] == field_name) &
        (picklist['Status'] == 'USED')
    ]

    if used_values.empty:
        return []

    # Return values sorted by usage (most common first)
    return used_values.sort_values('Count', ascending=False)['Picklist Value'].tolist()


def get_picklist_distribution_notes(field_name: str, profiling_data: Dict) -> str:
    """Get detailed picklist distribution notes for mapping."""
    if profiling_data is None:
        return ''

    picklist = profiling_data.get('picklist')
    if picklist is None:
        return ''

    field_data = picklist[picklist['Field Name'] == field_name]
    if field_data.empty:
        return ''

    notes = []

    # Get used values
    used = field_data[field_data['Status'] == 'USED'].sort_values('Count', ascending=False)
    if not used.empty:
        notes.append("ACTUAL USAGE:")
        for _, row in used.head(10).iterrows():  # Top 10 most common
            value = row['Picklist Value']
            count = row['Count']
            pct = row['Percentage']
            notes.append(f"  - '{value}': {count:,} records ({pct}%)")

    # Get unused values
    unused = field_data[field_data['Status'] == 'UNUSED']
    if not unused.empty:
        unused_list = unused['Picklist Value'].tolist()
        notes.append(f"\nUNUSED (metadata only): {', '.join(unused_list[:10])}")
        if len(unused_list) > 10:
            notes.append(f"  ... and {len(unused_list) - 10} more")

    # Get undefined values (data quality issue)
    undefined = field_data[field_data['Status'].str.contains('UNDEFINED', na=False)]
    if not undefined.empty:
        undefined_list = undefined['Picklist Value'].tolist()
        notes.append(f"\n‚ö†Ô∏è  DATA QUALITY ISSUE - Values in data but not in metadata: {', '.join(undefined_list)}")

    return '\n'.join(notes) if notes else ''


def enhance_mapping_notes(field_name: str, original_notes: str, profiling_data: Dict) -> str:
    """Enhance mapping notes with profiling insights."""
    if profiling_data is None:
        return original_notes

    enhanced_notes = [original_notes] if original_notes else []

    # Add population rate
    pop_rate = get_population_rate(field_name, profiling_data)
    if pop_rate > 0:
        enhanced_notes.append(f"\nüìä PROFILING: {pop_rate}% populated in production")

        if pop_rate < 10:
            enhanced_notes.append("‚ö†Ô∏è  LOW DATA - Consider skipping this field")
        elif pop_rate > 80:
            enhanced_notes.append("‚úÖ HIGH VALUE - Prioritize this field")

    # Add picklist distribution
    picklist_notes = get_picklist_distribution_notes(field_name, profiling_data)
    if picklist_notes:
        enhanced_notes.append(f"\n{picklist_notes}")

    # Check for truncation needs
    summary = profiling_data.get('summary')
    if summary is not None:
        match = summary[summary['Field Name'] == field_name]
        if not match.empty:
            # This would come from the profiling analysis
            # For now, we'll add a placeholder
            pass

    return '\n'.join(enhanced_notes)


def create_enhanced_mapping_example():
    """
    Example showing how to use profiling data in mapping creation.

    This is a template - adapt for your specific mapping needs.
    """
    print("\n" + "="*80)
    print("ENHANCED FIELD MAPPING WITH DATA PROFILING")
    print("="*80)

    print("""
This is an example template showing how to integrate data profiling
into your field mapping process.

Key Enhancements:

1. ‚úÖ Field Priority Based on Actual Data
   - Fields with >80% population ‚Üí PRIORITIZE
   - Fields with 50-80% population ‚Üí CONSIDER
   - Fields with 10-50% population ‚Üí LOW PRIORITY
   - Fields with <10% population ‚Üí SKIP

2. ‚úÖ Picklist Transformers Use Only USED Values
   - Before: Map all 20 possible values
   - After: Map only the 3 values actually used in production

3. ‚úÖ Data Quality Alerts
   - Flags undefined picklist values (in data but not metadata)
   - Identifies truncation needs
   - Shows null rates

4. ‚úÖ Real Usage Context
   - "Payment Terms: 80% use NET 30, 15% use NET 15, 5% use Prepaid"
   - Not just: "Payment Terms is a picklist"

Integration Steps:
------------------

BEFORE creating field mappings:

1. Run data profiler:
   $ python es_data_profiler.py --object Account --sample-size 10000

2. Review profiling outputs:
   - field_population_summary.csv ‚Üí Identify high-value fields
   - picklist_distributions.csv ‚Üí See actual value usage
   - field_recommendations.csv ‚Üí Get prioritization guidance

3. Use profiling insights when creating mappings:
   - Skip low-population fields (<10%)
   - Build transformers only for USED picklist values
   - Add data quality notes for undefined values

4. Generate optimized transformers:
   - Simpler code (fewer values to handle)
   - Faster execution
   - More accurate

Example Code Integration:
-------------------------

    # Load profiling data
    profiling_data = load_profiling_data(Path('../data-profiles'), 'Account')

    # Get field priority
    priority = get_field_priority('Payment_Terms__c', profiling_data)
    # Returns: "PRIORITIZE" (87% populated)

    # Get only used picklist values
    used_values = get_used_picklist_values('Payment_Terms__c', profiling_data)
    # Returns: ['NET 30', 'NET 15', 'Prepaid']
    # (Instead of all 20 possible values from metadata)

    # Enhanced mapping notes
    notes = enhance_mapping_notes('Payment_Terms__c', 'Payment terms field', profiling_data)
    # Returns:
    # Payment terms field
    # üìä PROFILING: 87.5% populated in production
    # ‚úÖ HIGH VALUE - Prioritize this field
    # ACTUAL USAGE:
    #   - 'NET 30': 4,523 records (80.2%)
    #   - 'NET 15': 845 records (15.0%)
    #   - 'Prepaid': 271 records (4.8%)
    # UNUSED (metadata only): NET 45, NET 60, COD, ...

This approach ensures your field mappings are based on REAL DATA,
not just metadata assumptions.
""")


def main():
    parser = argparse.ArgumentParser(
        description="Enhanced field mapping with data profiling insights"
    )
    parser.add_argument(
        '--object',
        help='Object to create mapping for (e.g., Account)'
    )
    parser.add_argument(
        '--profile-dir',
        default='../data-profiles',
        help='Directory containing profiling CSV files'
    )
    parser.add_argument(
        '--example',
        action='store_true',
        help='Show example usage and exit'
    )

    args = parser.parse_args()

    if args.example or not args.object:
        create_enhanced_mapping_example()
        return

    # Load profiling data
    profile_dir = Path(args.profile_dir)
    if not profile_dir.exists():
        print(f"‚ùå Profile directory not found: {profile_dir}")
        print(f"   Run data profiler first: python es_data_profiler.py --object {args.object}")
        sys.exit(1)

    profiling_data = load_profiling_data(profile_dir, args.object)
    if profiling_data is None:
        print(f"\nüí° TIP: Run profiler first:")
        print(f"   python es_data_profiler.py --object {args.object} --sample-size 10000")
        sys.exit(1)

    # Example: Show field priorities
    print(f"\nüìã Field Priorities for {args.object}:")
    print("="*80)

    recommendations = profiling_data['recommendations']
    summary = profiling_data['summary']

    # Show top prioritized fields
    prioritized = recommendations[recommendations['Recommendation'] == 'PRIORITIZE'].sort_values(
        'Population Rate %', ascending=False
    )

    if not prioritized.empty:
        print("\n‚úÖ PRIORITIZE (>80% populated):")
        for _, row in prioritized.head(10).iterrows():
            print(f"   - {row['Field Name']} ({row['Population Rate %']}%): {row['Reasoning']}")

    # Show fields to skip
    skip = recommendations[recommendations['Recommendation'] == 'SKIP']
    if not skip.empty:
        print(f"\n‚ùå SKIP (<10% populated) - {len(skip)} fields:")
        for _, row in skip.head(5).iterrows():
            print(f"   - {row['Field Name']} ({row['Population Rate %']}%)")
        if len(skip) > 5:
            print(f"   ... and {len(skip) - 5} more")

    print("\n" + "="*80)
    print("üí° Use these insights when creating field mappings!")
    print("="*80)


if __name__ == '__main__':
    main()
